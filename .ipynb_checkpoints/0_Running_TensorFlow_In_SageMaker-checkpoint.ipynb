{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Module 1] Train a Keras Sequential Model\n",
    "\n",
    "본 노트북(notebook)은 SageMaker 상에서 Keras Sequential model을 학습하는 방법을 단계별로 설명합니다. 본 노트북에서 사용한 모델은 간단한 deep CNN(Convolutional Neural Network) 모델로 [the Keras examples](https://github.com/keras-team/keras/blob/master/examples/cifar10_cnn.py)에 소개된 모델과 동일합니다.\n",
    "- 참고로, 본 모델은 25 epoch 학습 후에 검증셋의 정확도(accuracy)가 약 75%이고 50 epoch 학습 후에 검증셋의 정확도가 약 79% 입니다.\n",
    "- 본 워크샵 과정에서는 시간 관계상 5 epoch까지만 학습합니다. (단, Horovod 기반 분산 학습은 10 epoch까지 학습합니다.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The dataset\n",
    "\n",
    "[CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html)은 머신 러닝에서 가장 유명한 데이터셋 중 하나입니다.\n",
    "이 데이터셋은 10개의 다른 클래스로 구성된(클래스당 6,000장) 60,000장의 32x32 픽셀 이미지들로 구성되어 있습니다.\n",
    "아래 그림은 클래스당 10장의 이미지들을 랜덤으로 추출한 결과입니다. \n",
    "\n",
    "![cifar10](https://maet3608.github.io/nuts-ml/_images/cifar10.png)\n",
    "\n",
    "본 실습에서 여러분들은 deep CNN을 학습하여 영상 분류(image classification) 작업을 수행합니다. 다음 노트북들에서\n",
    "여러분들은 File Mode, Pipe Mode와 Horovod 기반 분산 학습(distributed training) 결과를 비교할 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the data\n",
    "아래 AWS CLI(Command Line Interface) 커맨드를 사용하여 S3(Amazon Simple Storage Service)에 저장된 TFRecord 데이터셋을 여러분의 로컬 노트북 인스턴스로 복사합니다.\n",
    "S3 경로는 `s3://floor28/data/cifar10` 입니다. \n",
    "\n",
    "### TFRecord는 무엇인가요?\n",
    "- Google에서 Tensorflow backend로 모델링 시에 공식적으로 권장하는 binary 포맷입니다.\n",
    "- Tensorflow의 protocol buffer 파일로 직렬화된 입력 데이터가 담겨 있습니다.\n",
    "- 대용량 데이터를 멀티스레딩으로 빠르게 스트리밍할 때 유용합니다. (모든 데이터는 메모리의 하나의 블록에 저장되므로, 입력 파일이 개별로 저장된 경우보다 데이터 로딩에 필요한 시간이 대폭 단축됩니다.)\n",
    "- Example 객체로 구성된 배열의 집합체입니다. (an array of Examples)\n",
    "- 아래 그림은 $m$차원 feautre가 $n$개의 샘플로 구성된 TFRecord 예시입니다.\n",
    "![TFRecord](./images/TFRecord.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://floor28/data/cifar10/eval/eval.tfrecords to data/eval/eval.tfrecords\n",
      "download: s3://floor28/data/cifar10/validation/validation.tfrecords to data/validation/validation.tfrecords\n",
      "download: s3://floor28/data/cifar10/train/train.tfrecords to data/train/train.tfrecords\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp --recursive s3://floor28/data/cifar10 ./data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the training locally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "본 스크립트는 모델 학습에 필요한 인자값(arguments)들을 사용합니다. 모델 학습에 필요한 인자값들은 아래와 같습니다.\n",
    "\n",
    "1. `model_dir` - 로그와 체크 포인트를 저장하는 경로\n",
    "2. `train, validation, eval` - TFRecord 데이터셋을 저장하는 경로\n",
    "3. `epochs` - epoch 횟수\n",
    "\n",
    "아래 명령어로 **<font color='red'>SageMaker 관련 API 호출 없이</font>** 로컬 노트북 인스턴스 환경에서 1 epoch만 학습해 봅니다. 참고로, MacBook Pro(15-inch, 2018) 2.6GHz Core i7 16GB 사양에서 2분 20초~2분 40초 소요됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/__init__.py:1467: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n",
      "\n",
      "WARNING:tensorflow:From training_script/cifar10_keras.py:31: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "2020-02-27 12:12:41.373619: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  AVX512F\n",
      "To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2020-02-27 12:12:41.422094: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2999995000 Hz\n",
      "2020-02-27 12:12:41.422526: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55f065d45130 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2020-02-27 12:12:41.422544: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2020-02-27 12:12:41.422786: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
      "WARNING:tensorflow:From training_script/cifar10_keras.py:35: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n",
      "\n",
      "WARNING:tensorflow:From training_script/cifar10_keras.py:35: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.\n",
      "\n",
      "INFO:root:getting data\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.FixedLenFeature is deprecated. Please use tf.io.FixedLenFeature instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.FixedLenFeature is deprecated. Please use tf.io.FixedLenFeature instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.parse_single_example is deprecated. Please use tf.io.parse_single_example instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.parse_single_example is deprecated. Please use tf.io.parse_single_example instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.image.resize_image_with_crop_or_pad is deprecated. Please use tf.image.resize_with_crop_or_pad instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.image.resize_image_with_crop_or_pad is deprecated. Please use tf.image.resize_with_crop_or_pad instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.random_crop is deprecated. Please use tf.image.random_crop instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.random_crop is deprecated. Please use tf.image.random_crop instead.\n",
      "\n",
      "WARNING:tensorflow:From training_script/cifar10_keras.py:144: DatasetV1.make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)`.\n",
      "WARNING:tensorflow:From training_script/cifar10_keras.py:144: DatasetV1.make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)`.\n",
      "INFO:root:configuring model\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "INFO:root:Starting training\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "Train on 128 samples, validate on 128 samples\n",
      "Epoch 1/1\n",
      "312/312 [==============================] - 89s 286ms/step - loss: 1.8295 - acc: 0.3280 - val_loss: 1.7667 - val_acc: 0.3482\n",
      "INFO:root:Test loss:1.759403645992279\n",
      "INFO:root:Test accuracy:0.34385016025641024\n",
      "WARNING:tensorflow:From training_script/cifar10_keras.py:184: The name tf.saved_model.signature_def_utils.predict_signature_def is deprecated. Please use tf.compat.v1.saved_model.signature_def_utils.predict_signature_def instead.\n",
      "\n",
      "WARNING:tensorflow:From training_script/cifar10_keras.py:184: The name tf.saved_model.signature_def_utils.predict_signature_def is deprecated. Please use tf.compat.v1.saved_model.signature_def_utils.predict_signature_def instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/saved_model/signature_def_utils_impl.py:201: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/saved_model/signature_def_utils_impl.py:201: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
      "WARNING:tensorflow:From training_script/cifar10_keras.py:187: The name tf.saved_model.builder.SavedModelBuilder is deprecated. Please use tf.compat.v1.saved_model.builder.SavedModelBuilder instead.\n",
      "\n",
      "WARNING:tensorflow:From training_script/cifar10_keras.py:187: The name tf.saved_model.builder.SavedModelBuilder is deprecated. Please use tf.compat.v1.saved_model.builder.SavedModelBuilder instead.\n",
      "\n",
      "WARNING:tensorflow:From training_script/cifar10_keras.py:190: The name tf.saved_model.tag_constants.SERVING is deprecated. Please use tf.saved_model.SERVING instead.\n",
      "\n",
      "WARNING:tensorflow:From training_script/cifar10_keras.py:190: The name tf.saved_model.tag_constants.SERVING is deprecated. Please use tf.saved_model.SERVING instead.\n",
      "\n",
      "INFO:tensorflow:No assets to save.\n",
      "INFO:tensorflow:No assets to save.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: ./logs/1/saved_model.pb\n",
      "INFO:tensorflow:SavedModel written to: ./logs/1/saved_model.pb\n",
      "INFO:root:Model successfully saved at: ./logs\n",
      "CPU times: user 1.8 s, sys: 239 ms, total: 2.04 s\n",
      "Wall time: 1min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!mkdir -p logs\n",
    "!python training_script/cifar10_keras.py --model_dir ./logs \\\n",
    "                                         --train data/train \\\n",
    "                                         --validation data/validation \\\n",
    "                                         --eval data/eval \\\n",
    "                                         --epochs 1\n",
    "!rm -rf logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color='blue'>본 스크립트는 SageMaker상의 notebook에서 구동하고 있지만, 여러분의 로컬 컴퓨터에서도 python과 jupyter notebook이 정상적으로 인스톨되어 있다면 동일하게 수행 가능합니다.</font>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use TensorFlow Script Mode\n",
    "\n",
    "TensorFlow 버전 1.11 이상에서 Amazon SageMaker Python SDK는 **스크립트 모드(Script mode)**를 지원합니다. 스크립트 모드는 종래 레거시 모드(Legacy mode) 대비 아래 장점들이 있습니다.\n",
    "\n",
    "* 스크립트 모드의 학습 스크립트는 일반적으로 TensorFlow 용으로 작성하는 학습 스크립트와 더 유사하므로 TensorFlow 학습 스크립트를 최소한의 변경으로 실행할 수 있습니다. 따라서, 기존 레거시 모드보다 TensorFlow 학습 스크립트를 수정하는 것이 더 쉽습니다. \n",
    "    - 레거시 모드는 Tensorflow Estimator API를 기반으로 한 아래의 함수들을 반드시 포함해야 합니다.\n",
    "        - 아래 함수들에서 하나의 함수를 만드시 포함해야 합니다.\n",
    "            - `model_fn`: 학습할 모델을 정의합니다,\n",
    "            - `keras_model_fn`: 학습할 tf.keras 모델을 정의합니다.\n",
    "            - `estimator_fn`: 학습할 tf.estimator.Estimator를 정의합니다.\n",
    "        - `train_input_fn`: 학습 데이터 로딩과 전처리를 수행합니다. \n",
    "        - `eval_input_fn`: 검증 데이터의 로딩과 전처리를 수행합니다.\n",
    "        - (Optional) `serving_input_fn`: 예측(prediction) 중에 모델에 전달할 feautre를 정의합니다. 이 함수는 학습시에만 사용되지만, SageMaker 엔드포인트에서 모델을 배포할 때 필요합니다.\n",
    "    - `if __name__ == “__main__”:` 블록을 정의할 수 없어 디버깅이 쉽지 않습니다.\n",
    "    \n",
    "* 스크립트 모드는 Python 2.7-와 Python 3.6-을 지원합니다.\n",
    "\n",
    "* 스크립트 모드는 **Hovorod 기반 분산 학습(distributed training)도 지원**합니다.\n",
    "\n",
    "TensorFlow 스크립트 모드에서 학습 스크립트를 작성하는 방법 및 Tensorflow 스크립트 모드의 estimator와 model 사용법에 대한 자세한 내용은\n",
    "https://sagemaker.readthedocs.io/en/stable/using_tf.html 을 참조하세요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing your script for training in SageMaker\n",
    "\n",
    "SageMaker 스크립트 모드의 학습 스크립트는 SageMaker 외부에서 실행할 수 있는 학습 스크립트와 매우 유사합니다.\n",
    "SageMaker는 하나의 인자값(argument), model_dir와 로그 및 모델 아티팩트(model artifacts)에 사용되는 S3 경로로 학습 스크립트를 실행합니다.\n",
    "\n",
    "SageMaker 학습 인스턴스에서는 학습의 컨테이너에 S3에 저장된 데이터를 다운로드하여 학습에 활용합니다. 그 때, S3 버킷의 데이터 경로와 컨테이너의 데이터 경로를 컨테이너 환경 변수를 통해 연결합니다.\n",
    "\n",
    "여러분은 다양한 환경 변수를 통해 학습 환경에 대한 유용한 속성들(properties)에 액세스할 수 있습니다.\n",
    "이 스크립트의 경우 `Train, Validation, Eval`이라는 3 개의 데이터 채널을 스크립트로 보냅니다.\n",
    "\n",
    "**`training_script/cifar10_keras.py`에서 스크립트 사본을 생성 후, `training_script/cifar10_keras_sm.py`로 저장하세요.**\n",
    "\n",
    "스크립트 사본을 생성하였다면 단계별로 아래의 작업들을 직접 시도합니다.\n",
    "\n",
    "----\n",
    "### TODO 1.\n",
    "`cifar10_keras_sm.py`파일에서 SageMaker API 환경 변수 SM_CHANNEL_TRAIN, SM_CHANNEL_VALIDATION, SM_CHANNEL_EVAL에서 디폴트 값을 가져오기 위해 train, validation, eval 인수를 수정해 주세요. \n",
    "\n",
    "`cifar10_keras_sm.py`의 `if __name__ == '__main__':` 블록 내에 아래 디폴트값을 수정해 주세요.\n",
    "\n",
    "```python\n",
    "parser.add_argument(\n",
    "        '--train',\n",
    "        type=str,\n",
    "        required=False,\n",
    "        default=os.environ.get('SM_CHANNEL_TRAIN'), # ----- 수정 부분 (default 경로 수정) ---\n",
    "        help='The directory where the CIFAR-10 input data is stored.')\n",
    "parser.add_argument(\n",
    "        '--validation',\n",
    "        type=str,\n",
    "        required=False,\n",
    "        default=os.environ.get('SM_CHANNEL_VALIDATION'), # ----- 수정 부분 (default 경로 수정) ---\n",
    "        help='The directory where the CIFAR-10 input data is stored.')\n",
    "parser.add_argument(\n",
    "        '--eval',\n",
    "        type=str,\n",
    "        required=False,\n",
    "        default=os.environ.get('SM_CHANNEL_EVAL'), # ----- 수정 부분 (default 경로 수정) ---\n",
    "        help='The directory where the CIFAR-10 input data is stored.')\n",
    "```\n",
    "\n",
    "환경 변수에 따른 S3 경로와 컨테이너 경로는 아래 표와 같습니다.\n",
    "\n",
    "|  S3 경로  |  환경 변수  |  컨테이너 경로  |\n",
    "| :---- | :---- | :----| \n",
    "|  s3://bucket_name/prefix/train  |  `SM_CHANNEL_TRAIN`  | `/opt/ml/input/data/train`  |\n",
    "|  s3://bucket_name/prefix/validation  |  `SM_CHANNEL_VALIDATION`  | `/opt/ml/input/data/validation`  |\n",
    "|  s3://bucket_name/prefix/eval  |  `SM_CHANNEL_EVAL`  | `/opt/ml/input/data/eval`  |\n",
    "|  s3://bucket_name/prefix/model.tar.gz  |  `SM_MODEL_DIR`  |  `/opt/ml/model`  |\n",
    "|  s3://bucket_name/prefix/output.tar.gz  |  `SM_OUTPUT_DATA_DIR`  |  `/opt/ml/output/data`  |\n",
    "\n",
    "얘를 들어, `/opt/ml/input/data/train`은 학습 데이터가 다운로드되는 컨테이너 내부의 디렉토리입니다.\n",
    "\n",
    "자세한 내용은 아래의 SageMaker Python SDK 문서를 확인하시기 바랍니다.<br>\n",
    "(https://sagemaker.readthedocs.io/en/stable/using_tf.html#preparing-a-script-mode-training-script)\n",
    "\n",
    "\n",
    "SageMaker는 train, validation, eval 경로들을 직접 인자로 보내지 않고, 대신 스크립트에서 환경 변수를 사용하여 해당 인자를 필요하지 않은 것으로 표시합니다.\n",
    "\n",
    "SageMaker는 유용한 환경 변수를 여러분이 작성한 학습 스크립트로 보냅니다. 예시들은 아래와 같습니다.\n",
    "* `SM_MODEL_DIR`: 학습 작업이 모델 아티팩트(model artifacts)를 저장할 수 있는 로컬 경로를 나타내는 문자열입니다. 학습 완료 후, 해당 경로 내 모델 아티팩트는 모델 호스팅을 위해 S3에 업로드됩니다. 이는 S3 위치인 학습 스크립트에 전달 된 model_dir 인수와 다르다는 점을 주의해 주세요. SM_MODEL_DIR은 항상 `/opt/ml/model`로 설정됩니다.\n",
    "* `SM_NUM_GPUS`: 호스트(Host)에서 사용 가능한 GPU 수를 나타내는 정수(integer)입니다.\n",
    "* `SM_OUTPUT_DATA_DIR`: 출력 아티팩트를 저장할 디렉토리의 경로를 나타내는 문자열입니다. 출력 아티팩트에는 체크포인트, 그래프 및 다른 저장용 파일들이 포함될 수 있지만 모델 아티팩트는 포함되지 않습니다. 이 출력 아티팩트들은 압축되어 모델 아티팩트와 동일한 접두사가 있는 S3 버킷으로 S3에 업로드됩니다.\n",
    "\n",
    "이 샘플 코드는 네트워크 지연을 줄이기 위해 모델의 체크포인트(checkpoints)를 로컬 환경에 저장합니다. 이들은 학습 종료 후 S3에 업로드할 수 있습니다.\n",
    "\n",
    "----\n",
    "### TODO 2.\n",
    "\n",
    "`cifar10_keras_sm.py`의 `if __name__ == '__main__':` 블록 내에 아래 인자값을 추가해 주세요.\n",
    "\n",
    "```python\n",
    "parser.add_argument(\n",
    "        '--model_output_dir',\n",
    "        type=str,\n",
    "        default=os.environ.get('SM_MODEL_DIR'))\n",
    "```\n",
    "\n",
    "----\n",
    "### TODO 3.\n",
    "`ModelCheckpoint` 함수의 저장 경로를 새 경로로 아래와 같이 수정해 주세요.\n",
    "\n",
    "From:\n",
    "```python\n",
    "callbacks.append(ModelCheckpoint(args.model_dir + '/checkpoint-{epoch}.h5'))\n",
    "```\n",
    "To:\n",
    "```python\n",
    "callbacks.append(ModelCheckpoint(args.model_output_dir + '/checkpoint-{epoch}.h5'))\n",
    "```\n",
    "\n",
    "----\n",
    "### TODO 4.\n",
    "`save_model` 함수의 인자값을 아래와 같이 수정해 주세요.\n",
    "\n",
    "From:  \n",
    "```python\n",
    "return save_model(model, args.model_dir)\n",
    "```\n",
    "To:  \n",
    "```python\n",
    "return save_model(model, args.model_output_dir)\n",
    "```\n",
    "\n",
    "<font color='blue'>**본 노트북 실습에 어려움이 있다면 솔루션 파일 `training_script/cifar10_keras_sm_solution.py`을 참조하시면 됩니다.**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test your script locally (just like on your laptop)\n",
    "\n",
    "테스트를 위해 위와 동일한 명령(command)으로 새 스크립트를 실행하고, 예상대로 실행되는지 확인합니다. <br>\n",
    "SageMaker TensorFlow API 호출 시에 환경 변수들은 자동으로 넘겨기지만, 로컬 주피터 노트북에서 테스트 시에는 수동으로 환경 변수들을 지정해야 합니다. (아래 예제 코드를 참조해 주세요.)\n",
    "\n",
    "```python\n",
    "%env SM_MODEL_DIR=./logs\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: SM_NUM_GPUS=1\n",
      "env: SM_MODEL_DIR=./logs\n",
      "env: SM_CHANNEL_TRAIN=data/train\n",
      "env: SM_CHANNEL_VALIDATION=data/validation\n",
      "env: SM_CHANNEL_EVAL=data/eval\n",
      "Using TensorFlow backend.\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/__init__.py:1467: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n",
      "\n",
      "WARNING:tensorflow:From training_script/cifar10_keras_sm.py:31: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "2020-02-27 13:43:37.532073: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  AVX512F\n",
      "To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2020-02-27 13:43:37.554120: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2999995000 Hz\n",
      "2020-02-27 13:43:37.554506: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55ea3c1dda60 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2020-02-27 13:43:37.554527: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2020-02-27 13:43:37.554799: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
      "WARNING:tensorflow:From training_script/cifar10_keras_sm.py:35: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n",
      "\n",
      "WARNING:tensorflow:From training_script/cifar10_keras_sm.py:35: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.\n",
      "\n",
      "INFO:root:getting data\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.FixedLenFeature is deprecated. Please use tf.io.FixedLenFeature instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.FixedLenFeature is deprecated. Please use tf.io.FixedLenFeature instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.parse_single_example is deprecated. Please use tf.io.parse_single_example instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.parse_single_example is deprecated. Please use tf.io.parse_single_example instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.image.resize_image_with_crop_or_pad is deprecated. Please use tf.image.resize_with_crop_or_pad instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.image.resize_image_with_crop_or_pad is deprecated. Please use tf.image.resize_with_crop_or_pad instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.random_crop is deprecated. Please use tf.image.random_crop instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.random_crop is deprecated. Please use tf.image.random_crop instead.\n",
      "\n",
      "WARNING:tensorflow:From training_script/cifar10_keras_sm.py:144: DatasetV1.make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)`.\n",
      "WARNING:tensorflow:From training_script/cifar10_keras_sm.py:144: DatasetV1.make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)`.\n",
      "INFO:root:configuring model\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "INFO:root:Starting training\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "Train on 128 samples, validate on 128 samples\n",
      "Epoch 1/1\n",
      "312/312 [==============================] - 89s 286ms/step - loss: 1.8757 - acc: 0.3107 - val_loss: 1.6960 - val_acc: 0.3722\n",
      "INFO:root:Test loss:1.694501823339707\n",
      "INFO:root:Test accuracy:0.3726963141025641\n",
      "WARNING:tensorflow:From training_script/cifar10_keras_sm.py:184: The name tf.saved_model.signature_def_utils.predict_signature_def is deprecated. Please use tf.compat.v1.saved_model.signature_def_utils.predict_signature_def instead.\n",
      "\n",
      "WARNING:tensorflow:From training_script/cifar10_keras_sm.py:184: The name tf.saved_model.signature_def_utils.predict_signature_def is deprecated. Please use tf.compat.v1.saved_model.signature_def_utils.predict_signature_def instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/saved_model/signature_def_utils_impl.py:201: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/saved_model/signature_def_utils_impl.py:201: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
      "WARNING:tensorflow:From training_script/cifar10_keras_sm.py:187: The name tf.saved_model.builder.SavedModelBuilder is deprecated. Please use tf.compat.v1.saved_model.builder.SavedModelBuilder instead.\n",
      "\n",
      "WARNING:tensorflow:From training_script/cifar10_keras_sm.py:187: The name tf.saved_model.builder.SavedModelBuilder is deprecated. Please use tf.compat.v1.saved_model.builder.SavedModelBuilder instead.\n",
      "\n",
      "WARNING:tensorflow:From training_script/cifar10_keras_sm.py:190: The name tf.saved_model.tag_constants.SERVING is deprecated. Please use tf.saved_model.SERVING instead.\n",
      "\n",
      "WARNING:tensorflow:From training_script/cifar10_keras_sm.py:190: The name tf.saved_model.tag_constants.SERVING is deprecated. Please use tf.saved_model.SERVING instead.\n",
      "\n",
      "INFO:tensorflow:No assets to save.\n",
      "INFO:tensorflow:No assets to save.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: ./logs/1/saved_model.pb\n",
      "INFO:tensorflow:SavedModel written to: ./logs/1/saved_model.pb\n",
      "INFO:root:Model successfully saved at: ./logs\n",
      "CPU times: user 1.67 s, sys: 216 ms, total: 1.89 s\n",
      "Wall time: 1min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!mkdir -p logs   \n",
    "\n",
    "# Number of GPUs on this machine\n",
    "%env SM_NUM_GPUS=1\n",
    "# Where to save the model\n",
    "%env SM_MODEL_DIR=./logs\n",
    "# Where the training data is\n",
    "%env SM_CHANNEL_TRAIN=data/train\n",
    "# Where the validation data is\n",
    "%env SM_CHANNEL_VALIDATION=data/validation\n",
    "# Where the evaluation data is\n",
    "%env SM_CHANNEL_EVAL=data/eval\n",
    "\n",
    "!python training_script/cifar10_keras_sm.py --model_dir ./logs --epochs 1\n",
    "\n",
    "!rm -rf logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use SageMaker local for local testing\n",
    "\n",
    "본격적으로 학습을 시작하기 전에 로컬 모드를 사용하여 디버깅을 먼저 수행합니다. 로컬 모드는 학습 인스턴스를 생성하는 과정이 없이 로컬 인스턴스로 컨테이너를 가져온 후 곧바로 학습을 수행하기 때문에 코드를 보다 신속히 검증할 수 있습니다.\n",
    "\n",
    "Amazon SageMaker Python SDK의 로컬 모드는 TensorFlow 또는 MXNet estimator서 단일 인자값을 변경하여 CPU (단일 및 다중 인스턴스) 및 GPU (단일 인스턴스) SageMaker 학습 작업을 에뮬레이션(enumlate)할 수 있습니다. 이를 위해 Docker compose와 NVIDIA Docker를 사용합니다.\n",
    "학습 작업을 시작하기 위해 `estimator.fit() ` 호출 시, Amazon ECS에서 Amazon SageMaker TensorFlow 컨테이너를 로컬 노트북 인스턴스로 다운로드합니다.\n",
    "\n",
    "로컬 모드의 학습을 통해 여러분의 코드가 현재 사용 중인 하드웨어를 적절히 활용하고 있는지 확인하기 위한 GPU 점유와 같은 지표(metric)를 쉽게 모니터링할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`sagemaker.tensorflow` 클래스를 사용하여 SageMaker Python SDK의 Tensorflow Estimator 인스턴스를 생성합니다.\n",
    "인자값으로 하이퍼파라메터와 다양한 설정들을 변경할 수 있습니다.\n",
    "\n",
    "자세한 내용은 [documentation](https://sagemaker.readthedocs.io/en/stable/using_tf.html#training-with-tensorflow-estimator)을 확인하시기 바랍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow import TensorFlow\n",
    "estimator = TensorFlow(base_job_name='cifar10',\n",
    "                       entry_point='cifar10_keras_sm.py',\n",
    "                       source_dir='training_script',\n",
    "                       role=role,\n",
    "                       framework_version='1.14.0',\n",
    "                       py_version='py3',\n",
    "                       script_mode=True,\n",
    "                       hyperparameters={'epochs' : 1},\n",
    "                       train_instance_count=1, \n",
    "                       train_instance_type='local')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습을 수행할 3개의 채널과 데이터의 경로를 지정합니다. **로컬 모드로 수행하기 때문에 S3 경로 대신 노트북 인스턴스의 경로를 지정하시면 됩니다.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating tmp8vmix1p__algo-1-98qki_1 ... \n",
      "\u001b[1BAttaching to tmp8vmix1p__algo-1-98qki_12mdone\u001b[0m\n",
      "\u001b[36malgo-1-98qki_1  |\u001b[0m /usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[36malgo-1-98qki_1  |\u001b[0m   _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "\u001b[36malgo-1-98qki_1  |\u001b[0m /usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[36malgo-1-98qki_1  |\u001b[0m   _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "\u001b[36malgo-1-98qki_1  |\u001b[0m /usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[36malgo-1-98qki_1  |\u001b[0m   _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "\u001b[36malgo-1-98qki_1  |\u001b[0m /usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[36malgo-1-98qki_1  |\u001b[0m   _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "\u001b[36malgo-1-98qki_1  |\u001b[0m /usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[36malgo-1-98qki_1  |\u001b[0m   _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "\u001b[36malgo-1-98qki_1  |\u001b[0m /usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[36malgo-1-98qki_1  |\u001b[0m   np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "\u001b[36malgo-1-98qki_1  |\u001b[0m /usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[36malgo-1-98qki_1  |\u001b[0m   _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "\u001b[36malgo-1-98qki_1  |\u001b[0m /usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[36malgo-1-98qki_1  |\u001b[0m   _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "\u001b[36malgo-1-98qki_1  |\u001b[0m /usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[36malgo-1-98qki_1  |\u001b[0m   _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "\u001b[36malgo-1-98qki_1  |\u001b[0m /usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[36malgo-1-98qki_1  |\u001b[0m   _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "\u001b[36malgo-1-98qki_1  |\u001b[0m /usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[36malgo-1-98qki_1  |\u001b[0m   _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "\u001b[36malgo-1-98qki_1  |\u001b[0m /usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[36malgo-1-98qki_1  |\u001b[0m   np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "\u001b[36malgo-1-98qki_1  |\u001b[0m Using TensorFlow backend.\n",
      "\u001b[36malgo-1-98qki_1  |\u001b[0m /usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[36malgo-1-98qki_1  |\u001b[0m   _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "\u001b[36malgo-1-98qki_1  |\u001b[0m /usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[36malgo-1-98qki_1  |\u001b[0m   _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "\u001b[36malgo-1-98qki_1  |\u001b[0m /usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[36malgo-1-98qki_1  |\u001b[0m   _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "\u001b[36malgo-1-98qki_1  |\u001b[0m /usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[36malgo-1-98qki_1  |\u001b[0m   _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "\u001b[36malgo-1-98qki_1  |\u001b[0m /usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[36malgo-1-98qki_1  |\u001b[0m   _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "\u001b[36malgo-1-98qki_1  |\u001b[0m /usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[36malgo-1-98qki_1  |\u001b[0m   np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "\u001b[36malgo-1-98qki_1  |\u001b[0m /usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[36malgo-1-98qki_1  |\u001b[0m   _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "\u001b[36malgo-1-98qki_1  |\u001b[0m /usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[36malgo-1-98qki_1  |\u001b[0m   _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "\u001b[36malgo-1-98qki_1  |\u001b[0m /usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[36malgo-1-98qki_1  |\u001b[0m   _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "\u001b[36malgo-1-98qki_1  |\u001b[0m /usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[36malgo-1-98qki_1  |\u001b[0m   _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "\u001b[36malgo-1-98qki_1  |\u001b[0m /usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[36malgo-1-98qki_1  |\u001b[0m   _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "\u001b[36malgo-1-98qki_1  |\u001b[0m /usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[36malgo-1-98qki_1  |\u001b[0m   np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "\u001b[36malgo-1-98qki_1  |\u001b[0m WARNING: Logging before flag parsing goes to stderr.\n",
      "\u001b[36malgo-1-98qki_1  |\u001b[0m W0227 13:50:38.250648 140531385243392 deprecation_wrapper.py:119] From cifar10_keras_sm.py:31: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\u001b[36malgo-1-98qki_1  |\u001b[0m \n",
      "\u001b[36malgo-1-98qki_1  |\u001b[0m W0227 13:50:38.286892 140531385243392 deprecation_wrapper.py:119] From cifar10_keras_sm.py:35: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n",
      "\u001b[36malgo-1-98qki_1  |\u001b[0m \n",
      "\u001b[36malgo-1-98qki_1  |\u001b[0m W0227 13:50:38.287060 140531385243392 deprecation_wrapper.py:119] From cifar10_keras_sm.py:35: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.\n",
      "\u001b[36malgo-1-98qki_1  |\u001b[0m \n",
      "\u001b[36malgo-1-98qki_1  |\u001b[0m I0227 13:50:38.288365 140531385243392 cifar10_keras_sm.py:198] getting data\n",
      "\u001b[36malgo-1-98qki_1  |\u001b[0m W0227 13:50:38.307203 140531385243392 deprecation_wrapper.py:119] From cifar10_keras_sm.py:167: The name tf.FixedLenFeature is deprecated. Please use tf.io.FixedLenFeature instead.\n",
      "\u001b[36malgo-1-98qki_1  |\u001b[0m \n",
      "\u001b[36malgo-1-98qki_1  |\u001b[0m W0227 13:50:38.307357 140531385243392 deprecation_wrapper.py:119] From cifar10_keras_sm.py:171: The name tf.parse_single_example is deprecated. Please use tf.io.parse_single_example instead.\n",
      "\u001b[36malgo-1-98qki_1  |\u001b[0m \n",
      "\u001b[36malgo-1-98qki_1  |\u001b[0m W0227 13:50:38.313515 140531385243392 deprecation_wrapper.py:119] From cifar10_keras_sm.py:153: The name tf.image.resize_image_with_crop_or_pad is deprecated. Please use tf.image.resize_with_crop_or_pad instead.\n",
      "\u001b[36malgo-1-98qki_1  |\u001b[0m \n",
      "\u001b[36malgo-1-98qki_1  |\u001b[0m W0227 13:50:38.321720 140531385243392 deprecation_wrapper.py:119] From cifar10_keras_sm.py:156: The name tf.random_crop is deprecated. Please use tf.image.random_crop instead.\n",
      "\u001b[36malgo-1-98qki_1  |\u001b[0m \n",
      "\u001b[36malgo-1-98qki_1  |\u001b[0m W0227 13:50:38.353573 140531385243392 deprecation.py:323] From cifar10_keras_sm.py:144: DatasetV1.make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "\u001b[36malgo-1-98qki_1  |\u001b[0m Instructions for updating:\n",
      "\u001b[36malgo-1-98qki_1  |\u001b[0m Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)`.\n",
      "\u001b[36malgo-1-98qki_1  |\u001b[0m I0227 13:50:38.486339 140531385243392 cifar10_keras_sm.py:203] configuring model\n",
      "\u001b[36malgo-1-98qki_1  |\u001b[0m W0227 13:50:38.486515 140531385243392 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\u001b[36malgo-1-98qki_1  |\u001b[0m \n",
      "\u001b[36malgo-1-98qki_1  |\u001b[0m 2020-02-27 13:50:38.555280: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\n",
      "\u001b[36malgo-1-98qki_1  |\u001b[0m W0227 13:50:38.570799 140531385243392 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\u001b[36malgo-1-98qki_1  |\u001b[0m \n",
      "\u001b[36malgo-1-98qki_1  |\u001b[0m W0227 13:50:38.680330 140531385243392 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\u001b[36malgo-1-98qki_1  |\u001b[0m \n",
      "\u001b[36malgo-1-98qki_1  |\u001b[0m W0227 13:50:38.684861 140531385243392 deprecation.py:506] From /usr/local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "\u001b[36malgo-1-98qki_1  |\u001b[0m Instructions for updating:\n",
      "\u001b[36malgo-1-98qki_1  |\u001b[0m Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "\u001b[36malgo-1-98qki_1  |\u001b[0m W0227 13:50:39.021159 140531385243392 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\u001b[36malgo-1-98qki_1  |\u001b[0m \n",
      "\u001b[36malgo-1-98qki_1  |\u001b[0m I0227 13:50:39.038145 140531385243392 cifar10_keras_sm.py:211] Starting training\n",
      "\u001b[36malgo-1-98qki_1  |\u001b[0m W0227 13:50:39.099139 140531385243392 deprecation.py:323] From /usr/local/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "\u001b[36malgo-1-98qki_1  |\u001b[0m Instructions for updating:\n",
      "\u001b[36malgo-1-98qki_1  |\u001b[0m Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "\u001b[36malgo-1-98qki_1  |\u001b[0m Train on 128 samples, validate on 128 samples\n",
      "\u001b[36malgo-1-98qki_1  |\u001b[0m Epoch 1/1\n",
      "312/312 [==============================] - 74s 238ms/step - loss: 1.8438 - acc: 0.3155 - val_loss: 1.8659 - val_acc: 0.3540\n",
      "\u001b[36malgo-1-98qki_1  |\u001b[0m I0227 13:52:04.282842 140531385243392 cifar10_keras_sm.py:220] Test loss:1.837726701528598\n",
      "\u001b[36malgo-1-98qki_1  |\u001b[0m I0227 13:52:04.283013 140531385243392 cifar10_keras_sm.py:221] Test accuracy:0.36047676282051283\n",
      "\u001b[36malgo-1-98qki_1  |\u001b[0m W0227 13:52:04.283147 140531385243392 deprecation_wrapper.py:119] From cifar10_keras_sm.py:184: The name tf.saved_model.signature_def_utils.predict_signature_def is deprecated. Please use tf.compat.v1.saved_model.signature_def_utils.predict_signature_def instead.\n",
      "\u001b[36malgo-1-98qki_1  |\u001b[0m \n",
      "\u001b[36malgo-1-98qki_1  |\u001b[0m W0227 13:52:04.283253 140531385243392 deprecation.py:323] From /usr/local/lib/python3.6/site-packages/tensorflow/python/saved_model/signature_def_utils_impl.py:201: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
      "\u001b[36malgo-1-98qki_1  |\u001b[0m Instructions for updating:\n",
      "\u001b[36malgo-1-98qki_1  |\u001b[0m This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
      "\u001b[36malgo-1-98qki_1  |\u001b[0m W0227 13:52:04.283559 140531385243392 deprecation_wrapper.py:119] From cifar10_keras_sm.py:187: The name tf.saved_model.builder.SavedModelBuilder is deprecated. Please use tf.compat.v1.saved_model.builder.SavedModelBuilder instead.\n",
      "\u001b[36malgo-1-98qki_1  |\u001b[0m \n",
      "\u001b[36malgo-1-98qki_1  |\u001b[0m W0227 13:52:04.283938 140531385243392 deprecation_wrapper.py:119] From cifar10_keras_sm.py:190: The name tf.saved_model.tag_constants.SERVING is deprecated. Please use tf.saved_model.SERVING instead.\n",
      "\u001b[36malgo-1-98qki_1  |\u001b[0m \n",
      "\u001b[36malgo-1-98qki_1  |\u001b[0m I0227 13:52:04.284210 140531385243392 builder_impl.py:636] No assets to save.\n",
      "\u001b[36malgo-1-98qki_1  |\u001b[0m I0227 13:52:04.284301 140531385243392 builder_impl.py:456] No assets to write.\n",
      "\u001b[36malgo-1-98qki_1  |\u001b[0m I0227 13:52:04.779135 140531385243392 builder_impl.py:421] SavedModel written to: /opt/ml/model/1/saved_model.pb\n",
      "\u001b[36malgo-1-98qki_1  |\u001b[0m I0227 13:52:04.779336 140531385243392 cifar10_keras_sm.py:194] Model successfully saved at: /opt/ml/model\n",
      "\u001b[36mtmp8vmix1p__algo-1-98qki_1 exited with code 0\n",
      "\u001b[0mAborting on container exit...\n",
      "===== Job Complete =====\n",
      "CPU times: user 835 ms, sys: 65.8 ms, total: 901 ms\n",
      "Wall time: 2min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "estimator.fit({'train': 'file://data/train',\n",
    "               'validation': 'file://data/validation',\n",
    "               'eval': 'file://data/eval'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimator가 처음 실행될 때 Amazon ECR 리포지토리(repository)에서 컨테이너 이미지를 다운로드해야 하지만 학습을 즉시 시작할 수 있습니다. 즉, 별도의 학습 클러스터가 프로비저닝 될 때까지 기다릴 필요가 없습니다. 또한 반복 및 테스트시 필요할 수 있는 후속 실행에서 MXNet 또는 TensorFlow 스크립트에 대한 수정 사항이 즉시 실행되기 시작합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using SageMaker for faster training time\n",
    "\n",
    "이번에는 로컬 모드를 사용하지 않고 SageMaker 학습에 GPU 학습 인스턴스를 생성하여 학습 시간을 단축해 봅니다.<br>\n",
    "로컬 모드와 다른 점들은 (1) `train_instance_type`이 로컬 모드의 ‘local’ 대신 여러분이 원하는 특정 인스턴스 유형으로 설정해야 하고, (2) 학습 데이터를 Amazon S3에 업로드 후 학습 경로를 S3 경로로 설정해야 합니다. \n",
    "\n",
    "SageMaker SDK는 S3 업로드를 위한 간단한 함수(`Session.upload_data()`)를 제공합니다. 이 함수를 통해 리턴되는 값은 데이터가 저장된 S3 경로입니다.\n",
    "좀 더 자세한 설정이 필요하다면 SageMaker SDK 대신 boto3를 사용하시면 됩니다.\n",
    "\n",
    "*[Note]: 고성능 워크로드를 위해 Amazon EFS와 Amazon FSx for Lustre도 지원하고 있습니다. 자세한 정보는 아래의 AWS 블로그를 참조해 주세요.<br>\n",
    "https://aws.amazon.com/blogs/machine-learning/speed-up-training-on-amazon-sagemaker-using-amazon-efs-or-amazon-fsx-for-lustre-file-systems/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-east-2-057716757052/data/DEMO-cifar10'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_location = sagemaker_session.upload_data(path='data', key_prefix='data/DEMO-cifar10')\n",
    "display(dataset_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "S3에 데이터 업로드를 완료했다면, Estimator를 새로 생성합니다. <br>\n",
    "아래 코드를 그대로 복사 후에 `train_instance_type='local'`을 `train_instance_type='ml.p2.xlarge'`로 수정하고\n",
    "`hyperparameters={'epochs': 1}`를 `hyperparameters={'epochs': 5}`로 수정합니다.\n",
    "\n",
    "```python\n",
    "from sagemaker.tensorflow import TensorFlow\n",
    "estimator = TensorFlow(base_job_name='cifar10',\n",
    "                       entry_point='cifar10_keras_sm.py',\n",
    "                       source_dir='training_script',\n",
    "                       role=role,\n",
    "                       framework_version='1.14.0',\n",
    "                       py_version='py3',\n",
    "                       script_mode=True,                       \n",
    "                       hyperparameters={'epochs': 1},\n",
    "                       train_instance_count=1, \n",
    "                       train_instance_type='local')\n",
    "```\n",
    "\n",
    "*[Note] \n",
    "2019년 8월부터 SageMaker에서도 학습 인스턴스에 EC2 spot instance를 사용하여 비용을 크게 절감할 수 있습니다. 자세한 정보는 아래의 AWS 블로그를 참조해 주세요.<br>\n",
    "https://aws.amazon.com/ko/blogs/korea/managed-spot-training-save-up-to-90-on-your-amazon-sagemaker-training-jobs/*\n",
    "\n",
    "만약 Managed Spot Instance로 학습하려면 다음 코드를 Estimator의 train_instance_type의 다음 행에 추가해 주세요.\n",
    "```python\n",
    "train_max_run = 3600,\n",
    "train_use_spot_instances = 'True',\n",
    "train_max_wait = 3600,\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow import TensorFlow\n",
    "estimator = TensorFlow(base_job_name='cifar10',\n",
    "                       entry_point='cifar10_keras_sm.py',\n",
    "                       source_dir='training_script',\n",
    "                       role=role,\n",
    "                       framework_version='1.14.0',\n",
    "                       py_version='py3',\n",
    "                       script_mode=True,                       \n",
    "                       hyperparameters={'epochs': 5},\n",
    "                       train_instance_count=1, \n",
    "                       train_instance_type='ml.p2.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습을 수행합니다. 이번에는 각각의 채널(`train, validation, eval`)에 S3의 데이터 저장 위치를 지정합니다.<br>\n",
    "학습 완료 후 Billable seconds도 확인해 보세요. Billable seconds는 실제로 학습 수행 시 과금되는 시간입니다.\n",
    "```\n",
    "Billable seconds: <time>\n",
    "```\n",
    "\n",
    "참고로, `ml.p2.xlarge` 인스턴스로 5 epoch 학습 시 전체 6분~7분이 소요되고, 실제 학습에 소요되는 시간은 3분~4분이 소요됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-02-27 13:55:06 Starting - Starting the training job...\n",
      "2020-02-27 13:55:08 Starting - Launching requested ML instances...\n",
      "2020-02-27 13:56:04 Starting - Preparing the instances for training.........\n",
      "2020-02-27 13:57:22 Downloading - Downloading input data...\n",
      "2020-02-27 13:58:05 Training - Downloading the training image.....\u001b[34m/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\u001b[0m\n",
      "\u001b[34mUsing TensorFlow backend.\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\u001b[0m\n",
      "\n",
      "2020-02-27 13:58:47 Training - Training image download completed. Training in progress.\u001b[34m/usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\u001b[0m\n",
      "\u001b[34mWARNING: Logging before flag parsing goes to stderr.\u001b[0m\n",
      "\u001b[34mW0227 13:58:54.103459 140443157276416 deprecation_wrapper.py:119] From cifar10_keras_sm.py:31: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\u001b[0m\n",
      "\u001b[34mW0227 13:58:54.884281 140443157276416 deprecation_wrapper.py:119] From cifar10_keras_sm.py:35: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n",
      "\u001b[0m\n",
      "\u001b[34mW0227 13:58:54.884538 140443157276416 deprecation_wrapper.py:119] From cifar10_keras_sm.py:35: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.\n",
      "\u001b[0m\n",
      "\u001b[34mI0227 13:58:54.886724 140443157276416 cifar10_keras_sm.py:198] getting data\u001b[0m\n",
      "\u001b[34mW0227 13:58:54.913781 140443157276416 deprecation_wrapper.py:119] From cifar10_keras_sm.py:167: The name tf.FixedLenFeature is deprecated. Please use tf.io.FixedLenFeature instead.\n",
      "\u001b[0m\n",
      "\u001b[34mW0227 13:58:54.913992 140443157276416 deprecation_wrapper.py:119] From cifar10_keras_sm.py:171: The name tf.parse_single_example is deprecated. Please use tf.io.parse_single_example instead.\n",
      "\u001b[0m\n",
      "\u001b[34mW0227 13:58:54.924864 140443157276416 deprecation_wrapper.py:119] From cifar10_keras_sm.py:153: The name tf.image.resize_image_with_crop_or_pad is deprecated. Please use tf.image.resize_with_crop_or_pad instead.\n",
      "\u001b[0m\n",
      "\u001b[34mW0227 13:58:54.943253 140443157276416 deprecation_wrapper.py:119] From cifar10_keras_sm.py:156: The name tf.random_crop is deprecated. Please use tf.image.random_crop instead.\n",
      "\u001b[0m\n",
      "\u001b[34mW0227 13:58:54.993260 140443157276416 deprecation.py:323] From cifar10_keras_sm.py:144: DatasetV1.make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mUse `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)`.\u001b[0m\n",
      "\u001b[34mI0227 13:58:55.223914 140443157276416 cifar10_keras_sm.py:203] configuring model\u001b[0m\n",
      "\u001b[34mW0227 13:58:55.224183 140443157276416 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\u001b[0m\n",
      "\u001b[34mW0227 13:58:56.812599 140443157276416 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\u001b[0m\n",
      "\u001b[34mW0227 13:58:56.983271 140443157276416 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\u001b[0m\n",
      "\u001b[34mW0227 13:58:56.990092 140443157276416 deprecation.py:506] From /usr/local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mPlease use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\u001b[0m\n",
      "\u001b[34mW0227 13:58:57.514341 140443157276416 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\u001b[0m\n",
      "\u001b[34mI0227 13:58:57.540437 140443157276416 cifar10_keras_sm.py:211] Starting training\u001b[0m\n",
      "\u001b[34mW0227 13:58:57.636937 140443157276416 deprecation.py:323] From /usr/local/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mUse tf.where in 2.0, which has the same broadcast rule as np.where\u001b[0m\n",
      "\u001b[34mTrain on 128 samples, validate on 128 samples\u001b[0m\n",
      "\u001b[34mEpoch 1/5\u001b[0m\n",
      "\u001b[34m  1/312 [..............................] - ETA: 45:18 - loss: 3.9412 - acc: 0.1016\n",
      "  2/312 [..............................] - ETA: 22:44 - loss: 3.8684 - acc: 0.0938\n",
      "  3/312 [..............................] - ETA: 15:13 - loss: 3.6843 - acc: 0.1120\n",
      "  4/312 [..............................] - ETA: 11:27 - loss: 3.6063 - acc: 0.1152\n",
      "  5/312 [..............................] - ETA: 9:12 - loss: 3.5816 - acc: 0.1141 \u001b[0m\n",
      "\u001b[34m  6/312 [..............................] - ETA: 7:42 - loss: 3.4656 - acc: 0.1224\n",
      "  7/312 [..............................] - ETA: 6:38 - loss: 3.3870 - acc: 0.1295\n",
      "  8/312 [..............................] - ETA: 5:50 - loss: 3.3075 - acc: 0.1309\n",
      "  9/312 [..............................] - ETA: 5:12 - loss: 3.2733 - acc: 0.1285\n",
      " 10/312 [..............................] - ETA: 4:42 - loss: 3.2002 - acc: 0.1406\n",
      " 11/312 [>.............................] - ETA: 4:17 - loss: 3.1364 - acc: 0.1449\n",
      " 12/312 [>.............................] - ETA: 3:56 - loss: 3.0847 - acc: 0.1491\n",
      " 13/312 [>.............................] - ETA: 3:39 - loss: 3.0378 - acc: 0.1496\n",
      " 14/312 [>.............................] - ETA: 3:24 - loss: 2.9983 - acc: 0.1496\n",
      " 15/312 [>.............................] - ETA: 3:11 - loss: 2.9410 - acc: 0.1542\n",
      " 16/312 [>.............................] - ETA: 2:59 - loss: 2.8954 - acc: 0.1592\n",
      " 17/312 [>.............................] - ETA: 2:49 - loss: 2.8574 - acc: 0.1622\n",
      " 18/312 [>.............................] - ETA: 2:40 - loss: 2.8224 - acc: 0.1649\n",
      " 19/312 [>.............................] - ETA: 2:32 - loss: 2.7951 - acc: 0.1649\n",
      " 20/312 [>.............................] - ETA: 2:25 - loss: 2.7715 - acc: 0.1691\n",
      " 21/312 [=>............................] - ETA: 2:18 - loss: 2.7503 - acc: 0.1682\u001b[0m\n",
      "\u001b[34m 22/312 [=>............................] - ETA: 2:13 - loss: 2.7286 - acc: 0.1694\n",
      " 23/312 [=>............................] - ETA: 2:07 - loss: 2.7053 - acc: 0.1705\n",
      " 24/312 [=>............................] - ETA: 2:02 - loss: 2.6772 - acc: 0.1742\n",
      " 25/312 [=>............................] - ETA: 1:58 - loss: 2.6570 - acc: 0.1762\n",
      " 26/312 [=>............................] - ETA: 1:53 - loss: 2.6432 - acc: 0.1764\n",
      " 27/312 [=>............................] - ETA: 1:49 - loss: 2.6221 - acc: 0.1785\n",
      " 28/312 [=>............................] - ETA: 1:46 - loss: 2.6047 - acc: 0.1802\n",
      " 29/312 [=>............................] - ETA: 1:42 - loss: 2.5889 - acc: 0.1829\n",
      " 30/312 [=>............................] - ETA: 1:39 - loss: 2.5763 - acc: 0.1833\n",
      " 31/312 [=>............................] - ETA: 1:36 - loss: 2.5583 - acc: 0.1867\n",
      " 32/312 [==>...........................] - ETA: 1:33 - loss: 2.5418 - acc: 0.1887\n",
      " 33/312 [==>...........................] - ETA: 1:31 - loss: 2.5252 - acc: 0.1901\n",
      " 34/312 [==>...........................] - ETA: 1:28 - loss: 2.5124 - acc: 0.1898\n",
      " 35/312 [==>...........................] - ETA: 1:26 - loss: 2.4965 - acc: 0.1922\n",
      " 36/312 [==>...........................] - ETA: 1:24 - loss: 2.4884 - acc: 0.1918\n",
      " 37/312 [==>...........................] - ETA: 1:22 - loss: 2.4753 - acc: 0.1936\u001b[0m\n",
      "\u001b[34m 38/312 [==>...........................] - ETA: 1:20 - loss: 2.4651 - acc: 0.1939\n",
      " 39/312 [==>...........................] - ETA: 1:18 - loss: 2.4507 - acc: 0.1981\n",
      " 40/312 [==>...........................] - ETA: 1:16 - loss: 2.4406 - acc: 0.1980\n",
      " 41/312 [==>...........................] - ETA: 1:14 - loss: 2.4275 - acc: 0.2012\n",
      " 42/312 [===>..........................] - ETA: 1:13 - loss: 2.4200 - acc: 0.2026\n",
      " 43/312 [===>..........................] - ETA: 1:11 - loss: 2.4099 - acc: 0.2040\n",
      " 44/312 [===>..........................] - ETA: 1:10 - loss: 2.3985 - acc: 0.2056\n",
      " 45/312 [===>..........................] - ETA: 1:08 - loss: 2.3877 - acc: 0.2066\n",
      " 46/312 [===>..........................] - ETA: 1:07 - loss: 2.3802 - acc: 0.2067\n",
      " 47/312 [===>..........................] - ETA: 1:05 - loss: 2.3727 - acc: 0.2086\n",
      " 48/312 [===>..........................] - ETA: 1:04 - loss: 2.3643 - acc: 0.2096\n",
      " 49/312 [===>..........................] - ETA: 1:03 - loss: 2.3564 - acc: 0.2122\n",
      " 50/312 [===>..........................] - ETA: 1:02 - loss: 2.3521 - acc: 0.2122\n",
      " 51/312 [===>..........................] - ETA: 1:01 - loss: 2.3459 - acc: 0.2142\n",
      " 52/312 [====>.........................] - ETA: 1:00 - loss: 2.3372 - acc: 0.2153\n",
      " 53/312 [====>.........................] - ETA: 58s - loss: 2.3348 - acc: 0.2155 \u001b[0m\n",
      "\u001b[34m 54/312 [====>.........................] - ETA: 57s - loss: 2.3286 - acc: 0.2159\n",
      " 55/312 [====>.........................] - ETA: 56s - loss: 2.3234 - acc: 0.2162\n",
      " 56/312 [====>.........................] - ETA: 56s - loss: 2.3175 - acc: 0.2161\n",
      " 57/312 [====>.........................] - ETA: 55s - loss: 2.3105 - acc: 0.2177\n",
      " 58/312 [====>.........................] - ETA: 54s - loss: 2.3056 - acc: 0.2183\n",
      " 59/312 [====>.........................] - ETA: 53s - loss: 2.3000 - acc: 0.2190\n",
      " 60/312 [====>.........................] - ETA: 52s - loss: 2.2926 - acc: 0.2203\n",
      " 61/312 [====>.........................] - ETA: 51s - loss: 2.2856 - acc: 0.2218\n",
      " 62/312 [====>.........................] - ETA: 50s - loss: 2.2807 - acc: 0.2229\n",
      " 63/312 [=====>........................] - ETA: 50s - loss: 2.2733 - acc: 0.2240\n",
      " 64/312 [=====>........................] - ETA: 49s - loss: 2.2696 - acc: 0.2249\n",
      " 65/312 [=====>........................] - ETA: 48s - loss: 2.2636 - acc: 0.2254\n",
      " 66/312 [=====>........................] - ETA: 48s - loss: 2.2592 - acc: 0.2257\n",
      " 67/312 [=====>........................] - ETA: 47s - loss: 2.2530 - acc: 0.2270\n",
      " 68/312 [=====>........................] - ETA: 46s - loss: 2.2455 - acc: 0.2293\u001b[0m\n",
      "\u001b[34m 69/312 [=====>........................] - ETA: 46s - loss: 2.2415 - acc: 0.2295\n",
      " 70/312 [=====>........................] - ETA: 45s - loss: 2.2371 - acc: 0.2301\n",
      " 71/312 [=====>........................] - ETA: 44s - loss: 2.2338 - acc: 0.2298\n",
      " 72/312 [=====>........................] - ETA: 44s - loss: 2.2291 - acc: 0.2296\n",
      " 73/312 [======>.......................] - ETA: 43s - loss: 2.2251 - acc: 0.2301\n",
      " 74/312 [======>.......................] - ETA: 43s - loss: 2.2218 - acc: 0.2306\n",
      " 75/312 [======>.......................] - ETA: 42s - loss: 2.2193 - acc: 0.2306\n",
      " 76/312 [======>.......................] - ETA: 42s - loss: 2.2163 - acc: 0.2305\n",
      " 77/312 [======>.......................] - ETA: 41s - loss: 2.2112 - acc: 0.2309\n",
      " 78/312 [======>.......................] - ETA: 40s - loss: 2.2053 - acc: 0.2325\n",
      " 79/312 [======>.......................] - ETA: 40s - loss: 2.2008 - acc: 0.2335\n",
      " 80/312 [======>.......................] - ETA: 39s - loss: 2.1962 - acc: 0.2340\n",
      " 81/312 [======>.......................] - ETA: 39s - loss: 2.1931 - acc: 0.2347\n",
      " 82/312 [======>.......................] - ETA: 38s - loss: 2.1903 - acc: 0.2341\n",
      " 83/312 [======>.......................] - ETA: 38s - loss: 2.1871 - acc: 0.2348\n",
      " 84/312 [=======>......................] - ETA: 38s - loss: 2.1831 - acc: 0.2357\u001b[0m\n",
      "\u001b[34m 85/312 [=======>......................] - ETA: 37s - loss: 2.1800 - acc: 0.2363\n",
      " 86/312 [=======>......................] - ETA: 37s - loss: 2.1753 - acc: 0.2365\n",
      " 87/312 [=======>......................] - ETA: 36s - loss: 2.1714 - acc: 0.2364\n",
      " 88/312 [=======>......................] - ETA: 36s - loss: 2.1677 - acc: 0.2370\n",
      " 89/312 [=======>......................] - ETA: 36s - loss: 2.1637 - acc: 0.2372\n",
      " 90/312 [=======>......................] - ETA: 35s - loss: 2.1615 - acc: 0.2374\n",
      " 91/312 [=======>......................] - ETA: 35s - loss: 2.1587 - acc: 0.2381\n",
      " 92/312 [=======>......................] - ETA: 34s - loss: 2.1553 - acc: 0.2385\n",
      " 93/312 [=======>......................] - ETA: 34s - loss: 2.1516 - acc: 0.2393\n",
      " 94/312 [========>.....................] - ETA: 34s - loss: 2.1485 - acc: 0.2404\n",
      " 95/312 [========>.....................] - ETA: 33s - loss: 2.1463 - acc: 0.2405\n",
      " 96/312 [========>.....................] - ETA: 33s - loss: 2.1430 - acc: 0.2409\n",
      " 97/312 [========>.....................] - ETA: 32s - loss: 2.1398 - acc: 0.2416\n",
      " 98/312 [========>.....................] - ETA: 32s - loss: 2.1388 - acc: 0.2413\n",
      " 99/312 [========>.....................] - ETA: 32s - loss: 2.1367 - acc: 0.2413\u001b[0m\n",
      "\u001b[34m100/312 [========>.....................] - ETA: 31s - loss: 2.1340 - acc: 0.2420\u001b[0m\n",
      "\u001b[34m101/312 [========>.....................] - ETA: 31s - loss: 2.1306 - acc: 0.2426\u001b[0m\n",
      "\u001b[34m102/312 [========>.....................] - ETA: 31s - loss: 2.1284 - acc: 0.2433\u001b[0m\n",
      "\u001b[34m103/312 [========>.....................] - ETA: 30s - loss: 2.1263 - acc: 0.2441\u001b[0m\n",
      "\u001b[34m104/312 [=========>....................] - ETA: 30s - loss: 2.1224 - acc: 0.2450\u001b[0m\n",
      "\u001b[34m105/312 [=========>....................] - ETA: 30s - loss: 2.1207 - acc: 0.2455\u001b[0m\n",
      "\u001b[34m106/312 [=========>....................] - ETA: 30s - loss: 2.1188 - acc: 0.2457\u001b[0m\n",
      "\u001b[34m107/312 [=========>....................] - ETA: 29s - loss: 2.1163 - acc: 0.2463\u001b[0m\n",
      "\u001b[34m108/312 [=========>....................] - ETA: 29s - loss: 2.1133 - acc: 0.2469\u001b[0m\n",
      "\u001b[34m109/312 [=========>....................] - ETA: 29s - loss: 2.1106 - acc: 0.2474\u001b[0m\n",
      "\u001b[34m110/312 [=========>....................] - ETA: 28s - loss: 2.1079 - acc: 0.2479\u001b[0m\n",
      "\u001b[34m111/312 [=========>....................] - ETA: 28s - loss: 2.1043 - acc: 0.2485\u001b[0m\n",
      "\u001b[34m112/312 [=========>....................] - ETA: 28s - loss: 2.1019 - acc: 0.2487\u001b[0m\n",
      "\u001b[34m113/312 [=========>....................] - ETA: 27s - loss: 2.0999 - acc: 0.2494\u001b[0m\n",
      "\u001b[34m114/312 [=========>....................] - ETA: 27s - loss: 2.0978 - acc: 0.2503\u001b[0m\n",
      "\u001b[34m115/312 [==========>...................] - ETA: 27s - loss: 2.0958 - acc: 0.2504\u001b[0m\n",
      "\u001b[34m116/312 [==========>...................] - ETA: 27s - loss: 2.0937 - acc: 0.2510\u001b[0m\n",
      "\u001b[34m117/312 [==========>...................] - ETA: 26s - loss: 2.0910 - acc: 0.2516\u001b[0m\n",
      "\u001b[34m118/312 [==========>...................] - ETA: 26s - loss: 2.0893 - acc: 0.2520\u001b[0m\n",
      "\u001b[34m119/312 [==========>...................] - ETA: 26s - loss: 2.0872 - acc: 0.2522\u001b[0m\n",
      "\u001b[34m120/312 [==========>...................] - ETA: 26s - loss: 2.0852 - acc: 0.2524\u001b[0m\n",
      "\u001b[34m121/312 [==========>...................] - ETA: 25s - loss: 2.0827 - acc: 0.2527\u001b[0m\n",
      "\u001b[34m122/312 [==========>...................] - ETA: 25s - loss: 2.0793 - acc: 0.2539\u001b[0m\n",
      "\u001b[34m123/312 [==========>...................] - ETA: 25s - loss: 2.0784 - acc: 0.2541\u001b[0m\n",
      "\u001b[34m124/312 [==========>...................] - ETA: 25s - loss: 2.0760 - acc: 0.2545\u001b[0m\n",
      "\u001b[34m125/312 [===========>..................] - ETA: 24s - loss: 2.0747 - acc: 0.2541\u001b[0m\n",
      "\u001b[34m126/312 [===========>..................] - ETA: 24s - loss: 2.0723 - acc: 0.2545\u001b[0m\n",
      "\u001b[34m127/312 [===========>..................] - ETA: 24s - loss: 2.0705 - acc: 0.2549\u001b[0m\n",
      "\u001b[34m128/312 [===========>..................] - ETA: 24s - loss: 2.0686 - acc: 0.2552\u001b[0m\n",
      "\u001b[34m129/312 [===========>..................] - ETA: 23s - loss: 2.0664 - acc: 0.2559\u001b[0m\n",
      "\u001b[34m130/312 [===========>..................] - ETA: 23s - loss: 2.0648 - acc: 0.2562\u001b[0m\n",
      "\u001b[34m131/312 [===========>..................] - ETA: 23s - loss: 2.0626 - acc: 0.2570\u001b[0m\n",
      "\u001b[34m132/312 [===========>..................] - ETA: 23s - loss: 2.0615 - acc: 0.2573\u001b[0m\n",
      "\u001b[34m133/312 [===========>..................] - ETA: 23s - loss: 2.0594 - acc: 0.2578\u001b[0m\n",
      "\u001b[34m134/312 [===========>..................] - ETA: 22s - loss: 2.0579 - acc: 0.2580\u001b[0m\n",
      "\u001b[34m135/312 [===========>..................] - ETA: 22s - loss: 2.0554 - acc: 0.2587\u001b[0m\n",
      "\u001b[34m136/312 [============>.................] - ETA: 22s - loss: 2.0535 - acc: 0.2587\u001b[0m\n",
      "\u001b[34m137/312 [============>.................] - ETA: 22s - loss: 2.0514 - acc: 0.2594\u001b[0m\n",
      "\u001b[34m138/312 [============>.................] - ETA: 22s - loss: 2.0501 - acc: 0.2596\u001b[0m\n",
      "\u001b[34m139/312 [============>.................] - ETA: 21s - loss: 2.0486 - acc: 0.2599\u001b[0m\n",
      "\u001b[34m140/312 [============>.................] - ETA: 21s - loss: 2.0466 - acc: 0.2604\u001b[0m\n",
      "\u001b[34m141/312 [============>.................] - ETA: 21s - loss: 2.0443 - acc: 0.2611\u001b[0m\n",
      "\u001b[34m142/312 [============>.................] - ETA: 21s - loss: 2.0430 - acc: 0.2616\u001b[0m\n",
      "\u001b[34m143/312 [============>.................] - ETA: 21s - loss: 2.0404 - acc: 0.2624\u001b[0m\n",
      "\u001b[34m144/312 [============>.................] - ETA: 20s - loss: 2.0390 - acc: 0.2631\u001b[0m\n",
      "\u001b[34m145/312 [============>.................] - ETA: 20s - loss: 2.0373 - acc: 0.2638\u001b[0m\n",
      "\u001b[34m146/312 [=============>................] - ETA: 20s - loss: 2.0358 - acc: 0.2641\u001b[0m\n",
      "\u001b[34m147/312 [=============>................] - ETA: 20s - loss: 2.0328 - acc: 0.2652\u001b[0m\n",
      "\u001b[34m148/312 [=============>................] - ETA: 20s - loss: 2.0307 - acc: 0.2658\u001b[0m\n",
      "\u001b[34m149/312 [=============>................] - ETA: 19s - loss: 2.0284 - acc: 0.2665\u001b[0m\n",
      "\u001b[34m150/312 [=============>................] - ETA: 19s - loss: 2.0268 - acc: 0.2670\u001b[0m\n",
      "\u001b[34m151/312 [=============>................] - ETA: 19s - loss: 2.0247 - acc: 0.2676\u001b[0m\n",
      "\u001b[34m152/312 [=============>................] - ETA: 19s - loss: 2.0232 - acc: 0.2681\u001b[0m\n",
      "\u001b[34m153/312 [=============>................] - ETA: 19s - loss: 2.0213 - acc: 0.2685\u001b[0m\n",
      "\u001b[34m154/312 [=============>................] - ETA: 18s - loss: 2.0181 - acc: 0.2694\u001b[0m\n",
      "\u001b[34m155/312 [=============>................] - ETA: 18s - loss: 2.0162 - acc: 0.2696\u001b[0m\n",
      "\u001b[34m156/312 [==============>...............] - ETA: 18s - loss: 2.0150 - acc: 0.2699\u001b[0m\n",
      "\u001b[34m157/312 [==============>...............] - ETA: 18s - loss: 2.0134 - acc: 0.2705\u001b[0m\n",
      "\u001b[34m158/312 [==============>...............] - ETA: 18s - loss: 2.0120 - acc: 0.2706\u001b[0m\n",
      "\u001b[34m159/312 [==============>...............] - ETA: 18s - loss: 2.0115 - acc: 0.2707\u001b[0m\n",
      "\u001b[34m160/312 [==============>...............] - ETA: 17s - loss: 2.0099 - acc: 0.2714\u001b[0m\n",
      "\u001b[34m161/312 [==============>...............] - ETA: 17s - loss: 2.0088 - acc: 0.2717\u001b[0m\n",
      "\u001b[34m162/312 [==============>...............] - ETA: 17s - loss: 2.0071 - acc: 0.2719\u001b[0m\n",
      "\u001b[34m163/312 [==============>...............] - ETA: 17s - loss: 2.0055 - acc: 0.2723\u001b[0m\n",
      "\u001b[34m164/312 [==============>...............] - ETA: 17s - loss: 2.0039 - acc: 0.2726\u001b[0m\n",
      "\u001b[34m165/312 [==============>...............] - ETA: 17s - loss: 2.0027 - acc: 0.2725\u001b[0m\n",
      "\u001b[34m166/312 [==============>...............] - ETA: 16s - loss: 2.0003 - acc: 0.2732\u001b[0m\n",
      "\u001b[34m167/312 [===============>..............] - ETA: 16s - loss: 1.9984 - acc: 0.2736\u001b[0m\n",
      "\u001b[34m168/312 [===============>..............] - ETA: 16s - loss: 1.9967 - acc: 0.2742\u001b[0m\n",
      "\u001b[34m169/312 [===============>..............] - ETA: 16s - loss: 1.9960 - acc: 0.2743\u001b[0m\n",
      "\u001b[34m170/312 [===============>..............] - ETA: 16s - loss: 1.9949 - acc: 0.2742\u001b[0m\n",
      "\u001b[34m171/312 [===============>..............] - ETA: 16s - loss: 1.9934 - acc: 0.2744\u001b[0m\n",
      "\u001b[34m172/312 [===============>..............] - ETA: 15s - loss: 1.9921 - acc: 0.2743\u001b[0m\n",
      "\u001b[34m173/312 [===============>..............] - ETA: 15s - loss: 1.9902 - acc: 0.2747\u001b[0m\n",
      "\u001b[34m174/312 [===============>..............] - ETA: 15s - loss: 1.9886 - acc: 0.2751\u001b[0m\n",
      "\u001b[34m175/312 [===============>..............] - ETA: 15s - loss: 1.9878 - acc: 0.2759\u001b[0m\n",
      "\u001b[34m176/312 [===============>..............] - ETA: 15s - loss: 1.9867 - acc: 0.2766\u001b[0m\n",
      "\u001b[34m177/312 [================>.............] - ETA: 15s - loss: 1.9858 - acc: 0.2770\u001b[0m\n",
      "\u001b[34m178/312 [================>.............] - ETA: 15s - loss: 1.9842 - acc: 0.2771\u001b[0m\n",
      "\u001b[34m179/312 [================>.............] - ETA: 14s - loss: 1.9822 - acc: 0.2777\u001b[0m\n",
      "\u001b[34m180/312 [================>.............] - ETA: 14s - loss: 1.9809 - acc: 0.2782\u001b[0m\n",
      "\u001b[34m181/312 [================>.............] - ETA: 14s - loss: 1.9793 - acc: 0.2790\u001b[0m\n",
      "\u001b[34m182/312 [================>.............] - ETA: 14s - loss: 1.9774 - acc: 0.2797\u001b[0m\n",
      "\u001b[34m183/312 [================>.............] - ETA: 14s - loss: 1.9758 - acc: 0.2798\u001b[0m\n",
      "\u001b[34m184/312 [================>.............] - ETA: 14s - loss: 1.9749 - acc: 0.2800\u001b[0m\n",
      "\u001b[34m185/312 [================>.............] - ETA: 14s - loss: 1.9734 - acc: 0.2804\u001b[0m\n",
      "\u001b[34m186/312 [================>.............] - ETA: 13s - loss: 1.9718 - acc: 0.2807\u001b[0m\n",
      "\u001b[34m187/312 [================>.............] - ETA: 13s - loss: 1.9705 - acc: 0.2812\u001b[0m\n",
      "\u001b[34m188/312 [=================>............] - ETA: 13s - loss: 1.9689 - acc: 0.2814\u001b[0m\n",
      "\u001b[34m189/312 [=================>............] - ETA: 13s - loss: 1.9673 - acc: 0.2820\u001b[0m\n",
      "\u001b[34m190/312 [=================>............] - ETA: 13s - loss: 1.9656 - acc: 0.2823\u001b[0m\n",
      "\u001b[34m191/312 [=================>............] - ETA: 13s - loss: 1.9642 - acc: 0.2825\u001b[0m\n",
      "\u001b[34m192/312 [=================>............] - ETA: 13s - loss: 1.9627 - acc: 0.2829\u001b[0m\n",
      "\u001b[34m193/312 [=================>............] - ETA: 12s - loss: 1.9618 - acc: 0.2833\u001b[0m\n",
      "\u001b[34m194/312 [=================>............] - ETA: 12s - loss: 1.9600 - acc: 0.2840\u001b[0m\n",
      "\u001b[34m195/312 [=================>............] - ETA: 12s - loss: 1.9588 - acc: 0.2843\u001b[0m\n",
      "\u001b[34m196/312 [=================>............] - ETA: 12s - loss: 1.9590 - acc: 0.2841\u001b[0m\n",
      "\u001b[34m197/312 [=================>............] - ETA: 12s - loss: 1.9576 - acc: 0.2843\u001b[0m\n",
      "\u001b[34m198/312 [==================>...........] - ETA: 12s - loss: 1.9566 - acc: 0.2846\u001b[0m\n",
      "\u001b[34m199/312 [==================>...........] - ETA: 12s - loss: 1.9558 - acc: 0.2850\u001b[0m\n",
      "\u001b[34m200/312 [==================>...........] - ETA: 11s - loss: 1.9539 - acc: 0.2855\u001b[0m\n",
      "\u001b[34m201/312 [==================>...........] - ETA: 11s - loss: 1.9526 - acc: 0.2860\u001b[0m\n",
      "\u001b[34m202/312 [==================>...........] - ETA: 11s - loss: 1.9514 - acc: 0.2859\u001b[0m\n",
      "\u001b[34m203/312 [==================>...........] - ETA: 11s - loss: 1.9502 - acc: 0.2862\u001b[0m\n",
      "\u001b[34m204/312 [==================>...........] - ETA: 11s - loss: 1.9491 - acc: 0.2864\u001b[0m\n",
      "\u001b[34m205/312 [==================>...........] - ETA: 11s - loss: 1.9483 - acc: 0.2868\u001b[0m\n",
      "\u001b[34m206/312 [==================>...........] - ETA: 11s - loss: 1.9477 - acc: 0.2869\u001b[0m\n",
      "\u001b[34m207/312 [==================>...........] - ETA: 11s - loss: 1.9461 - acc: 0.2876\u001b[0m\n",
      "\u001b[34m208/312 [===================>..........] - ETA: 10s - loss: 1.9442 - acc: 0.2883\u001b[0m\n",
      "\u001b[34m209/312 [===================>..........] - ETA: 10s - loss: 1.9425 - acc: 0.2890\u001b[0m\n",
      "\u001b[34m210/312 [===================>..........] - ETA: 10s - loss: 1.9407 - acc: 0.2894\u001b[0m\n",
      "\u001b[34m211/312 [===================>..........] - ETA: 10s - loss: 1.9389 - acc: 0.2898\u001b[0m\n",
      "\u001b[34m212/312 [===================>..........] - ETA: 10s - loss: 1.9372 - acc: 0.2904\u001b[0m\n",
      "\u001b[34m213/312 [===================>..........] - ETA: 10s - loss: 1.9356 - acc: 0.2910\u001b[0m\n",
      "\u001b[34m214/312 [===================>..........] - ETA: 10s - loss: 1.9343 - acc: 0.2914\u001b[0m\n",
      "\u001b[34m215/312 [===================>..........] - ETA: 10s - loss: 1.9328 - acc: 0.2918\u001b[0m\n",
      "\u001b[34m216/312 [===================>..........] - ETA: 9s - loss: 1.9314 - acc: 0.2922 \u001b[0m\n",
      "\u001b[34m217/312 [===================>..........] - ETA: 9s - loss: 1.9302 - acc: 0.2926\u001b[0m\n",
      "\u001b[34m218/312 [===================>..........] - ETA: 9s - loss: 1.9288 - acc: 0.2929\u001b[0m\n",
      "\u001b[34m219/312 [====================>.........] - ETA: 9s - loss: 1.9278 - acc: 0.2932\u001b[0m\n",
      "\u001b[34m220/312 [====================>.........] - ETA: 9s - loss: 1.9265 - acc: 0.2936\u001b[0m\n",
      "\u001b[34m221/312 [====================>.........] - ETA: 9s - loss: 1.9249 - acc: 0.2940\u001b[0m\n",
      "\u001b[34m222/312 [====================>.........] - ETA: 9s - loss: 1.9236 - acc: 0.2946\u001b[0m\n",
      "\u001b[34m223/312 [====================>.........] - ETA: 9s - loss: 1.9228 - acc: 0.2949\u001b[0m\n",
      "\u001b[34m224/312 [====================>.........] - ETA: 9s - loss: 1.9211 - acc: 0.2955\u001b[0m\n",
      "\u001b[34m225/312 [====================>.........] - ETA: 8s - loss: 1.9195 - acc: 0.2960\u001b[0m\n",
      "\u001b[34m226/312 [====================>.........] - ETA: 8s - loss: 1.9180 - acc: 0.2965\u001b[0m\n",
      "\u001b[34m227/312 [====================>.........] - ETA: 8s - loss: 1.9163 - acc: 0.2972\u001b[0m\n",
      "\u001b[34m228/312 [====================>.........] - ETA: 8s - loss: 1.9149 - acc: 0.2975\u001b[0m\n",
      "\u001b[34m229/312 [=====================>........] - ETA: 8s - loss: 1.9133 - acc: 0.2978\u001b[0m\n",
      "\u001b[34m230/312 [=====================>........] - ETA: 8s - loss: 1.9121 - acc: 0.2982\u001b[0m\n",
      "\u001b[34m231/312 [=====================>........] - ETA: 8s - loss: 1.9118 - acc: 0.2988\u001b[0m\n",
      "\u001b[34m232/312 [=====================>........] - ETA: 8s - loss: 1.9102 - acc: 0.2993\u001b[0m\n",
      "\u001b[34m233/312 [=====================>........] - ETA: 7s - loss: 1.9087 - acc: 0.2998\u001b[0m\n",
      "\u001b[34m234/312 [=====================>........] - ETA: 7s - loss: 1.9080 - acc: 0.3001\u001b[0m\n",
      "\u001b[34m235/312 [=====================>........] - ETA: 7s - loss: 1.9065 - acc: 0.3008\u001b[0m\n",
      "\u001b[34m236/312 [=====================>........] - ETA: 7s - loss: 1.9057 - acc: 0.3010\u001b[0m\n",
      "\u001b[34m237/312 [=====================>........] - ETA: 7s - loss: 1.9048 - acc: 0.3012\u001b[0m\n",
      "\u001b[34m238/312 [=====================>........] - ETA: 7s - loss: 1.9035 - acc: 0.3018\u001b[0m\n",
      "\u001b[34m239/312 [=====================>........] - ETA: 7s - loss: 1.9022 - acc: 0.3023\u001b[0m\n",
      "\u001b[34m240/312 [======================>.......] - ETA: 7s - loss: 1.9012 - acc: 0.3026\u001b[0m\n",
      "\u001b[34m241/312 [======================>.......] - ETA: 7s - loss: 1.9003 - acc: 0.3029\u001b[0m\n",
      "\u001b[34m242/312 [======================>.......] - ETA: 6s - loss: 1.8991 - acc: 0.3033\u001b[0m\n",
      "\u001b[34m243/312 [======================>.......] - ETA: 6s - loss: 1.8982 - acc: 0.3035\u001b[0m\n",
      "\u001b[34m244/312 [======================>.......] - ETA: 6s - loss: 1.8975 - acc: 0.3037\u001b[0m\n",
      "\u001b[34m245/312 [======================>.......] - ETA: 6s - loss: 1.8957 - acc: 0.3044\u001b[0m\n",
      "\u001b[34m246/312 [======================>.......] - ETA: 6s - loss: 1.8950 - acc: 0.3046\u001b[0m\n",
      "\u001b[34m247/312 [======================>.......] - ETA: 6s - loss: 1.8940 - acc: 0.3050\u001b[0m\n",
      "\u001b[34m248/312 [======================>.......] - ETA: 6s - loss: 1.8936 - acc: 0.3051\u001b[0m\n",
      "\u001b[34m249/312 [======================>.......] - ETA: 6s - loss: 1.8918 - acc: 0.3057\u001b[0m\n",
      "\u001b[34m250/312 [=======================>......] - ETA: 6s - loss: 1.8904 - acc: 0.3060\u001b[0m\n",
      "\u001b[34m251/312 [=======================>......] - ETA: 6s - loss: 1.8893 - acc: 0.3061\u001b[0m\n",
      "\u001b[34m252/312 [=======================>......] - ETA: 5s - loss: 1.8881 - acc: 0.3066\u001b[0m\n",
      "\u001b[34m253/312 [=======================>......] - ETA: 5s - loss: 1.8875 - acc: 0.3068\u001b[0m\n",
      "\u001b[34m254/312 [=======================>......] - ETA: 5s - loss: 1.8861 - acc: 0.3074\u001b[0m\n",
      "\u001b[34m255/312 [=======================>......] - ETA: 5s - loss: 1.8851 - acc: 0.3077\u001b[0m\n",
      "\u001b[34m256/312 [=======================>......] - ETA: 5s - loss: 1.8843 - acc: 0.3080\u001b[0m\n",
      "\u001b[34m257/312 [=======================>......] - ETA: 5s - loss: 1.8836 - acc: 0.3083\u001b[0m\n",
      "\u001b[34m258/312 [=======================>......] - ETA: 5s - loss: 1.8827 - acc: 0.3087\u001b[0m\n",
      "\u001b[34m259/312 [=======================>......] - ETA: 5s - loss: 1.8820 - acc: 0.3087\u001b[0m\n",
      "\u001b[34m260/312 [========================>.....] - ETA: 5s - loss: 1.8809 - acc: 0.3089\u001b[0m\n",
      "\u001b[34m261/312 [========================>.....] - ETA: 4s - loss: 1.8802 - acc: 0.3091\u001b[0m\n",
      "\u001b[34m262/312 [========================>.....] - ETA: 4s - loss: 1.8796 - acc: 0.3092\u001b[0m\n",
      "\u001b[34m263/312 [========================>.....] - ETA: 4s - loss: 1.8784 - acc: 0.3099\u001b[0m\n",
      "\u001b[34m264/312 [========================>.....] - ETA: 4s - loss: 1.8770 - acc: 0.3104\u001b[0m\n",
      "\u001b[34m265/312 [========================>.....] - ETA: 4s - loss: 1.8758 - acc: 0.3111\u001b[0m\n",
      "\u001b[34m266/312 [========================>.....] - ETA: 4s - loss: 1.8746 - acc: 0.3115\u001b[0m\n",
      "\u001b[34m267/312 [========================>.....] - ETA: 4s - loss: 1.8734 - acc: 0.3119\u001b[0m\n",
      "\u001b[34m268/312 [========================>.....] - ETA: 4s - loss: 1.8722 - acc: 0.3124\u001b[0m\n",
      "\u001b[34m269/312 [========================>.....] - ETA: 4s - loss: 1.8710 - acc: 0.3127\u001b[0m\n",
      "\u001b[34m270/312 [========================>.....] - ETA: 4s - loss: 1.8701 - acc: 0.3130\u001b[0m\n",
      "\u001b[34m271/312 [=========================>....] - ETA: 3s - loss: 1.8687 - acc: 0.3133\u001b[0m\n",
      "\u001b[34m272/312 [=========================>....] - ETA: 3s - loss: 1.8673 - acc: 0.3137\u001b[0m\n",
      "\u001b[34m273/312 [=========================>....] - ETA: 3s - loss: 1.8666 - acc: 0.3140\u001b[0m\n",
      "\u001b[34m274/312 [=========================>....] - ETA: 3s - loss: 1.8660 - acc: 0.3142\u001b[0m\n",
      "\u001b[34m275/312 [=========================>....] - ETA: 3s - loss: 1.8650 - acc: 0.3147\u001b[0m\n",
      "\u001b[34m276/312 [=========================>....] - ETA: 3s - loss: 1.8645 - acc: 0.3150\u001b[0m\n",
      "\u001b[34m277/312 [=========================>....] - ETA: 3s - loss: 1.8635 - acc: 0.3154\u001b[0m\n",
      "\u001b[34m278/312 [=========================>....] - ETA: 3s - loss: 1.8618 - acc: 0.3161\u001b[0m\n",
      "\u001b[34m279/312 [=========================>....] - ETA: 3s - loss: 1.8603 - acc: 0.3166\u001b[0m\n",
      "\u001b[34m280/312 [=========================>....] - ETA: 3s - loss: 1.8593 - acc: 0.3170\u001b[0m\n",
      "\u001b[34m281/312 [==========================>...] - ETA: 2s - loss: 1.8584 - acc: 0.3171\u001b[0m\n",
      "\u001b[34m282/312 [==========================>...] - ETA: 2s - loss: 1.8575 - acc: 0.3174\u001b[0m\n",
      "\u001b[34m283/312 [==========================>...] - ETA: 2s - loss: 1.8567 - acc: 0.3178\u001b[0m\n",
      "\u001b[34m284/312 [==========================>...] - ETA: 2s - loss: 1.8561 - acc: 0.3181\u001b[0m\n",
      "\u001b[34m285/312 [==========================>...] - ETA: 2s - loss: 1.8549 - acc: 0.3187\u001b[0m\n",
      "\u001b[34m286/312 [==========================>...] - ETA: 2s - loss: 1.8543 - acc: 0.3188\u001b[0m\n",
      "\u001b[34m287/312 [==========================>...] - ETA: 2s - loss: 1.8536 - acc: 0.3191\u001b[0m\n",
      "\u001b[34m288/312 [==========================>...] - ETA: 2s - loss: 1.8531 - acc: 0.3193\u001b[0m\n",
      "\u001b[34m289/312 [==========================>...] - ETA: 2s - loss: 1.8520 - acc: 0.3195\u001b[0m\n",
      "\u001b[34m290/312 [==========================>...] - ETA: 2s - loss: 1.8511 - acc: 0.3196\u001b[0m\n",
      "\u001b[34m291/312 [==========================>...] - ETA: 1s - loss: 1.8500 - acc: 0.3199\u001b[0m\n",
      "\u001b[34m292/312 [===========================>..] - ETA: 1s - loss: 1.8490 - acc: 0.3204\u001b[0m\n",
      "\u001b[34m293/312 [===========================>..] - ETA: 1s - loss: 1.8484 - acc: 0.3206\u001b[0m\n",
      "\u001b[34m294/312 [===========================>..] - ETA: 1s - loss: 1.8472 - acc: 0.3210\u001b[0m\n",
      "\u001b[34m295/312 [===========================>..] - ETA: 1s - loss: 1.8467 - acc: 0.3212\u001b[0m\n",
      "\u001b[34m296/312 [===========================>..] - ETA: 1s - loss: 1.8458 - acc: 0.3216\u001b[0m\n",
      "\u001b[34m297/312 [===========================>..] - ETA: 1s - loss: 1.8449 - acc: 0.3221\u001b[0m\n",
      "\u001b[34m298/312 [===========================>..] - ETA: 1s - loss: 1.8439 - acc: 0.3223\u001b[0m\n",
      "\u001b[34m299/312 [===========================>..] - ETA: 1s - loss: 1.8430 - acc: 0.3227\u001b[0m\n",
      "\u001b[34m300/312 [===========================>..] - ETA: 1s - loss: 1.8421 - acc: 0.3230\u001b[0m\n",
      "\u001b[34m301/312 [===========================>..] - ETA: 1s - loss: 1.8413 - acc: 0.3233\u001b[0m\n",
      "\u001b[34m302/312 [============================>.] - ETA: 0s - loss: 1.8404 - acc: 0.3239\u001b[0m\n",
      "\u001b[34m303/312 [============================>.] - ETA: 0s - loss: 1.8396 - acc: 0.3241\u001b[0m\n",
      "\u001b[34m304/312 [============================>.] - ETA: 0s - loss: 1.8388 - acc: 0.3246\u001b[0m\n",
      "\u001b[34m305/312 [============================>.] - ETA: 0s - loss: 1.8380 - acc: 0.3246\u001b[0m\n",
      "\u001b[34m306/312 [============================>.] - ETA: 0s - loss: 1.8365 - acc: 0.3251\u001b[0m\n",
      "\u001b[34m307/312 [============================>.] - ETA: 0s - loss: 1.8356 - acc: 0.3254\u001b[0m\n",
      "\u001b[34m308/312 [============================>.] - ETA: 0s - loss: 1.8344 - acc: 0.3258\u001b[0m\n",
      "\u001b[34m309/312 [============================>.] - ETA: 0s - loss: 1.8337 - acc: 0.3261\u001b[0m\n",
      "\u001b[34m310/312 [============================>.] - ETA: 0s - loss: 1.8327 - acc: 0.3264\u001b[0m\n",
      "\u001b[34m311/312 [============================>.] - ETA: 0s - loss: 1.8323 - acc: 0.3263\u001b[0m\n",
      "\u001b[34m312/312 [==============================] - 31s 99ms/step - loss: 1.8321 - acc: 0.3263 - val_loss: 3.1529 - val_acc: 0.1608\u001b[0m\n",
      "\u001b[34mEpoch 2/5\n",
      "\n",
      "  1/312 [..............................] - ETA: 19s - loss: 1.5446 - acc: 0.4297\n",
      "  2/312 [..............................] - ETA: 19s - loss: 1.5587 - acc: 0.4219\u001b[0m\n",
      "\u001b[34m  3/312 [..............................] - ETA: 19s - loss: 1.5771 - acc: 0.4167\n",
      "  4/312 [..............................] - ETA: 19s - loss: 1.5947 - acc: 0.3984\n",
      "  5/312 [..............................] - ETA: 19s - loss: 1.6079 - acc: 0.3984\n",
      "  6/312 [..............................] - ETA: 19s - loss: 1.6052 - acc: 0.4023\n",
      "  7/312 [..............................] - ETA: 19s - loss: 1.6005 - acc: 0.4074\n",
      "  8/312 [..............................] - ETA: 19s - loss: 1.6057 - acc: 0.3945\n",
      "  9/312 [..............................] - ETA: 19s - loss: 1.5957 - acc: 0.3958\n",
      " 10/312 [..............................] - ETA: 19s - loss: 1.5977 - acc: 0.3953\n",
      " 11/312 [>.............................] - ETA: 18s - loss: 1.5964 - acc: 0.3991\n",
      " 12/312 [>.............................] - ETA: 18s - loss: 1.5907 - acc: 0.4010\n",
      " 13/312 [>.............................] - ETA: 18s - loss: 1.5949 - acc: 0.4026\n",
      " 14/312 [>.............................] - ETA: 18s - loss: 1.5934 - acc: 0.4074\n",
      " 15/312 [>.............................] - ETA: 18s - loss: 1.5939 - acc: 0.4083\n",
      " 16/312 [>.............................] - ETA: 18s - loss: 1.6088 - acc: 0.4058\n",
      " 17/312 [>.............................] - ETA: 18s - loss: 1.6031 - acc: 0.4062\n",
      " 18/312 [>.............................] - ETA: 18s - loss: 1.5991 - acc: 0.4067\u001b[0m\n",
      "\u001b[34m 19/312 [>.............................] - ETA: 18s - loss: 1.5980 - acc: 0.4067\n",
      " 20/312 [>.............................] - ETA: 18s - loss: 1.6005 - acc: 0.4055\n",
      " 21/312 [=>............................] - ETA: 18s - loss: 1.6012 - acc: 0.4051\n",
      " 22/312 [=>............................] - ETA: 18s - loss: 1.5971 - acc: 0.4045\n",
      " 23/312 [=>............................] - ETA: 18s - loss: 1.5935 - acc: 0.4059\n",
      " 24/312 [=>............................] - ETA: 18s - loss: 1.5944 - acc: 0.4053\n",
      " 25/312 [=>............................] - ETA: 18s - loss: 1.5966 - acc: 0.4072\n",
      " 26/312 [=>............................] - ETA: 17s - loss: 1.5946 - acc: 0.4102\n",
      " 27/312 [=>............................] - ETA: 17s - loss: 1.5951 - acc: 0.4091\n",
      " 28/312 [=>............................] - ETA: 17s - loss: 1.5878 - acc: 0.4093\n",
      " 29/312 [=>............................] - ETA: 17s - loss: 1.5834 - acc: 0.4103\n",
      " 30/312 [=>............................] - ETA: 17s - loss: 1.5788 - acc: 0.4130\n",
      " 31/312 [=>............................] - ETA: 17s - loss: 1.5770 - acc: 0.4138\n",
      " 32/312 [==>...........................] - ETA: 17s - loss: 1.5736 - acc: 0.4146\n",
      " 33/312 [==>...........................] - ETA: 17s - loss: 1.5716 - acc: 0.4141\n",
      " 34/312 [==>...........................] - ETA: 17s - loss: 1.5716 - acc: 0.4122\u001b[0m\n",
      "\u001b[34m 35/312 [==>...........................] - ETA: 17s - loss: 1.5719 - acc: 0.4121\n",
      " 36/312 [==>...........................] - ETA: 17s - loss: 1.5709 - acc: 0.4121\n",
      " 37/312 [==>...........................] - ETA: 17s - loss: 1.5680 - acc: 0.4141\n",
      " 38/312 [==>...........................] - ETA: 17s - loss: 1.5675 - acc: 0.4128\n",
      " 39/312 [==>...........................] - ETA: 17s - loss: 1.5686 - acc: 0.4135\n",
      " 40/312 [==>...........................] - ETA: 17s - loss: 1.5725 - acc: 0.4123\n",
      " 41/312 [==>...........................] - ETA: 17s - loss: 1.5688 - acc: 0.4133\n",
      " 42/312 [===>..........................] - ETA: 17s - loss: 1.5671 - acc: 0.4139\n",
      " 43/312 [===>..........................] - ETA: 16s - loss: 1.5645 - acc: 0.4153\n",
      " 44/312 [===>..........................] - ETA: 16s - loss: 1.5618 - acc: 0.4164\n",
      " 45/312 [===>..........................] - ETA: 16s - loss: 1.5594 - acc: 0.4179\n",
      " 46/312 [===>..........................] - ETA: 16s - loss: 1.5582 - acc: 0.4181\n",
      " 47/312 [===>..........................] - ETA: 16s - loss: 1.5567 - acc: 0.4195\n",
      " 48/312 [===>..........................] - ETA: 16s - loss: 1.5579 - acc: 0.4191\n",
      " 49/312 [===>..........................] - ETA: 16s - loss: 1.5558 - acc: 0.4196\n",
      " 50/312 [===>..........................] - ETA: 16s - loss: 1.5549 - acc: 0.4206\u001b[0m\n",
      "\u001b[34m 51/312 [===>..........................] - ETA: 16s - loss: 1.5542 - acc: 0.4203\n",
      " 52/312 [====>.........................] - ETA: 16s - loss: 1.5528 - acc: 0.4202\n",
      " 53/312 [====>.........................] - ETA: 16s - loss: 1.5512 - acc: 0.4213\n",
      " 54/312 [====>.........................] - ETA: 16s - loss: 1.5521 - acc: 0.4214\n",
      " 55/312 [====>.........................] - ETA: 16s - loss: 1.5517 - acc: 0.4212\n",
      " 56/312 [====>.........................] - ETA: 16s - loss: 1.5517 - acc: 0.4205\n",
      " 57/312 [====>.........................] - ETA: 16s - loss: 1.5517 - acc: 0.4197\n",
      " 58/312 [====>.........................] - ETA: 16s - loss: 1.5538 - acc: 0.4185\n",
      " 59/312 [====>.........................] - ETA: 15s - loss: 1.5519 - acc: 0.4202\n",
      " 60/312 [====>.........................] - ETA: 15s - loss: 1.5522 - acc: 0.4198\n",
      " 61/312 [====>.........................] - ETA: 15s - loss: 1.5537 - acc: 0.4210\n",
      " 62/312 [====>.........................] - ETA: 15s - loss: 1.5515 - acc: 0.4219\n",
      " 63/312 [=====>........................] - ETA: 15s - loss: 1.5491 - acc: 0.4225\n",
      " 64/312 [=====>........................] - ETA: 15s - loss: 1.5476 - acc: 0.4232\n",
      " 65/312 [=====>........................] - ETA: 15s - loss: 1.5451 - acc: 0.4244\u001b[0m\n",
      "\u001b[34m 66/312 [=====>........................] - ETA: 15s - loss: 1.5455 - acc: 0.4248\n",
      " 67/312 [=====>........................] - ETA: 15s - loss: 1.5446 - acc: 0.4246\n",
      " 68/312 [=====>........................] - ETA: 15s - loss: 1.5444 - acc: 0.4231\n",
      " 69/312 [=====>........................] - ETA: 15s - loss: 1.5451 - acc: 0.4233\n",
      " 70/312 [=====>........................] - ETA: 15s - loss: 1.5416 - acc: 0.4253\n",
      " 71/312 [=====>........................] - ETA: 15s - loss: 1.5427 - acc: 0.4253\n",
      " 72/312 [=====>........................] - ETA: 15s - loss: 1.5416 - acc: 0.4256\n",
      " 73/312 [======>.......................] - ETA: 15s - loss: 1.5407 - acc: 0.4268\n",
      " 74/312 [======>.......................] - ETA: 15s - loss: 1.5408 - acc: 0.4268\n",
      " 75/312 [======>.......................] - ETA: 15s - loss: 1.5386 - acc: 0.4273\n",
      " 76/312 [======>.......................] - ETA: 14s - loss: 1.5374 - acc: 0.4276\n",
      " 77/312 [======>.......................] - ETA: 14s - loss: 1.5360 - acc: 0.4281\n",
      " 78/312 [======>.......................] - ETA: 14s - loss: 1.5334 - acc: 0.4291\n",
      " 79/312 [======>.......................] - ETA: 14s - loss: 1.5325 - acc: 0.4292\n",
      " 80/312 [======>.......................] - ETA: 14s - loss: 1.5312 - acc: 0.4298\n",
      " 81/312 [======>.......................] - ETA: 14s - loss: 1.5291 - acc: 0.4305\u001b[0m\n",
      "\u001b[34m 82/312 [======>.......................] - ETA: 14s - loss: 1.5290 - acc: 0.4304\n",
      " 83/312 [======>.......................] - ETA: 14s - loss: 1.5292 - acc: 0.4305\n",
      " 84/312 [=======>......................] - ETA: 14s - loss: 1.5311 - acc: 0.4300\n",
      " 85/312 [=======>......................] - ETA: 14s - loss: 1.5309 - acc: 0.4305\n",
      " 86/312 [=======>......................] - ETA: 14s - loss: 1.5305 - acc: 0.4313\n",
      " 87/312 [=======>......................] - ETA: 14s - loss: 1.5295 - acc: 0.4319\n",
      " 88/312 [=======>......................] - ETA: 14s - loss: 1.5262 - acc: 0.4331\n",
      " 89/312 [=======>......................] - ETA: 14s - loss: 1.5237 - acc: 0.4337\n",
      " 90/312 [=======>......................] - ETA: 14s - loss: 1.5238 - acc: 0.4339\n",
      " 91/312 [=======>......................] - ETA: 13s - loss: 1.5256 - acc: 0.4337\n",
      " 92/312 [=======>......................] - ETA: 13s - loss: 1.5238 - acc: 0.4344\n",
      " 93/312 [=======>......................] - ETA: 13s - loss: 1.5237 - acc: 0.4346\n",
      " 94/312 [========>.....................] - ETA: 13s - loss: 1.5232 - acc: 0.4350\n",
      " 95/312 [========>.....................] - ETA: 13s - loss: 1.5229 - acc: 0.4352\n",
      " 96/312 [========>.....................] - ETA: 13s - loss: 1.5232 - acc: 0.4355\n",
      " 97/312 [========>.....................] - ETA: 13s - loss: 1.5225 - acc: 0.4361\u001b[0m\n",
      "\u001b[34m 98/312 [========>.....................] - ETA: 13s - loss: 1.5225 - acc: 0.4360\n",
      " 99/312 [========>.....................] - ETA: 13s - loss: 1.5252 - acc: 0.4354\u001b[0m\n",
      "\u001b[34m100/312 [========>.....................] - ETA: 13s - loss: 1.5251 - acc: 0.4355\u001b[0m\n",
      "\u001b[34m101/312 [========>.....................] - ETA: 13s - loss: 1.5256 - acc: 0.4355\u001b[0m\n",
      "\u001b[34m102/312 [========>.....................] - ETA: 13s - loss: 1.5272 - acc: 0.4347\u001b[0m\n",
      "\u001b[34m103/312 [========>.....................] - ETA: 13s - loss: 1.5271 - acc: 0.4349\u001b[0m\n",
      "\u001b[34m104/312 [=========>....................] - ETA: 13s - loss: 1.5260 - acc: 0.4349\u001b[0m\n",
      "\u001b[34m105/312 [=========>....................] - ETA: 13s - loss: 1.5259 - acc: 0.4344\u001b[0m\n",
      "\u001b[34m106/312 [=========>....................] - ETA: 13s - loss: 1.5264 - acc: 0.4346\u001b[0m\n",
      "\u001b[34m107/312 [=========>....................] - ETA: 12s - loss: 1.5259 - acc: 0.4346\u001b[0m\n",
      "\u001b[34m108/312 [=========>....................] - ETA: 12s - loss: 1.5263 - acc: 0.4346\u001b[0m\n",
      "\u001b[34m109/312 [=========>....................] - ETA: 12s - loss: 1.5252 - acc: 0.4351\u001b[0m\n",
      "\u001b[34m110/312 [=========>....................] - ETA: 12s - loss: 1.5242 - acc: 0.4354\u001b[0m\n",
      "\u001b[34m111/312 [=========>....................] - ETA: 12s - loss: 1.5234 - acc: 0.4357\u001b[0m\n",
      "\u001b[34m112/312 [=========>....................] - ETA: 12s - loss: 1.5240 - acc: 0.4355\u001b[0m\n",
      "\u001b[34m113/312 [=========>....................] - ETA: 12s - loss: 1.5227 - acc: 0.4362\u001b[0m\n",
      "\u001b[34m114/312 [=========>....................] - ETA: 12s - loss: 1.5233 - acc: 0.4364\u001b[0m\n",
      "\u001b[34m115/312 [==========>...................] - ETA: 12s - loss: 1.5240 - acc: 0.4363\u001b[0m\n",
      "\u001b[34m116/312 [==========>...................] - ETA: 12s - loss: 1.5227 - acc: 0.4369\u001b[0m\n",
      "\u001b[34m117/312 [==========>...................] - ETA: 12s - loss: 1.5215 - acc: 0.4368\u001b[0m\n",
      "\u001b[34m118/312 [==========>...................] - ETA: 12s - loss: 1.5206 - acc: 0.4371\u001b[0m\n",
      "\u001b[34m119/312 [==========>...................] - ETA: 12s - loss: 1.5194 - acc: 0.4377\u001b[0m\n",
      "\u001b[34m120/312 [==========>...................] - ETA: 12s - loss: 1.5193 - acc: 0.4374\u001b[0m\n",
      "\u001b[34m121/312 [==========>...................] - ETA: 12s - loss: 1.5181 - acc: 0.4375\u001b[0m\n",
      "\u001b[34m122/312 [==========>...................] - ETA: 12s - loss: 1.5170 - acc: 0.4378\u001b[0m\n",
      "\u001b[34m123/312 [==========>...................] - ETA: 12s - loss: 1.5164 - acc: 0.4385\u001b[0m\n",
      "\u001b[34m124/312 [==========>...................] - ETA: 11s - loss: 1.5150 - acc: 0.4391\u001b[0m\n",
      "\u001b[34m125/312 [===========>..................] - ETA: 11s - loss: 1.5140 - acc: 0.4396\u001b[0m\n",
      "\u001b[34m126/312 [===========>..................] - ETA: 11s - loss: 1.5122 - acc: 0.4404\u001b[0m\n",
      "\u001b[34m127/312 [===========>..................] - ETA: 11s - loss: 1.5123 - acc: 0.4401\u001b[0m\n",
      "\u001b[34m128/312 [===========>..................] - ETA: 11s - loss: 1.5120 - acc: 0.4404\u001b[0m\n",
      "\u001b[34m129/312 [===========>..................] - ETA: 11s - loss: 1.5110 - acc: 0.4409\u001b[0m\n",
      "\u001b[34m130/312 [===========>..................] - ETA: 11s - loss: 1.5102 - acc: 0.4410\u001b[0m\n",
      "\u001b[34m131/312 [===========>..................] - ETA: 11s - loss: 1.5085 - acc: 0.4418\u001b[0m\n",
      "\u001b[34m132/312 [===========>..................] - ETA: 11s - loss: 1.5078 - acc: 0.4419\u001b[0m\n",
      "\u001b[34m133/312 [===========>..................] - ETA: 11s - loss: 1.5080 - acc: 0.4418\u001b[0m\n",
      "\u001b[34m134/312 [===========>..................] - ETA: 11s - loss: 1.5079 - acc: 0.4423\u001b[0m\n",
      "\u001b[34m135/312 [===========>..................] - ETA: 11s - loss: 1.5081 - acc: 0.4423\u001b[0m\n",
      "\u001b[34m136/312 [============>.................] - ETA: 11s - loss: 1.5080 - acc: 0.4425\u001b[0m\n",
      "\u001b[34m137/312 [============>.................] - ETA: 11s - loss: 1.5070 - acc: 0.4425\u001b[0m\n",
      "\u001b[34m138/312 [============>.................] - ETA: 11s - loss: 1.5075 - acc: 0.4421\u001b[0m\n",
      "\u001b[34m139/312 [============>.................] - ETA: 10s - loss: 1.5059 - acc: 0.4433\u001b[0m\n",
      "\u001b[34m140/312 [============>.................] - ETA: 10s - loss: 1.5050 - acc: 0.4436\u001b[0m\n",
      "\u001b[34m141/312 [============>.................] - ETA: 10s - loss: 1.5043 - acc: 0.4439\u001b[0m\n",
      "\u001b[34m142/312 [============>.................] - ETA: 10s - loss: 1.5030 - acc: 0.4446\u001b[0m\n",
      "\u001b[34m143/312 [============>.................] - ETA: 10s - loss: 1.5021 - acc: 0.4448\u001b[0m\n",
      "\u001b[34m144/312 [============>.................] - ETA: 10s - loss: 1.5018 - acc: 0.4448\u001b[0m\n",
      "\u001b[34m145/312 [============>.................] - ETA: 10s - loss: 1.5016 - acc: 0.4448\u001b[0m\n",
      "\u001b[34m146/312 [=============>................] - ETA: 10s - loss: 1.4999 - acc: 0.4455\u001b[0m\n",
      "\u001b[34m147/312 [=============>................] - ETA: 10s - loss: 1.5005 - acc: 0.4455\u001b[0m\n",
      "\u001b[34m148/312 [=============>................] - ETA: 10s - loss: 1.5005 - acc: 0.4456\u001b[0m\n",
      "\u001b[34m149/312 [=============>................] - ETA: 10s - loss: 1.4994 - acc: 0.4458\u001b[0m\n",
      "\u001b[34m150/312 [=============>................] - ETA: 10s - loss: 1.5002 - acc: 0.4457\u001b[0m\n",
      "\u001b[34m151/312 [=============>................] - ETA: 10s - loss: 1.4989 - acc: 0.4463\u001b[0m\n",
      "\u001b[34m152/312 [=============>................] - ETA: 10s - loss: 1.4981 - acc: 0.4468\u001b[0m\n",
      "\u001b[34m153/312 [=============>................] - ETA: 10s - loss: 1.4967 - acc: 0.4473\u001b[0m\n",
      "\u001b[34m154/312 [=============>................] - ETA: 10s - loss: 1.4968 - acc: 0.4471\u001b[0m\n",
      "\u001b[34m155/312 [=============>................] - ETA: 9s - loss: 1.4967 - acc: 0.4472 \u001b[0m\n",
      "\u001b[34m156/312 [==============>...............] - ETA: 9s - loss: 1.4971 - acc: 0.4473\u001b[0m\n",
      "\u001b[34m157/312 [==============>...............] - ETA: 9s - loss: 1.4974 - acc: 0.4475\u001b[0m\n",
      "\u001b[34m158/312 [==============>...............] - ETA: 9s - loss: 1.4971 - acc: 0.4475\u001b[0m\n",
      "\u001b[34m159/312 [==============>...............] - ETA: 9s - loss: 1.4967 - acc: 0.4475\u001b[0m\n",
      "\u001b[34m160/312 [==============>...............] - ETA: 9s - loss: 1.4967 - acc: 0.4476\u001b[0m\n",
      "\u001b[34m161/312 [==============>...............] - ETA: 9s - loss: 1.4959 - acc: 0.4474\u001b[0m\n",
      "\u001b[34m162/312 [==============>...............] - ETA: 9s - loss: 1.4958 - acc: 0.4476\u001b[0m\n",
      "\u001b[34m163/312 [==============>...............] - ETA: 9s - loss: 1.4957 - acc: 0.4476\u001b[0m\n",
      "\u001b[34m164/312 [==============>...............] - ETA: 9s - loss: 1.4947 - acc: 0.4482\u001b[0m\n",
      "\u001b[34m165/312 [==============>...............] - ETA: 9s - loss: 1.4938 - acc: 0.4485\u001b[0m\n",
      "\u001b[34m166/312 [==============>...............] - ETA: 9s - loss: 1.4926 - acc: 0.4492\u001b[0m\n",
      "\u001b[34m167/312 [===============>..............] - ETA: 9s - loss: 1.4911 - acc: 0.4497\u001b[0m\n",
      "\u001b[34m168/312 [===============>..............] - ETA: 9s - loss: 1.4913 - acc: 0.4497\u001b[0m\n",
      "\u001b[34m169/312 [===============>..............] - ETA: 9s - loss: 1.4904 - acc: 0.4502\u001b[0m\n",
      "\u001b[34m170/312 [===============>..............] - ETA: 9s - loss: 1.4890 - acc: 0.4505\u001b[0m\n",
      "\u001b[34m171/312 [===============>..............] - ETA: 8s - loss: 1.4887 - acc: 0.4506\u001b[0m\n",
      "\u001b[34m172/312 [===============>..............] - ETA: 8s - loss: 1.4889 - acc: 0.4506\u001b[0m\n",
      "\u001b[34m173/312 [===============>..............] - ETA: 8s - loss: 1.4883 - acc: 0.4507\u001b[0m\n",
      "\u001b[34m174/312 [===============>..............] - ETA: 8s - loss: 1.4875 - acc: 0.4512\u001b[0m\n",
      "\u001b[34m175/312 [===============>..............] - ETA: 8s - loss: 1.4879 - acc: 0.4512\u001b[0m\n",
      "\u001b[34m176/312 [===============>..............] - ETA: 8s - loss: 1.4866 - acc: 0.4519\u001b[0m\n",
      "\u001b[34m177/312 [================>.............] - ETA: 8s - loss: 1.4861 - acc: 0.4522\u001b[0m\n",
      "\u001b[34m178/312 [================>.............] - ETA: 8s - loss: 1.4863 - acc: 0.4523\u001b[0m\n",
      "\u001b[34m179/312 [================>.............] - ETA: 8s - loss: 1.4866 - acc: 0.4523\u001b[0m\n",
      "\u001b[34m180/312 [================>.............] - ETA: 8s - loss: 1.4857 - acc: 0.4528\u001b[0m\n",
      "\u001b[34m181/312 [================>.............] - ETA: 8s - loss: 1.4852 - acc: 0.4529\u001b[0m\n",
      "\u001b[34m182/312 [================>.............] - ETA: 8s - loss: 1.4854 - acc: 0.4529\u001b[0m\n",
      "\u001b[34m183/312 [================>.............] - ETA: 8s - loss: 1.4860 - acc: 0.4530\u001b[0m\n",
      "\u001b[34m184/312 [================>.............] - ETA: 8s - loss: 1.4858 - acc: 0.4531\u001b[0m\n",
      "\u001b[34m185/312 [================>.............] - ETA: 8s - loss: 1.4861 - acc: 0.4533\u001b[0m\n",
      "\u001b[34m186/312 [================>.............] - ETA: 8s - loss: 1.4859 - acc: 0.4535\u001b[0m\n",
      "\u001b[34m187/312 [================>.............] - ETA: 7s - loss: 1.4862 - acc: 0.4535\u001b[0m\n",
      "\u001b[34m188/312 [=================>............] - ETA: 7s - loss: 1.4852 - acc: 0.4540\u001b[0m\n",
      "\u001b[34m189/312 [=================>............] - ETA: 7s - loss: 1.4855 - acc: 0.4537\u001b[0m\n",
      "\u001b[34m190/312 [=================>............] - ETA: 7s - loss: 1.4846 - acc: 0.4541\u001b[0m\n",
      "\u001b[34m191/312 [=================>............] - ETA: 7s - loss: 1.4844 - acc: 0.4544\u001b[0m\n",
      "\u001b[34m192/312 [=================>............] - ETA: 7s - loss: 1.4842 - acc: 0.4545\u001b[0m\n",
      "\u001b[34m193/312 [=================>............] - ETA: 7s - loss: 1.4840 - acc: 0.4544\u001b[0m\n",
      "\u001b[34m194/312 [=================>............] - ETA: 7s - loss: 1.4832 - acc: 0.4549\u001b[0m\n",
      "\u001b[34m195/312 [=================>............] - ETA: 7s - loss: 1.4824 - acc: 0.4551\u001b[0m\n",
      "\u001b[34m196/312 [=================>............] - ETA: 7s - loss: 1.4818 - acc: 0.4554\u001b[0m\n",
      "\u001b[34m197/312 [=================>............] - ETA: 7s - loss: 1.4817 - acc: 0.4552\u001b[0m\n",
      "\u001b[34m198/312 [==================>...........] - ETA: 7s - loss: 1.4820 - acc: 0.4551\u001b[0m\n",
      "\u001b[34m199/312 [==================>...........] - ETA: 7s - loss: 1.4817 - acc: 0.4552\u001b[0m\n",
      "\u001b[34m200/312 [==================>...........] - ETA: 7s - loss: 1.4815 - acc: 0.4554\u001b[0m\n",
      "\u001b[34m201/312 [==================>...........] - ETA: 7s - loss: 1.4810 - acc: 0.4556\u001b[0m\n",
      "\u001b[34m202/312 [==================>...........] - ETA: 7s - loss: 1.4801 - acc: 0.4557\u001b[0m\n",
      "\u001b[34m203/312 [==================>...........] - ETA: 6s - loss: 1.4798 - acc: 0.4560\u001b[0m\n",
      "\u001b[34m204/312 [==================>...........] - ETA: 6s - loss: 1.4790 - acc: 0.4565\u001b[0m\n",
      "\u001b[34m205/312 [==================>...........] - ETA: 6s - loss: 1.4786 - acc: 0.4568\u001b[0m\n",
      "\u001b[34m206/312 [==================>...........] - ETA: 6s - loss: 1.4775 - acc: 0.4572\u001b[0m\n",
      "\u001b[34m207/312 [==================>...........] - ETA: 6s - loss: 1.4766 - acc: 0.4575\u001b[0m\n",
      "\u001b[34m208/312 [===================>..........] - ETA: 6s - loss: 1.4755 - acc: 0.4578\u001b[0m\n",
      "\u001b[34m209/312 [===================>..........] - ETA: 6s - loss: 1.4748 - acc: 0.4581\u001b[0m\n",
      "\u001b[34m210/312 [===================>..........] - ETA: 6s - loss: 1.4743 - acc: 0.4582\u001b[0m\n",
      "\u001b[34m211/312 [===================>..........] - ETA: 6s - loss: 1.4733 - acc: 0.4587\u001b[0m\n",
      "\u001b[34m212/312 [===================>..........] - ETA: 6s - loss: 1.4731 - acc: 0.4589\u001b[0m\n",
      "\u001b[34m213/312 [===================>..........] - ETA: 6s - loss: 1.4730 - acc: 0.4588\u001b[0m\n",
      "\u001b[34m214/312 [===================>..........] - ETA: 6s - loss: 1.4726 - acc: 0.4587\u001b[0m\n",
      "\u001b[34m215/312 [===================>..........] - ETA: 6s - loss: 1.4721 - acc: 0.4589\u001b[0m\n",
      "\u001b[34m216/312 [===================>..........] - ETA: 6s - loss: 1.4722 - acc: 0.4591\u001b[0m\n",
      "\u001b[34m217/312 [===================>..........] - ETA: 6s - loss: 1.4718 - acc: 0.4589\u001b[0m\n",
      "\u001b[34m218/312 [===================>..........] - ETA: 5s - loss: 1.4720 - acc: 0.4586\u001b[0m\n",
      "\u001b[34m219/312 [====================>.........] - ETA: 5s - loss: 1.4717 - acc: 0.4587\u001b[0m\n",
      "\u001b[34m220/312 [====================>.........] - ETA: 5s - loss: 1.4709 - acc: 0.4588\u001b[0m\n",
      "\u001b[34m221/312 [====================>.........] - ETA: 5s - loss: 1.4707 - acc: 0.4589\u001b[0m\n",
      "\u001b[34m222/312 [====================>.........] - ETA: 5s - loss: 1.4701 - acc: 0.4590\u001b[0m\n",
      "\u001b[34m223/312 [====================>.........] - ETA: 5s - loss: 1.4696 - acc: 0.4591\u001b[0m\n",
      "\u001b[34m224/312 [====================>.........] - ETA: 5s - loss: 1.4691 - acc: 0.4590\u001b[0m\n",
      "\u001b[34m225/312 [====================>.........] - ETA: 5s - loss: 1.4684 - acc: 0.4595\u001b[0m\n",
      "\u001b[34m226/312 [====================>.........] - ETA: 5s - loss: 1.4676 - acc: 0.4599\u001b[0m\n",
      "\u001b[34m227/312 [====================>.........] - ETA: 5s - loss: 1.4670 - acc: 0.4599\u001b[0m\n",
      "\u001b[34m228/312 [====================>.........] - ETA: 5s - loss: 1.4660 - acc: 0.4604\u001b[0m\n",
      "\u001b[34m229/312 [=====================>........] - ETA: 5s - loss: 1.4661 - acc: 0.4604\u001b[0m\n",
      "\u001b[34m230/312 [=====================>........] - ETA: 5s - loss: 1.4658 - acc: 0.4605\u001b[0m\n",
      "\u001b[34m231/312 [=====================>........] - ETA: 5s - loss: 1.4655 - acc: 0.4608\u001b[0m\n",
      "\u001b[34m232/312 [=====================>........] - ETA: 5s - loss: 1.4656 - acc: 0.4609\u001b[0m\n",
      "\u001b[34m233/312 [=====================>........] - ETA: 5s - loss: 1.4652 - acc: 0.4609\u001b[0m\n",
      "\u001b[34m234/312 [=====================>........] - ETA: 4s - loss: 1.4649 - acc: 0.4605\u001b[0m\n",
      "\u001b[34m235/312 [=====================>........] - ETA: 4s - loss: 1.4648 - acc: 0.4607\u001b[0m\n",
      "\u001b[34m236/312 [=====================>........] - ETA: 4s - loss: 1.4652 - acc: 0.4607\u001b[0m\n",
      "\u001b[34m237/312 [=====================>........] - ETA: 4s - loss: 1.4644 - acc: 0.4610\u001b[0m\n",
      "\u001b[34m238/312 [=====================>........] - ETA: 4s - loss: 1.4635 - acc: 0.4612\u001b[0m\n",
      "\u001b[34m239/312 [=====================>........] - ETA: 4s - loss: 1.4634 - acc: 0.4613\u001b[0m\n",
      "\u001b[34m240/312 [======================>.......] - ETA: 4s - loss: 1.4633 - acc: 0.4612\u001b[0m\n",
      "\u001b[34m241/312 [======================>.......] - ETA: 4s - loss: 1.4631 - acc: 0.4609\u001b[0m\n",
      "\u001b[34m242/312 [======================>.......] - ETA: 4s - loss: 1.4628 - acc: 0.4611\u001b[0m\n",
      "\u001b[34m243/312 [======================>.......] - ETA: 4s - loss: 1.4632 - acc: 0.4609\u001b[0m\n",
      "\u001b[34m244/312 [======================>.......] - ETA: 4s - loss: 1.4627 - acc: 0.4611\u001b[0m\n",
      "\u001b[34m245/312 [======================>.......] - ETA: 4s - loss: 1.4622 - acc: 0.4613\u001b[0m\n",
      "\u001b[34m246/312 [======================>.......] - ETA: 4s - loss: 1.4616 - acc: 0.4615\u001b[0m\n",
      "\u001b[34m247/312 [======================>.......] - ETA: 4s - loss: 1.4610 - acc: 0.4618\u001b[0m\n",
      "\u001b[34m248/312 [======================>.......] - ETA: 4s - loss: 1.4601 - acc: 0.4619\u001b[0m\n",
      "\u001b[34m249/312 [======================>.......] - ETA: 4s - loss: 1.4602 - acc: 0.4618\u001b[0m\n",
      "\u001b[34m250/312 [=======================>......] - ETA: 3s - loss: 1.4598 - acc: 0.4622\u001b[0m\n",
      "\u001b[34m251/312 [=======================>......] - ETA: 3s - loss: 1.4596 - acc: 0.4624\u001b[0m\n",
      "\u001b[34m252/312 [=======================>......] - ETA: 3s - loss: 1.4596 - acc: 0.4626\u001b[0m\n",
      "\u001b[34m253/312 [=======================>......] - ETA: 3s - loss: 1.4599 - acc: 0.4626\u001b[0m\n",
      "\u001b[34m254/312 [=======================>......] - ETA: 3s - loss: 1.4596 - acc: 0.4628\u001b[0m\n",
      "\u001b[34m255/312 [=======================>......] - ETA: 3s - loss: 1.4591 - acc: 0.4629\u001b[0m\n",
      "\u001b[34m256/312 [=======================>......] - ETA: 3s - loss: 1.4592 - acc: 0.4630\u001b[0m\n",
      "\u001b[34m257/312 [=======================>......] - ETA: 3s - loss: 1.4590 - acc: 0.4631\u001b[0m\n",
      "\u001b[34m258/312 [=======================>......] - ETA: 3s - loss: 1.4584 - acc: 0.4634\u001b[0m\n",
      "\u001b[34m259/312 [=======================>......] - ETA: 3s - loss: 1.4585 - acc: 0.4634\u001b[0m\n",
      "\u001b[34m260/312 [========================>.....] - ETA: 3s - loss: 1.4578 - acc: 0.4636\u001b[0m\n",
      "\u001b[34m261/312 [========================>.....] - ETA: 3s - loss: 1.4582 - acc: 0.4634\u001b[0m\n",
      "\u001b[34m262/312 [========================>.....] - ETA: 3s - loss: 1.4577 - acc: 0.4636\u001b[0m\n",
      "\u001b[34m263/312 [========================>.....] - ETA: 3s - loss: 1.4570 - acc: 0.4638\u001b[0m\n",
      "\u001b[34m264/312 [========================>.....] - ETA: 3s - loss: 1.4565 - acc: 0.4640\u001b[0m\n",
      "\u001b[34m265/312 [========================>.....] - ETA: 2s - loss: 1.4562 - acc: 0.4641\u001b[0m\n",
      "\u001b[34m266/312 [========================>.....] - ETA: 2s - loss: 1.4562 - acc: 0.4641\u001b[0m\n",
      "\u001b[34m267/312 [========================>.....] - ETA: 2s - loss: 1.4554 - acc: 0.4644\u001b[0m\n",
      "\u001b[34m268/312 [========================>.....] - ETA: 2s - loss: 1.4545 - acc: 0.4647\u001b[0m\n",
      "\u001b[34m269/312 [========================>.....] - ETA: 2s - loss: 1.4537 - acc: 0.4651\u001b[0m\n",
      "\u001b[34m270/312 [========================>.....] - ETA: 2s - loss: 1.4538 - acc: 0.4651\u001b[0m\n",
      "\u001b[34m271/312 [=========================>....] - ETA: 2s - loss: 1.4534 - acc: 0.4655\u001b[0m\n",
      "\u001b[34m272/312 [=========================>....] - ETA: 2s - loss: 1.4528 - acc: 0.4656\u001b[0m\n",
      "\u001b[34m273/312 [=========================>....] - ETA: 2s - loss: 1.4523 - acc: 0.4657\u001b[0m\n",
      "\u001b[34m274/312 [=========================>....] - ETA: 2s - loss: 1.4518 - acc: 0.4659\u001b[0m\n",
      "\u001b[34m275/312 [=========================>....] - ETA: 2s - loss: 1.4515 - acc: 0.4659\u001b[0m\n",
      "\u001b[34m276/312 [=========================>....] - ETA: 2s - loss: 1.4510 - acc: 0.4661\u001b[0m\n",
      "\u001b[34m277/312 [=========================>....] - ETA: 2s - loss: 1.4509 - acc: 0.4662\u001b[0m\n",
      "\u001b[34m278/312 [=========================>....] - ETA: 2s - loss: 1.4508 - acc: 0.4662\u001b[0m\n",
      "\u001b[34m279/312 [=========================>....] - ETA: 2s - loss: 1.4502 - acc: 0.4665\u001b[0m\n",
      "\u001b[34m280/312 [=========================>....] - ETA: 2s - loss: 1.4503 - acc: 0.4664\u001b[0m\n",
      "\u001b[34m281/312 [==========================>...] - ETA: 1s - loss: 1.4503 - acc: 0.4665\u001b[0m\n",
      "\u001b[34m282/312 [==========================>...] - ETA: 1s - loss: 1.4501 - acc: 0.4665\u001b[0m\n",
      "\u001b[34m283/312 [==========================>...] - ETA: 1s - loss: 1.4501 - acc: 0.4666\u001b[0m\n",
      "\u001b[34m284/312 [==========================>...] - ETA: 1s - loss: 1.4507 - acc: 0.4665\u001b[0m\n",
      "\u001b[34m285/312 [==========================>...] - ETA: 1s - loss: 1.4505 - acc: 0.4667\u001b[0m\n",
      "\u001b[34m286/312 [==========================>...] - ETA: 1s - loss: 1.4503 - acc: 0.4665\u001b[0m\n",
      "\u001b[34m287/312 [==========================>...] - ETA: 1s - loss: 1.4502 - acc: 0.4667\u001b[0m\n",
      "\u001b[34m288/312 [==========================>...] - ETA: 1s - loss: 1.4503 - acc: 0.4668\u001b[0m\n",
      "\u001b[34m289/312 [==========================>...] - ETA: 1s - loss: 1.4497 - acc: 0.4669\u001b[0m\n",
      "\u001b[34m290/312 [==========================>...] - ETA: 1s - loss: 1.4491 - acc: 0.4670\u001b[0m\n",
      "\u001b[34m291/312 [==========================>...] - ETA: 1s - loss: 1.4495 - acc: 0.4669\u001b[0m\n",
      "\u001b[34m292/312 [===========================>..] - ETA: 1s - loss: 1.4485 - acc: 0.4672\u001b[0m\n",
      "\u001b[34m293/312 [===========================>..] - ETA: 1s - loss: 1.4483 - acc: 0.4673\u001b[0m\n",
      "\u001b[34m294/312 [===========================>..] - ETA: 1s - loss: 1.4480 - acc: 0.4675\u001b[0m\n",
      "\u001b[34m295/312 [===========================>..] - ETA: 1s - loss: 1.4474 - acc: 0.4677\u001b[0m\n",
      "\u001b[34m296/312 [===========================>..] - ETA: 1s - loss: 1.4474 - acc: 0.4680\u001b[0m\n",
      "\u001b[34m297/312 [===========================>..] - ETA: 0s - loss: 1.4470 - acc: 0.4682\u001b[0m\n",
      "\u001b[34m298/312 [===========================>..] - ETA: 0s - loss: 1.4467 - acc: 0.4683\u001b[0m\n",
      "\u001b[34m299/312 [===========================>..] - ETA: 0s - loss: 1.4467 - acc: 0.4684\u001b[0m\n",
      "\u001b[34m300/312 [===========================>..] - ETA: 0s - loss: 1.4469 - acc: 0.4685\u001b[0m\n",
      "\u001b[34m301/312 [===========================>..] - ETA: 0s - loss: 1.4467 - acc: 0.4686\u001b[0m\n",
      "\u001b[34m302/312 [============================>.] - ETA: 0s - loss: 1.4463 - acc: 0.4686\u001b[0m\n",
      "\u001b[34m303/312 [============================>.] - ETA: 0s - loss: 1.4461 - acc: 0.4685\u001b[0m\n",
      "\u001b[34m304/312 [============================>.] - ETA: 0s - loss: 1.4455 - acc: 0.4687\u001b[0m\n",
      "\u001b[34m305/312 [============================>.] - ETA: 0s - loss: 1.4455 - acc: 0.4687\u001b[0m\n",
      "\u001b[34m306/312 [============================>.] - ETA: 0s - loss: 1.4455 - acc: 0.4686\u001b[0m\n",
      "\u001b[34m307/312 [============================>.] - ETA: 0s - loss: 1.4456 - acc: 0.4684\u001b[0m\n",
      "\u001b[34m308/312 [============================>.] - ETA: 0s - loss: 1.4455 - acc: 0.4685\u001b[0m\n",
      "\u001b[34m309/312 [============================>.] - ETA: 0s - loss: 1.4448 - acc: 0.4689\u001b[0m\n",
      "\u001b[34m310/312 [============================>.] - ETA: 0s - loss: 1.4449 - acc: 0.4689\u001b[0m\n",
      "\u001b[34m311/312 [============================>.] - ETA: 0s - loss: 1.4447 - acc: 0.4690\u001b[0m\n",
      "\u001b[34m312/312 [==============================] - 22s 70ms/step - loss: 1.4441 - acc: 0.4694 - val_loss: 1.3832 - val_acc: 0.4931\u001b[0m\n",
      "\u001b[34mEpoch 3/5\n",
      "\n",
      "  1/312 [..............................] - ETA: 20s - loss: 1.3609 - acc: 0.4844\u001b[0m\n",
      "\u001b[34m  2/312 [..............................] - ETA: 24s - loss: 1.3377 - acc: 0.5195\n",
      "  3/312 [..............................] - ETA: 22s - loss: 1.3600 - acc: 0.5104\n",
      "  4/312 [..............................] - ETA: 22s - loss: 1.3559 - acc: 0.5000\n",
      "  5/312 [..............................] - ETA: 21s - loss: 1.3369 - acc: 0.5062\n",
      "  6/312 [..............................] - ETA: 21s - loss: 1.3489 - acc: 0.4935\n",
      "  7/312 [..............................] - ETA: 20s - loss: 1.3388 - acc: 0.4933\n",
      "  8/312 [..............................] - ETA: 20s - loss: 1.3264 - acc: 0.5020\n",
      "  9/312 [..............................] - ETA: 20s - loss: 1.3147 - acc: 0.5087\n",
      " 10/312 [..............................] - ETA: 20s - loss: 1.3080 - acc: 0.5125\n",
      " 11/312 [>.............................] - ETA: 20s - loss: 1.3132 - acc: 0.5092\n",
      " 12/312 [>.............................] - ETA: 19s - loss: 1.3032 - acc: 0.5156\n",
      " 13/312 [>.............................] - ETA: 19s - loss: 1.3038 - acc: 0.5168\n",
      " 14/312 [>.............................] - ETA: 19s - loss: 1.3188 - acc: 0.5134\n",
      " 15/312 [>.............................] - ETA: 19s - loss: 1.3216 - acc: 0.5130\n",
      " 16/312 [>.............................] - ETA: 19s - loss: 1.3199 - acc: 0.5146\n",
      " 17/312 [>.............................] - ETA: 19s - loss: 1.3250 - acc: 0.5124\u001b[0m\n",
      "\u001b[34m 18/312 [>.............................] - ETA: 19s - loss: 1.3201 - acc: 0.5113\n",
      " 19/312 [>.............................] - ETA: 19s - loss: 1.3144 - acc: 0.5099\n",
      " 20/312 [>.............................] - ETA: 19s - loss: 1.3115 - acc: 0.5105\n",
      " 21/312 [=>............................] - ETA: 19s - loss: 1.3045 - acc: 0.5134\n",
      " 22/312 [=>............................] - ETA: 18s - loss: 1.3071 - acc: 0.5142\n",
      " 23/312 [=>............................] - ETA: 18s - loss: 1.3164 - acc: 0.5112\n",
      " 24/312 [=>............................] - ETA: 18s - loss: 1.3139 - acc: 0.5107\n",
      " 25/312 [=>............................] - ETA: 18s - loss: 1.3195 - acc: 0.5088\n",
      " 26/312 [=>............................] - ETA: 18s - loss: 1.3158 - acc: 0.5099\n",
      " 27/312 [=>............................] - ETA: 18s - loss: 1.3103 - acc: 0.5127\n",
      " 28/312 [=>............................] - ETA: 18s - loss: 1.3096 - acc: 0.5145\n",
      " 29/312 [=>............................] - ETA: 18s - loss: 1.3086 - acc: 0.5151\n",
      " 30/312 [=>............................] - ETA: 18s - loss: 1.3049 - acc: 0.5182\n",
      " 31/312 [=>............................] - ETA: 18s - loss: 1.3162 - acc: 0.5159\n",
      " 32/312 [==>...........................] - ETA: 18s - loss: 1.3134 - acc: 0.5171\n",
      " 33/312 [==>...........................] - ETA: 17s - loss: 1.3172 - acc: 0.5154\u001b[0m\n",
      "\u001b[34m 34/312 [==>...........................] - ETA: 17s - loss: 1.3142 - acc: 0.5172\n",
      " 35/312 [==>...........................] - ETA: 17s - loss: 1.3101 - acc: 0.5194\n",
      " 36/312 [==>...........................] - ETA: 17s - loss: 1.3085 - acc: 0.5217\n",
      " 37/312 [==>...........................] - ETA: 17s - loss: 1.3041 - acc: 0.5226\n",
      " 38/312 [==>...........................] - ETA: 17s - loss: 1.3039 - acc: 0.5226\n",
      " 39/312 [==>...........................] - ETA: 17s - loss: 1.3042 - acc: 0.5228\n",
      " 40/312 [==>...........................] - ETA: 17s - loss: 1.3053 - acc: 0.5223\n",
      " 41/312 [==>...........................] - ETA: 17s - loss: 1.3096 - acc: 0.5223\n",
      " 42/312 [===>..........................] - ETA: 17s - loss: 1.3118 - acc: 0.5195\n",
      " 43/312 [===>..........................] - ETA: 17s - loss: 1.3126 - acc: 0.5187\n",
      " 44/312 [===>..........................] - ETA: 17s - loss: 1.3148 - acc: 0.5186\n",
      " 45/312 [===>..........................] - ETA: 17s - loss: 1.3161 - acc: 0.5175\n",
      " 46/312 [===>..........................] - ETA: 17s - loss: 1.3157 - acc: 0.5172\n",
      " 47/312 [===>..........................] - ETA: 17s - loss: 1.3150 - acc: 0.5165\n",
      " 48/312 [===>..........................] - ETA: 16s - loss: 1.3134 - acc: 0.5168\n",
      " 49/312 [===>..........................] - ETA: 16s - loss: 1.3147 - acc: 0.5172\u001b[0m\n",
      "\u001b[34m 50/312 [===>..........................] - ETA: 16s - loss: 1.3141 - acc: 0.5184\n",
      " 51/312 [===>..........................] - ETA: 16s - loss: 1.3104 - acc: 0.5201\n",
      " 52/312 [====>.........................] - ETA: 16s - loss: 1.3118 - acc: 0.5203\n",
      " 53/312 [====>.........................] - ETA: 16s - loss: 1.3120 - acc: 0.5214\n",
      " 54/312 [====>.........................] - ETA: 16s - loss: 1.3128 - acc: 0.5211\n",
      " 55/312 [====>.........................] - ETA: 16s - loss: 1.3136 - acc: 0.5202\n",
      " 56/312 [====>.........................] - ETA: 16s - loss: 1.3130 - acc: 0.5212\n",
      " 57/312 [====>.........................] - ETA: 16s - loss: 1.3128 - acc: 0.5207\n",
      " 58/312 [====>.........................] - ETA: 16s - loss: 1.3130 - acc: 0.5198\n",
      " 59/312 [====>.........................] - ETA: 16s - loss: 1.3121 - acc: 0.5195\n",
      " 60/312 [====>.........................] - ETA: 16s - loss: 1.3112 - acc: 0.5207\n",
      " 61/312 [====>.........................] - ETA: 16s - loss: 1.3087 - acc: 0.5213\n",
      " 62/312 [====>.........................] - ETA: 15s - loss: 1.3083 - acc: 0.5218\n",
      " 63/312 [=====>........................] - ETA: 15s - loss: 1.3103 - acc: 0.5216\n",
      " 64/312 [=====>........................] - ETA: 15s - loss: 1.3101 - acc: 0.5210\n",
      " 65/312 [=====>........................] - ETA: 15s - loss: 1.3096 - acc: 0.5215\u001b[0m\n",
      "\u001b[34m 66/312 [=====>........................] - ETA: 15s - loss: 1.3077 - acc: 0.5220\n",
      " 67/312 [=====>........................] - ETA: 15s - loss: 1.3061 - acc: 0.5233\n",
      " 68/312 [=====>........................] - ETA: 15s - loss: 1.3048 - acc: 0.5239\n",
      " 69/312 [=====>........................] - ETA: 15s - loss: 1.3031 - acc: 0.5245\n",
      " 70/312 [=====>........................] - ETA: 15s - loss: 1.3047 - acc: 0.5234\n",
      " 71/312 [=====>........................] - ETA: 15s - loss: 1.3027 - acc: 0.5240\n",
      " 72/312 [=====>........................] - ETA: 15s - loss: 1.3040 - acc: 0.5238\n",
      " 73/312 [======>.......................] - ETA: 15s - loss: 1.3038 - acc: 0.5242\n",
      " 74/312 [======>.......................] - ETA: 15s - loss: 1.3036 - acc: 0.5245\n",
      " 75/312 [======>.......................] - ETA: 15s - loss: 1.3033 - acc: 0.5246\n",
      " 76/312 [======>.......................] - ETA: 15s - loss: 1.3022 - acc: 0.5250\n",
      " 77/312 [======>.......................] - ETA: 14s - loss: 1.3032 - acc: 0.5252\n",
      " 78/312 [======>.......................] - ETA: 14s - loss: 1.3035 - acc: 0.5251\n",
      " 79/312 [======>.......................] - ETA: 14s - loss: 1.3023 - acc: 0.5250\n",
      " 80/312 [======>.......................] - ETA: 14s - loss: 1.3027 - acc: 0.5259\n",
      " 81/312 [======>.......................] - ETA: 14s - loss: 1.3029 - acc: 0.5268\u001b[0m\n",
      "\u001b[34m 82/312 [======>.......................] - ETA: 14s - loss: 1.3020 - acc: 0.5271\n",
      " 83/312 [======>.......................] - ETA: 14s - loss: 1.3032 - acc: 0.5266\n",
      " 84/312 [=======>......................] - ETA: 14s - loss: 1.3011 - acc: 0.5269\n",
      " 85/312 [=======>......................] - ETA: 14s - loss: 1.3022 - acc: 0.5271\n",
      " 86/312 [=======>......................] - ETA: 14s - loss: 1.3025 - acc: 0.5265\n",
      " 87/312 [=======>......................] - ETA: 14s - loss: 1.3010 - acc: 0.5270\n",
      " 88/312 [=======>......................] - ETA: 14s - loss: 1.3010 - acc: 0.5267\n",
      " 89/312 [=======>......................] - ETA: 14s - loss: 1.3023 - acc: 0.5261\n",
      " 90/312 [=======>......................] - ETA: 14s - loss: 1.3029 - acc: 0.5257\n",
      " 91/312 [=======>......................] - ETA: 14s - loss: 1.3019 - acc: 0.5258\n",
      " 92/312 [=======>......................] - ETA: 13s - loss: 1.3013 - acc: 0.5263\n",
      " 93/312 [=======>......................] - ETA: 13s - loss: 1.3026 - acc: 0.5260\n",
      " 94/312 [========>.....................] - ETA: 13s - loss: 1.3043 - acc: 0.5256\n",
      " 95/312 [========>.....................] - ETA: 13s - loss: 1.3034 - acc: 0.5262\n",
      " 96/312 [========>.....................] - ETA: 13s - loss: 1.3017 - acc: 0.5273\u001b[0m\n",
      "\u001b[34m 97/312 [========>.....................] - ETA: 13s - loss: 1.3026 - acc: 0.5266\n",
      " 98/312 [========>.....................] - ETA: 13s - loss: 1.3032 - acc: 0.5267\n",
      " 99/312 [========>.....................] - ETA: 13s - loss: 1.3024 - acc: 0.5269\u001b[0m\n",
      "\u001b[34m100/312 [========>.....................] - ETA: 13s - loss: 1.3021 - acc: 0.5269\u001b[0m\n",
      "\u001b[34m101/312 [========>.....................] - ETA: 13s - loss: 1.3021 - acc: 0.5270\u001b[0m\n",
      "\u001b[34m102/312 [========>.....................] - ETA: 13s - loss: 1.3020 - acc: 0.5272\u001b[0m\n",
      "\u001b[34m103/312 [========>.....................] - ETA: 13s - loss: 1.3018 - acc: 0.5275\u001b[0m\n",
      "\u001b[34m104/312 [=========>....................] - ETA: 13s - loss: 1.3019 - acc: 0.5276\u001b[0m\n",
      "\u001b[34m105/312 [=========>....................] - ETA: 13s - loss: 1.3023 - acc: 0.5273\u001b[0m\n",
      "\u001b[34m106/312 [=========>....................] - ETA: 13s - loss: 1.3010 - acc: 0.5279\u001b[0m\n",
      "\u001b[34m107/312 [=========>....................] - ETA: 13s - loss: 1.3009 - acc: 0.5283\u001b[0m\n",
      "\u001b[34m108/312 [=========>....................] - ETA: 12s - loss: 1.2996 - acc: 0.5287\u001b[0m\n",
      "\u001b[34m109/312 [=========>....................] - ETA: 12s - loss: 1.2984 - acc: 0.5292\u001b[0m\n",
      "\u001b[34m110/312 [=========>....................] - ETA: 12s - loss: 1.2988 - acc: 0.5290\u001b[0m\n",
      "\u001b[34m111/312 [=========>....................] - ETA: 12s - loss: 1.2983 - acc: 0.5293\u001b[0m\n",
      "\u001b[34m112/312 [=========>....................] - ETA: 12s - loss: 1.2981 - acc: 0.5294\u001b[0m\n",
      "\u001b[34m113/312 [=========>....................] - ETA: 12s - loss: 1.3005 - acc: 0.5286\u001b[0m\n",
      "\u001b[34m114/312 [=========>....................] - ETA: 12s - loss: 1.3002 - acc: 0.5289\u001b[0m\n",
      "\u001b[34m115/312 [==========>...................] - ETA: 12s - loss: 1.2998 - acc: 0.5287\u001b[0m\n",
      "\u001b[34m116/312 [==========>...................] - ETA: 12s - loss: 1.3015 - acc: 0.5283\u001b[0m\n",
      "\u001b[34m117/312 [==========>...................] - ETA: 12s - loss: 1.3002 - acc: 0.5287\u001b[0m\n",
      "\u001b[34m118/312 [==========>...................] - ETA: 12s - loss: 1.2999 - acc: 0.5294\u001b[0m\n",
      "\u001b[34m119/312 [==========>...................] - ETA: 12s - loss: 1.3005 - acc: 0.5293\u001b[0m\n",
      "\u001b[34m120/312 [==========>...................] - ETA: 12s - loss: 1.2996 - acc: 0.5298\u001b[0m\n",
      "\u001b[34m121/312 [==========>...................] - ETA: 12s - loss: 1.3005 - acc: 0.5295\u001b[0m\n",
      "\u001b[34m122/312 [==========>...................] - ETA: 12s - loss: 1.2993 - acc: 0.5298\u001b[0m\n",
      "\u001b[34m123/312 [==========>...................] - ETA: 12s - loss: 1.2981 - acc: 0.5302\u001b[0m\n",
      "\u001b[34m124/312 [==========>...................] - ETA: 11s - loss: 1.2976 - acc: 0.5304\u001b[0m\n",
      "\u001b[34m125/312 [===========>..................] - ETA: 11s - loss: 1.2989 - acc: 0.5298\u001b[0m\n",
      "\u001b[34m126/312 [===========>..................] - ETA: 11s - loss: 1.2988 - acc: 0.5301\u001b[0m\n",
      "\u001b[34m127/312 [===========>..................] - ETA: 11s - loss: 1.2974 - acc: 0.5307\u001b[0m\n",
      "\u001b[34m128/312 [===========>..................] - ETA: 11s - loss: 1.2974 - acc: 0.5305\u001b[0m\n",
      "\u001b[34m129/312 [===========>..................] - ETA: 11s - loss: 1.2973 - acc: 0.5307\u001b[0m\n",
      "\u001b[34m130/312 [===========>..................] - ETA: 11s - loss: 1.2979 - acc: 0.5308\u001b[0m\n",
      "\u001b[34m131/312 [===========>..................] - ETA: 11s - loss: 1.2982 - acc: 0.5309\u001b[0m\n",
      "\u001b[34m132/312 [===========>..................] - ETA: 11s - loss: 1.2975 - acc: 0.5311\u001b[0m\n",
      "\u001b[34m133/312 [===========>..................] - ETA: 11s - loss: 1.2979 - acc: 0.5307\u001b[0m\n",
      "\u001b[34m134/312 [===========>..................] - ETA: 11s - loss: 1.2979 - acc: 0.5309\u001b[0m\n",
      "\u001b[34m135/312 [===========>..................] - ETA: 11s - loss: 1.2983 - acc: 0.5308\u001b[0m\n",
      "\u001b[34m136/312 [============>.................] - ETA: 11s - loss: 1.2989 - acc: 0.5307\u001b[0m\n",
      "\u001b[34m137/312 [============>.................] - ETA: 11s - loss: 1.2993 - acc: 0.5304\u001b[0m\n",
      "\u001b[34m138/312 [============>.................] - ETA: 11s - loss: 1.2998 - acc: 0.5302\u001b[0m\n",
      "\u001b[34m139/312 [============>.................] - ETA: 11s - loss: 1.2992 - acc: 0.5304\u001b[0m\n",
      "\u001b[34m140/312 [============>.................] - ETA: 10s - loss: 1.2993 - acc: 0.5301\u001b[0m\n",
      "\u001b[34m141/312 [============>.................] - ETA: 10s - loss: 1.2988 - acc: 0.5299\u001b[0m\n",
      "\u001b[34m142/312 [============>.................] - ETA: 10s - loss: 1.2984 - acc: 0.5298\u001b[0m\n",
      "\u001b[34m143/312 [============>.................] - ETA: 10s - loss: 1.2973 - acc: 0.5300\u001b[0m\n",
      "\u001b[34m144/312 [============>.................] - ETA: 10s - loss: 1.2971 - acc: 0.5307\u001b[0m\n",
      "\u001b[34m145/312 [============>.................] - ETA: 10s - loss: 1.2971 - acc: 0.5311\u001b[0m\n",
      "\u001b[34m146/312 [=============>................] - ETA: 10s - loss: 1.2967 - acc: 0.5314\u001b[0m\n",
      "\u001b[34m147/312 [=============>................] - ETA: 10s - loss: 1.2967 - acc: 0.5313\u001b[0m\n",
      "\u001b[34m148/312 [=============>................] - ETA: 10s - loss: 1.2958 - acc: 0.5316\u001b[0m\n",
      "\u001b[34m149/312 [=============>................] - ETA: 10s - loss: 1.2957 - acc: 0.5314\u001b[0m\n",
      "\u001b[34m150/312 [=============>................] - ETA: 10s - loss: 1.2965 - acc: 0.5312\u001b[0m\n",
      "\u001b[34m151/312 [=============>................] - ETA: 10s - loss: 1.2964 - acc: 0.5314\u001b[0m\n",
      "\u001b[34m152/312 [=============>................] - ETA: 10s - loss: 1.2971 - acc: 0.5310\u001b[0m\n",
      "\u001b[34m153/312 [=============>................] - ETA: 10s - loss: 1.2966 - acc: 0.5312\u001b[0m\n",
      "\u001b[34m154/312 [=============>................] - ETA: 10s - loss: 1.2963 - acc: 0.5311\u001b[0m\n",
      "\u001b[34m155/312 [=============>................] - ETA: 9s - loss: 1.2963 - acc: 0.5310 \u001b[0m\n",
      "\u001b[34m156/312 [==============>...............] - ETA: 9s - loss: 1.2963 - acc: 0.5311\u001b[0m\n",
      "\u001b[34m157/312 [==============>...............] - ETA: 9s - loss: 1.2954 - acc: 0.5313\u001b[0m\n",
      "\u001b[34m158/312 [==============>...............] - ETA: 9s - loss: 1.2955 - acc: 0.5311\u001b[0m\n",
      "\u001b[34m159/312 [==============>...............] - ETA: 9s - loss: 1.2954 - acc: 0.5311\u001b[0m\n",
      "\u001b[34m160/312 [==============>...............] - ETA: 9s - loss: 1.2947 - acc: 0.5316\u001b[0m\n",
      "\u001b[34m161/312 [==============>...............] - ETA: 9s - loss: 1.2934 - acc: 0.5321\u001b[0m\n",
      "\u001b[34m162/312 [==============>...............] - ETA: 9s - loss: 1.2926 - acc: 0.5323\u001b[0m\n",
      "\u001b[34m163/312 [==============>...............] - ETA: 9s - loss: 1.2928 - acc: 0.5328\u001b[0m\n",
      "\u001b[34m164/312 [==============>...............] - ETA: 9s - loss: 1.2921 - acc: 0.5330\u001b[0m\n",
      "\u001b[34m165/312 [==============>...............] - ETA: 9s - loss: 1.2918 - acc: 0.5329\u001b[0m\n",
      "\u001b[34m166/312 [==============>...............] - ETA: 9s - loss: 1.2913 - acc: 0.5330\u001b[0m\n",
      "\u001b[34m167/312 [===============>..............] - ETA: 9s - loss: 1.2905 - acc: 0.5333\u001b[0m\n",
      "\u001b[34m168/312 [===============>..............] - ETA: 9s - loss: 1.2906 - acc: 0.5336\u001b[0m\n",
      "\u001b[34m169/312 [===============>..............] - ETA: 9s - loss: 1.2905 - acc: 0.5336\u001b[0m\n",
      "\u001b[34m170/312 [===============>..............] - ETA: 9s - loss: 1.2895 - acc: 0.5341\u001b[0m\n",
      "\u001b[34m171/312 [===============>..............] - ETA: 8s - loss: 1.2883 - acc: 0.5343\u001b[0m\n",
      "\u001b[34m172/312 [===============>..............] - ETA: 8s - loss: 1.2884 - acc: 0.5341\u001b[0m\n",
      "\u001b[34m173/312 [===============>..............] - ETA: 8s - loss: 1.2881 - acc: 0.5341\u001b[0m\n",
      "\u001b[34m174/312 [===============>..............] - ETA: 8s - loss: 1.2874 - acc: 0.5343\u001b[0m\n",
      "\u001b[34m175/312 [===============>..............] - ETA: 8s - loss: 1.2876 - acc: 0.5345\u001b[0m\n",
      "\u001b[34m176/312 [===============>..............] - ETA: 8s - loss: 1.2863 - acc: 0.5348\u001b[0m\n",
      "\u001b[34m177/312 [================>.............] - ETA: 8s - loss: 1.2858 - acc: 0.5349\u001b[0m\n",
      "\u001b[34m178/312 [================>.............] - ETA: 8s - loss: 1.2855 - acc: 0.5351\u001b[0m\n",
      "\u001b[34m179/312 [================>.............] - ETA: 8s - loss: 1.2859 - acc: 0.5346\u001b[0m\n",
      "\u001b[34m180/312 [================>.............] - ETA: 8s - loss: 1.2863 - acc: 0.5348\u001b[0m\n",
      "\u001b[34m181/312 [================>.............] - ETA: 8s - loss: 1.2860 - acc: 0.5347\u001b[0m\n",
      "\u001b[34m182/312 [================>.............] - ETA: 8s - loss: 1.2863 - acc: 0.5348\u001b[0m\n",
      "\u001b[34m183/312 [================>.............] - ETA: 8s - loss: 1.2858 - acc: 0.5349\u001b[0m\n",
      "\u001b[34m184/312 [================>.............] - ETA: 8s - loss: 1.2849 - acc: 0.5350\u001b[0m\n",
      "\u001b[34m185/312 [================>.............] - ETA: 8s - loss: 1.2852 - acc: 0.5350\u001b[0m\n",
      "\u001b[34m186/312 [================>.............] - ETA: 8s - loss: 1.2854 - acc: 0.5349\u001b[0m\n",
      "\u001b[34m187/312 [================>.............] - ETA: 7s - loss: 1.2851 - acc: 0.5349\u001b[0m\n",
      "\u001b[34m188/312 [=================>............] - ETA: 7s - loss: 1.2845 - acc: 0.5351\u001b[0m\n",
      "\u001b[34m189/312 [=================>............] - ETA: 7s - loss: 1.2852 - acc: 0.5348\u001b[0m\n",
      "\u001b[34m190/312 [=================>............] - ETA: 7s - loss: 1.2841 - acc: 0.5353\u001b[0m\n",
      "\u001b[34m191/312 [=================>............] - ETA: 7s - loss: 1.2839 - acc: 0.5353\u001b[0m\n",
      "\u001b[34m192/312 [=================>............] - ETA: 7s - loss: 1.2825 - acc: 0.5357\u001b[0m\n",
      "\u001b[34m193/312 [=================>............] - ETA: 7s - loss: 1.2822 - acc: 0.5357\u001b[0m\n",
      "\u001b[34m194/312 [=================>............] - ETA: 7s - loss: 1.2808 - acc: 0.5364\u001b[0m\n",
      "\u001b[34m195/312 [=================>............] - ETA: 7s - loss: 1.2803 - acc: 0.5367\u001b[0m\n",
      "\u001b[34m196/312 [=================>............] - ETA: 7s - loss: 1.2798 - acc: 0.5368\u001b[0m\n",
      "\u001b[34m197/312 [=================>............] - ETA: 7s - loss: 1.2794 - acc: 0.5368\u001b[0m\n",
      "\u001b[34m198/312 [==================>...........] - ETA: 7s - loss: 1.2796 - acc: 0.5367\u001b[0m\n",
      "\u001b[34m199/312 [==================>...........] - ETA: 7s - loss: 1.2792 - acc: 0.5368\u001b[0m\n",
      "\u001b[34m200/312 [==================>...........] - ETA: 7s - loss: 1.2787 - acc: 0.5368\u001b[0m\n",
      "\u001b[34m201/312 [==================>...........] - ETA: 7s - loss: 1.2778 - acc: 0.5370\u001b[0m\n",
      "\u001b[34m202/312 [==================>...........] - ETA: 7s - loss: 1.2784 - acc: 0.5366\u001b[0m\n",
      "\u001b[34m203/312 [==================>...........] - ETA: 6s - loss: 1.2780 - acc: 0.5368\u001b[0m\n",
      "\u001b[34m204/312 [==================>...........] - ETA: 6s - loss: 1.2774 - acc: 0.5369\u001b[0m\n",
      "\u001b[34m205/312 [==================>...........] - ETA: 6s - loss: 1.2768 - acc: 0.5373\u001b[0m\n",
      "\u001b[34m206/312 [==================>...........] - ETA: 6s - loss: 1.2759 - acc: 0.5379\u001b[0m\n",
      "\u001b[34m207/312 [==================>...........] - ETA: 6s - loss: 1.2753 - acc: 0.5380\u001b[0m\n",
      "\u001b[34m208/312 [===================>..........] - ETA: 6s - loss: 1.2746 - acc: 0.5382\u001b[0m\n",
      "\u001b[34m209/312 [===================>..........] - ETA: 6s - loss: 1.2740 - acc: 0.5385\u001b[0m\n",
      "\u001b[34m210/312 [===================>..........] - ETA: 6s - loss: 1.2734 - acc: 0.5384\u001b[0m\n",
      "\u001b[34m211/312 [===================>..........] - ETA: 6s - loss: 1.2733 - acc: 0.5384\u001b[0m\n",
      "\u001b[34m212/312 [===================>..........] - ETA: 6s - loss: 1.2727 - acc: 0.5385\u001b[0m\n",
      "\u001b[34m213/312 [===================>..........] - ETA: 6s - loss: 1.2722 - acc: 0.5388\u001b[0m\n",
      "\u001b[34m214/312 [===================>..........] - ETA: 6s - loss: 1.2715 - acc: 0.5390\u001b[0m\n",
      "\u001b[34m215/312 [===================>..........] - ETA: 6s - loss: 1.2710 - acc: 0.5392\u001b[0m\n",
      "\u001b[34m216/312 [===================>..........] - ETA: 6s - loss: 1.2701 - acc: 0.5395\u001b[0m\n",
      "\u001b[34m217/312 [===================>..........] - ETA: 6s - loss: 1.2702 - acc: 0.5395\u001b[0m\n",
      "\u001b[34m218/312 [===================>..........] - ETA: 5s - loss: 1.2699 - acc: 0.5397\u001b[0m\n",
      "\u001b[34m219/312 [====================>.........] - ETA: 5s - loss: 1.2703 - acc: 0.5396\u001b[0m\n",
      "\u001b[34m220/312 [====================>.........] - ETA: 5s - loss: 1.2707 - acc: 0.5396\u001b[0m\n",
      "\u001b[34m221/312 [====================>.........] - ETA: 5s - loss: 1.2706 - acc: 0.5398\u001b[0m\n",
      "\u001b[34m222/312 [====================>.........] - ETA: 5s - loss: 1.2708 - acc: 0.5398\u001b[0m\n",
      "\u001b[34m223/312 [====================>.........] - ETA: 5s - loss: 1.2706 - acc: 0.5398\u001b[0m\n",
      "\u001b[34m224/312 [====================>.........] - ETA: 5s - loss: 1.2712 - acc: 0.5396\u001b[0m\n",
      "\u001b[34m225/312 [====================>.........] - ETA: 5s - loss: 1.2710 - acc: 0.5398\u001b[0m\n",
      "\u001b[34m226/312 [====================>.........] - ETA: 5s - loss: 1.2704 - acc: 0.5399\u001b[0m\n",
      "\u001b[34m227/312 [====================>.........] - ETA: 5s - loss: 1.2710 - acc: 0.5395\u001b[0m\n",
      "\u001b[34m228/312 [====================>.........] - ETA: 5s - loss: 1.2709 - acc: 0.5396\u001b[0m\n",
      "\u001b[34m229/312 [=====================>........] - ETA: 5s - loss: 1.2714 - acc: 0.5393\u001b[0m\n",
      "\u001b[34m230/312 [=====================>........] - ETA: 5s - loss: 1.2710 - acc: 0.5396\u001b[0m\n",
      "\u001b[34m231/312 [=====================>........] - ETA: 5s - loss: 1.2708 - acc: 0.5396\u001b[0m\n",
      "\u001b[34m232/312 [=====================>........] - ETA: 5s - loss: 1.2715 - acc: 0.5397\u001b[0m\n",
      "\u001b[34m233/312 [=====================>........] - ETA: 5s - loss: 1.2713 - acc: 0.5398\u001b[0m\n",
      "\u001b[34m234/312 [=====================>........] - ETA: 4s - loss: 1.2719 - acc: 0.5397\u001b[0m\n",
      "\u001b[34m235/312 [=====================>........] - ETA: 4s - loss: 1.2717 - acc: 0.5399\u001b[0m\n",
      "\u001b[34m236/312 [=====================>........] - ETA: 4s - loss: 1.2725 - acc: 0.5398\u001b[0m\n",
      "\u001b[34m237/312 [=====================>........] - ETA: 4s - loss: 1.2731 - acc: 0.5396\u001b[0m\n",
      "\u001b[34m238/312 [=====================>........] - ETA: 4s - loss: 1.2731 - acc: 0.5395\u001b[0m\n",
      "\u001b[34m239/312 [=====================>........] - ETA: 4s - loss: 1.2726 - acc: 0.5397\u001b[0m\n",
      "\u001b[34m240/312 [======================>.......] - ETA: 4s - loss: 1.2724 - acc: 0.5396\u001b[0m\n",
      "\u001b[34m241/312 [======================>.......] - ETA: 4s - loss: 1.2720 - acc: 0.5399\u001b[0m\n",
      "\u001b[34m242/312 [======================>.......] - ETA: 4s - loss: 1.2717 - acc: 0.5399\u001b[0m\n",
      "\u001b[34m243/312 [======================>.......] - ETA: 4s - loss: 1.2710 - acc: 0.5403\u001b[0m\n",
      "\u001b[34m244/312 [======================>.......] - ETA: 4s - loss: 1.2710 - acc: 0.5404\u001b[0m\n",
      "\u001b[34m245/312 [======================>.......] - ETA: 4s - loss: 1.2709 - acc: 0.5404\u001b[0m\n",
      "\u001b[34m246/312 [======================>.......] - ETA: 4s - loss: 1.2711 - acc: 0.5403\u001b[0m\n",
      "\u001b[34m247/312 [======================>.......] - ETA: 4s - loss: 1.2712 - acc: 0.5403\u001b[0m\n",
      "\u001b[34m248/312 [======================>.......] - ETA: 4s - loss: 1.2712 - acc: 0.5401\u001b[0m\n",
      "\u001b[34m249/312 [======================>.......] - ETA: 4s - loss: 1.2714 - acc: 0.5402\u001b[0m\n",
      "\u001b[34m250/312 [=======================>......] - ETA: 3s - loss: 1.2707 - acc: 0.5405\u001b[0m\n",
      "\u001b[34m251/312 [=======================>......] - ETA: 3s - loss: 1.2700 - acc: 0.5406\u001b[0m\n",
      "\u001b[34m252/312 [=======================>......] - ETA: 3s - loss: 1.2699 - acc: 0.5409\u001b[0m\n",
      "\u001b[34m253/312 [=======================>......] - ETA: 3s - loss: 1.2706 - acc: 0.5406\u001b[0m\n",
      "\u001b[34m254/312 [=======================>......] - ETA: 3s - loss: 1.2697 - acc: 0.5409\u001b[0m\n",
      "\u001b[34m255/312 [=======================>......] - ETA: 3s - loss: 1.2696 - acc: 0.5410\u001b[0m\n",
      "\u001b[34m256/312 [=======================>......] - ETA: 3s - loss: 1.2686 - acc: 0.5413\u001b[0m\n",
      "\u001b[34m257/312 [=======================>......] - ETA: 3s - loss: 1.2682 - acc: 0.5416\u001b[0m\n",
      "\u001b[34m258/312 [=======================>......] - ETA: 3s - loss: 1.2681 - acc: 0.5416\u001b[0m\n",
      "\u001b[34m259/312 [=======================>......] - ETA: 3s - loss: 1.2675 - acc: 0.5419\u001b[0m\n",
      "\u001b[34m260/312 [========================>.....] - ETA: 3s - loss: 1.2675 - acc: 0.5419\u001b[0m\n",
      "\u001b[34m261/312 [========================>.....] - ETA: 3s - loss: 1.2667 - acc: 0.5424\u001b[0m\n",
      "\u001b[34m262/312 [========================>.....] - ETA: 3s - loss: 1.2669 - acc: 0.5423\u001b[0m\n",
      "\u001b[34m263/312 [========================>.....] - ETA: 3s - loss: 1.2674 - acc: 0.5423\u001b[0m\n",
      "\u001b[34m264/312 [========================>.....] - ETA: 3s - loss: 1.2674 - acc: 0.5425\u001b[0m\n",
      "\u001b[34m265/312 [========================>.....] - ETA: 2s - loss: 1.2671 - acc: 0.5425\u001b[0m\n",
      "\u001b[34m266/312 [========================>.....] - ETA: 2s - loss: 1.2673 - acc: 0.5424\u001b[0m\n",
      "\u001b[34m267/312 [========================>.....] - ETA: 2s - loss: 1.2669 - acc: 0.5424\u001b[0m\n",
      "\u001b[34m268/312 [========================>.....] - ETA: 2s - loss: 1.2671 - acc: 0.5424\u001b[0m\n",
      "\u001b[34m269/312 [========================>.....] - ETA: 2s - loss: 1.2662 - acc: 0.5428\u001b[0m\n",
      "\u001b[34m270/312 [========================>.....] - ETA: 2s - loss: 1.2655 - acc: 0.5432\u001b[0m\n",
      "\u001b[34m271/312 [=========================>....] - ETA: 2s - loss: 1.2653 - acc: 0.5434\u001b[0m\n",
      "\u001b[34m272/312 [=========================>....] - ETA: 2s - loss: 1.2643 - acc: 0.5437\u001b[0m\n",
      "\u001b[34m273/312 [=========================>....] - ETA: 2s - loss: 1.2647 - acc: 0.5436\u001b[0m\n",
      "\u001b[34m274/312 [=========================>....] - ETA: 2s - loss: 1.2641 - acc: 0.5437\u001b[0m\n",
      "\u001b[34m275/312 [=========================>....] - ETA: 2s - loss: 1.2637 - acc: 0.5439\u001b[0m\n",
      "\u001b[34m276/312 [=========================>....] - ETA: 2s - loss: 1.2628 - acc: 0.5443\u001b[0m\n",
      "\u001b[34m277/312 [=========================>....] - ETA: 2s - loss: 1.2627 - acc: 0.5444\u001b[0m\n",
      "\u001b[34m278/312 [=========================>....] - ETA: 2s - loss: 1.2618 - acc: 0.5449\u001b[0m\n",
      "\u001b[34m279/312 [=========================>....] - ETA: 2s - loss: 1.2615 - acc: 0.5450\u001b[0m\n",
      "\u001b[34m280/312 [=========================>....] - ETA: 2s - loss: 1.2613 - acc: 0.5450\u001b[0m\n",
      "\u001b[34m281/312 [==========================>...] - ETA: 1s - loss: 1.2611 - acc: 0.5450\u001b[0m\n",
      "\u001b[34m282/312 [==========================>...] - ETA: 1s - loss: 1.2610 - acc: 0.5451\u001b[0m\n",
      "\u001b[34m283/312 [==========================>...] - ETA: 1s - loss: 1.2601 - acc: 0.5455\u001b[0m\n",
      "\u001b[34m284/312 [==========================>...] - ETA: 1s - loss: 1.2600 - acc: 0.5455\u001b[0m\n",
      "\u001b[34m285/312 [==========================>...] - ETA: 1s - loss: 1.2597 - acc: 0.5455\u001b[0m\n",
      "\u001b[34m286/312 [==========================>...] - ETA: 1s - loss: 1.2596 - acc: 0.5456\u001b[0m\n",
      "\u001b[34m287/312 [==========================>...] - ETA: 1s - loss: 1.2592 - acc: 0.5458\u001b[0m\n",
      "\u001b[34m288/312 [==========================>...] - ETA: 1s - loss: 1.2586 - acc: 0.5460\u001b[0m\n",
      "\u001b[34m289/312 [==========================>...] - ETA: 1s - loss: 1.2582 - acc: 0.5460\u001b[0m\n",
      "\u001b[34m290/312 [==========================>...] - ETA: 1s - loss: 1.2581 - acc: 0.5461\u001b[0m\n",
      "\u001b[34m291/312 [==========================>...] - ETA: 1s - loss: 1.2582 - acc: 0.5461\u001b[0m\n",
      "\u001b[34m292/312 [===========================>..] - ETA: 1s - loss: 1.2577 - acc: 0.5462\u001b[0m\n",
      "\u001b[34m293/312 [===========================>..] - ETA: 1s - loss: 1.2573 - acc: 0.5463\u001b[0m\n",
      "\u001b[34m294/312 [===========================>..] - ETA: 1s - loss: 1.2572 - acc: 0.5464\u001b[0m\n",
      "\u001b[34m295/312 [===========================>..] - ETA: 1s - loss: 1.2566 - acc: 0.5466\u001b[0m\n",
      "\u001b[34m296/312 [===========================>..] - ETA: 1s - loss: 1.2567 - acc: 0.5467\u001b[0m\n",
      "\u001b[34m297/312 [===========================>..] - ETA: 0s - loss: 1.2567 - acc: 0.5467\u001b[0m\n",
      "\u001b[34m298/312 [===========================>..] - ETA: 0s - loss: 1.2565 - acc: 0.5469\u001b[0m\n",
      "\u001b[34m299/312 [===========================>..] - ETA: 0s - loss: 1.2557 - acc: 0.5472\u001b[0m\n",
      "\u001b[34m300/312 [===========================>..] - ETA: 0s - loss: 1.2553 - acc: 0.5474\u001b[0m\n",
      "\u001b[34m301/312 [===========================>..] - ETA: 0s - loss: 1.2552 - acc: 0.5474\u001b[0m\n",
      "\u001b[34m302/312 [============================>.] - ETA: 0s - loss: 1.2549 - acc: 0.5475\u001b[0m\n",
      "\u001b[34m303/312 [============================>.] - ETA: 0s - loss: 1.2546 - acc: 0.5477\u001b[0m\n",
      "\u001b[34m304/312 [============================>.] - ETA: 0s - loss: 1.2542 - acc: 0.5479\u001b[0m\n",
      "\u001b[34m305/312 [============================>.] - ETA: 0s - loss: 1.2539 - acc: 0.5481\u001b[0m\n",
      "\u001b[34m306/312 [============================>.] - ETA: 0s - loss: 1.2539 - acc: 0.5480\u001b[0m\n",
      "\u001b[34m307/312 [============================>.] - ETA: 0s - loss: 1.2534 - acc: 0.5482\u001b[0m\n",
      "\u001b[34m308/312 [============================>.] - ETA: 0s - loss: 1.2533 - acc: 0.5483\u001b[0m\n",
      "\u001b[34m309/312 [============================>.] - ETA: 0s - loss: 1.2536 - acc: 0.5481\u001b[0m\n",
      "\u001b[34m310/312 [============================>.] - ETA: 0s - loss: 1.2540 - acc: 0.5480\u001b[0m\n",
      "\u001b[34m311/312 [============================>.] - ETA: 0s - loss: 1.2538 - acc: 0.5482\u001b[0m\n",
      "\u001b[34m312/312 [==============================] - 22s 70ms/step - loss: 1.2536 - acc: 0.5483 - val_loss: 1.3173 - val_acc: 0.5134\u001b[0m\n",
      "\u001b[34mEpoch 4/5\n",
      "\n",
      "  1/312 [..............................] - ETA: 19s - loss: 1.1195 - acc: 0.6172\n",
      "  2/312 [..............................] - ETA: 19s - loss: 1.1727 - acc: 0.6133\u001b[0m\n",
      "\u001b[34m  3/312 [..............................] - ETA: 19s - loss: 1.1490 - acc: 0.6068\n",
      "  4/312 [..............................] - ETA: 19s - loss: 1.1542 - acc: 0.5996\n",
      "  5/312 [..............................] - ETA: 19s - loss: 1.1860 - acc: 0.5844\n",
      "  6/312 [..............................] - ETA: 19s - loss: 1.1461 - acc: 0.5872\n",
      "  7/312 [..............................] - ETA: 19s - loss: 1.1611 - acc: 0.5915\n",
      "  8/312 [..............................] - ETA: 19s - loss: 1.1740 - acc: 0.5830\n",
      "  9/312 [..............................] - ETA: 19s - loss: 1.1802 - acc: 0.5833\n",
      " 10/312 [..............................] - ETA: 18s - loss: 1.1810 - acc: 0.5844\n",
      " 11/312 [>.............................] - ETA: 18s - loss: 1.1824 - acc: 0.5817\n",
      " 12/312 [>.............................] - ETA: 18s - loss: 1.2055 - acc: 0.5749\n",
      " 13/312 [>.............................] - ETA: 18s - loss: 1.2093 - acc: 0.5751\n",
      " 14/312 [>.............................] - ETA: 18s - loss: 1.2144 - acc: 0.5698\n",
      " 15/312 [>.............................] - ETA: 18s - loss: 1.2127 - acc: 0.5734\n",
      " 16/312 [>.............................] - ETA: 18s - loss: 1.2045 - acc: 0.5757\n",
      " 17/312 [>.............................] - ETA: 18s - loss: 1.2006 - acc: 0.5763\n",
      " 18/312 [>.............................] - ETA: 18s - loss: 1.2106 - acc: 0.5738\u001b[0m\n",
      "\u001b[34m 19/312 [>.............................] - ETA: 18s - loss: 1.2063 - acc: 0.5761\n",
      " 20/312 [>.............................] - ETA: 18s - loss: 1.2118 - acc: 0.5730\n",
      " 21/312 [=>............................] - ETA: 18s - loss: 1.2178 - acc: 0.5707\n",
      " 22/312 [=>............................] - ETA: 18s - loss: 1.2165 - acc: 0.5710\n",
      " 23/312 [=>............................] - ETA: 18s - loss: 1.2192 - acc: 0.5686\n",
      " 24/312 [=>............................] - ETA: 18s - loss: 1.2241 - acc: 0.5648\n",
      " 25/312 [=>............................] - ETA: 17s - loss: 1.2257 - acc: 0.5650\n",
      " 26/312 [=>............................] - ETA: 17s - loss: 1.2202 - acc: 0.5655\n",
      " 27/312 [=>............................] - ETA: 17s - loss: 1.2203 - acc: 0.5639\n",
      " 28/312 [=>............................] - ETA: 17s - loss: 1.2174 - acc: 0.5650\n",
      " 29/312 [=>............................] - ETA: 17s - loss: 1.2234 - acc: 0.5609\n",
      " 30/312 [=>............................] - ETA: 17s - loss: 1.2262 - acc: 0.5596\n",
      " 31/312 [=>............................] - ETA: 17s - loss: 1.2250 - acc: 0.5607\n",
      " 32/312 [==>...........................] - ETA: 17s - loss: 1.2254 - acc: 0.5610\n",
      " 33/312 [==>...........................] - ETA: 17s - loss: 1.2252 - acc: 0.5599\n",
      " 34/312 [==>...........................] - ETA: 17s - loss: 1.2255 - acc: 0.5600\u001b[0m\n",
      "\u001b[34m 35/312 [==>...........................] - ETA: 17s - loss: 1.2260 - acc: 0.5589\n",
      " 36/312 [==>...........................] - ETA: 17s - loss: 1.2266 - acc: 0.5575\n",
      " 37/312 [==>...........................] - ETA: 17s - loss: 1.2275 - acc: 0.5576\n",
      " 38/312 [==>...........................] - ETA: 17s - loss: 1.2252 - acc: 0.5590\n",
      " 39/312 [==>...........................] - ETA: 17s - loss: 1.2261 - acc: 0.5595\n",
      " 40/312 [==>...........................] - ETA: 17s - loss: 1.2237 - acc: 0.5598\n",
      " 41/312 [==>...........................] - ETA: 17s - loss: 1.2196 - acc: 0.5621\n",
      " 42/312 [===>..........................] - ETA: 16s - loss: 1.2169 - acc: 0.5629\n",
      " 43/312 [===>..........................] - ETA: 16s - loss: 1.2169 - acc: 0.5630\n",
      " 44/312 [===>..........................] - ETA: 16s - loss: 1.2147 - acc: 0.5639\n",
      " 45/312 [===>..........................] - ETA: 16s - loss: 1.2121 - acc: 0.5644\n",
      " 46/312 [===>..........................] - ETA: 16s - loss: 1.2140 - acc: 0.5645\n",
      " 47/312 [===>..........................] - ETA: 16s - loss: 1.2146 - acc: 0.5650\n",
      " 48/312 [===>..........................] - ETA: 16s - loss: 1.2131 - acc: 0.5661\n",
      " 49/312 [===>..........................] - ETA: 16s - loss: 1.2150 - acc: 0.5655\u001b[0m\n",
      "\u001b[34m 50/312 [===>..........................] - ETA: 16s - loss: 1.2150 - acc: 0.5658\n",
      " 51/312 [===>..........................] - ETA: 16s - loss: 1.2147 - acc: 0.5657\n",
      " 52/312 [====>.........................] - ETA: 16s - loss: 1.2145 - acc: 0.5666\n",
      " 53/312 [====>.........................] - ETA: 16s - loss: 1.2172 - acc: 0.5656\n",
      " 54/312 [====>.........................] - ETA: 16s - loss: 1.2177 - acc: 0.5650\n",
      " 55/312 [====>.........................] - ETA: 16s - loss: 1.2166 - acc: 0.5651\n",
      " 56/312 [====>.........................] - ETA: 16s - loss: 1.2175 - acc: 0.5638\n",
      " 57/312 [====>.........................] - ETA: 16s - loss: 1.2149 - acc: 0.5647\n",
      " 58/312 [====>.........................] - ETA: 15s - loss: 1.2159 - acc: 0.5636\n",
      " 59/312 [====>.........................] - ETA: 15s - loss: 1.2135 - acc: 0.5644\n",
      " 60/312 [====>.........................] - ETA: 15s - loss: 1.2126 - acc: 0.5650\n",
      " 61/312 [====>.........................] - ETA: 15s - loss: 1.2098 - acc: 0.5648\n",
      " 62/312 [====>.........................] - ETA: 15s - loss: 1.2114 - acc: 0.5644\n",
      " 63/312 [=====>........................] - ETA: 15s - loss: 1.2090 - acc: 0.5655\n",
      " 64/312 [=====>........................] - ETA: 15s - loss: 1.2058 - acc: 0.5670\n",
      " 65/312 [=====>........................] - ETA: 15s - loss: 1.2044 - acc: 0.5673\u001b[0m\n",
      "\u001b[34m 66/312 [=====>........................] - ETA: 15s - loss: 1.2050 - acc: 0.5669\n",
      " 67/312 [=====>........................] - ETA: 15s - loss: 1.2042 - acc: 0.5667\n",
      " 68/312 [=====>........................] - ETA: 15s - loss: 1.2022 - acc: 0.5670\n",
      " 69/312 [=====>........................] - ETA: 15s - loss: 1.2023 - acc: 0.5667\n",
      " 70/312 [=====>........................] - ETA: 15s - loss: 1.2014 - acc: 0.5672\n",
      " 71/312 [=====>........................] - ETA: 15s - loss: 1.2017 - acc: 0.5667\n",
      " 72/312 [=====>........................] - ETA: 15s - loss: 1.2020 - acc: 0.5665\n",
      " 73/312 [======>.......................] - ETA: 15s - loss: 1.2018 - acc: 0.5661\n",
      " 74/312 [======>.......................] - ETA: 15s - loss: 1.2011 - acc: 0.5664\n",
      " 75/312 [======>.......................] - ETA: 14s - loss: 1.1991 - acc: 0.5673\n",
      " 76/312 [======>.......................] - ETA: 14s - loss: 1.1983 - acc: 0.5679\n",
      " 77/312 [======>.......................] - ETA: 14s - loss: 1.1975 - acc: 0.5684\n",
      " 78/312 [======>.......................] - ETA: 14s - loss: 1.1967 - acc: 0.5687\n",
      " 79/312 [======>.......................] - ETA: 14s - loss: 1.1971 - acc: 0.5689\n",
      " 80/312 [======>.......................] - ETA: 14s - loss: 1.1967 - acc: 0.5690\n",
      " 81/312 [======>.......................] - ETA: 14s - loss: 1.1984 - acc: 0.5683\u001b[0m\n",
      "\u001b[34m 82/312 [======>.......................] - ETA: 14s - loss: 1.1983 - acc: 0.5687\n",
      " 83/312 [======>.......................] - ETA: 14s - loss: 1.1984 - acc: 0.5686\n",
      " 84/312 [=======>......................] - ETA: 14s - loss: 1.1973 - acc: 0.5685\n",
      " 85/312 [=======>......................] - ETA: 14s - loss: 1.1940 - acc: 0.5699\n",
      " 86/312 [=======>......................] - ETA: 14s - loss: 1.1928 - acc: 0.5704\n",
      " 87/312 [=======>......................] - ETA: 14s - loss: 1.1916 - acc: 0.5710\n",
      " 88/312 [=======>......................] - ETA: 14s - loss: 1.1917 - acc: 0.5708\n",
      " 89/312 [=======>......................] - ETA: 14s - loss: 1.1912 - acc: 0.5712\n",
      " 90/312 [=======>......................] - ETA: 14s - loss: 1.1925 - acc: 0.5704\n",
      " 91/312 [=======>......................] - ETA: 13s - loss: 1.1928 - acc: 0.5700\n",
      " 92/312 [=======>......................] - ETA: 13s - loss: 1.1948 - acc: 0.5694\n",
      " 93/312 [=======>......................] - ETA: 13s - loss: 1.1926 - acc: 0.5701\n",
      " 94/312 [========>.....................] - ETA: 13s - loss: 1.1926 - acc: 0.5705\n",
      " 95/312 [========>.....................] - ETA: 13s - loss: 1.1914 - acc: 0.5706\n",
      " 96/312 [========>.....................] - ETA: 13s - loss: 1.1899 - acc: 0.5710\n",
      " 97/312 [========>.....................] - ETA: 13s - loss: 1.1901 - acc: 0.5711\u001b[0m\n",
      "\u001b[34m 98/312 [========>.....................] - ETA: 13s - loss: 1.1910 - acc: 0.5708\n",
      " 99/312 [========>.....................] - ETA: 13s - loss: 1.1907 - acc: 0.5713\u001b[0m\n",
      "\u001b[34m100/312 [========>.....................] - ETA: 13s - loss: 1.1900 - acc: 0.5715\u001b[0m\n",
      "\u001b[34m101/312 [========>.....................] - ETA: 13s - loss: 1.1891 - acc: 0.5723\u001b[0m\n",
      "\u001b[34m102/312 [========>.....................] - ETA: 13s - loss: 1.1878 - acc: 0.5728\u001b[0m\n",
      "\u001b[34m103/312 [========>.....................] - ETA: 13s - loss: 1.1863 - acc: 0.5731\u001b[0m\n",
      "\u001b[34m104/312 [=========>....................] - ETA: 13s - loss: 1.1868 - acc: 0.5726\u001b[0m\n",
      "\u001b[34m105/312 [=========>....................] - ETA: 13s - loss: 1.1862 - acc: 0.5730\u001b[0m\n",
      "\u001b[34m106/312 [=========>....................] - ETA: 12s - loss: 1.1866 - acc: 0.5727\u001b[0m\n",
      "\u001b[34m107/312 [=========>....................] - ETA: 12s - loss: 1.1868 - acc: 0.5723\u001b[0m\n",
      "\u001b[34m108/312 [=========>....................] - ETA: 12s - loss: 1.1867 - acc: 0.5726\u001b[0m\n",
      "\u001b[34m109/312 [=========>....................] - ETA: 12s - loss: 1.1911 - acc: 0.5711\u001b[0m\n",
      "\u001b[34m110/312 [=========>....................] - ETA: 12s - loss: 1.1897 - acc: 0.5715\u001b[0m\n",
      "\u001b[34m111/312 [=========>....................] - ETA: 12s - loss: 1.1902 - acc: 0.5714\u001b[0m\n",
      "\u001b[34m112/312 [=========>....................] - ETA: 12s - loss: 1.1894 - acc: 0.5718\u001b[0m\n",
      "\u001b[34m113/312 [=========>....................] - ETA: 12s - loss: 1.1893 - acc: 0.5719\u001b[0m\n",
      "\u001b[34m114/312 [=========>....................] - ETA: 12s - loss: 1.1898 - acc: 0.5724\u001b[0m\n",
      "\u001b[34m115/312 [==========>...................] - ETA: 12s - loss: 1.1911 - acc: 0.5722\u001b[0m\n",
      "\u001b[34m116/312 [==========>...................] - ETA: 12s - loss: 1.1906 - acc: 0.5730\u001b[0m\n",
      "\u001b[34m117/312 [==========>...................] - ETA: 12s - loss: 1.1898 - acc: 0.5733\u001b[0m\n",
      "\u001b[34m118/312 [==========>...................] - ETA: 12s - loss: 1.1886 - acc: 0.5737\u001b[0m\n",
      "\u001b[34m119/312 [==========>...................] - ETA: 12s - loss: 1.1879 - acc: 0.5740\u001b[0m\n",
      "\u001b[34m120/312 [==========>...................] - ETA: 12s - loss: 1.1875 - acc: 0.5740\u001b[0m\n",
      "\u001b[34m121/312 [==========>...................] - ETA: 12s - loss: 1.1865 - acc: 0.5748\u001b[0m\n",
      "\u001b[34m122/312 [==========>...................] - ETA: 11s - loss: 1.1876 - acc: 0.5745\u001b[0m\n",
      "\u001b[34m123/312 [==========>...................] - ETA: 11s - loss: 1.1873 - acc: 0.5748\u001b[0m\n",
      "\u001b[34m124/312 [==========>...................] - ETA: 11s - loss: 1.1872 - acc: 0.5750\u001b[0m\n",
      "\u001b[34m125/312 [===========>..................] - ETA: 11s - loss: 1.1880 - acc: 0.5747\u001b[0m\n",
      "\u001b[34m126/312 [===========>..................] - ETA: 11s - loss: 1.1885 - acc: 0.5747\u001b[0m\n",
      "\u001b[34m127/312 [===========>..................] - ETA: 11s - loss: 1.1877 - acc: 0.5747\u001b[0m\n",
      "\u001b[34m128/312 [===========>..................] - ETA: 11s - loss: 1.1889 - acc: 0.5744\u001b[0m\n",
      "\u001b[34m129/312 [===========>..................] - ETA: 11s - loss: 1.1892 - acc: 0.5746\u001b[0m\n",
      "\u001b[34m130/312 [===========>..................] - ETA: 11s - loss: 1.1889 - acc: 0.5742\u001b[0m\n",
      "\u001b[34m131/312 [===========>..................] - ETA: 11s - loss: 1.1876 - acc: 0.5747\u001b[0m\n",
      "\u001b[34m132/312 [===========>..................] - ETA: 11s - loss: 1.1884 - acc: 0.5744\u001b[0m\n",
      "\u001b[34m133/312 [===========>..................] - ETA: 11s - loss: 1.1879 - acc: 0.5747\u001b[0m\n",
      "\u001b[34m134/312 [===========>..................] - ETA: 11s - loss: 1.1874 - acc: 0.5749\u001b[0m\n",
      "\u001b[34m135/312 [===========>..................] - ETA: 11s - loss: 1.1881 - acc: 0.5748\u001b[0m\n",
      "\u001b[34m136/312 [============>.................] - ETA: 11s - loss: 1.1887 - acc: 0.5743\u001b[0m\n",
      "\u001b[34m137/312 [============>.................] - ETA: 11s - loss: 1.1883 - acc: 0.5747\u001b[0m\n",
      "\u001b[34m138/312 [============>.................] - ETA: 11s - loss: 1.1877 - acc: 0.5752\u001b[0m\n",
      "\u001b[34m139/312 [============>.................] - ETA: 10s - loss: 1.1875 - acc: 0.5755\u001b[0m\n",
      "\u001b[34m140/312 [============>.................] - ETA: 10s - loss: 1.1877 - acc: 0.5754\u001b[0m\n",
      "\u001b[34m141/312 [============>.................] - ETA: 10s - loss: 1.1882 - acc: 0.5748\u001b[0m\n",
      "\u001b[34m142/312 [============>.................] - ETA: 10s - loss: 1.1871 - acc: 0.5752\u001b[0m\n",
      "\u001b[34m143/312 [============>.................] - ETA: 10s - loss: 1.1868 - acc: 0.5759\u001b[0m\n",
      "\u001b[34m144/312 [============>.................] - ETA: 10s - loss: 1.1863 - acc: 0.5764\u001b[0m\n",
      "\u001b[34m145/312 [============>.................] - ETA: 10s - loss: 1.1871 - acc: 0.5762\u001b[0m\n",
      "\u001b[34m146/312 [=============>................] - ETA: 10s - loss: 1.1872 - acc: 0.5760\u001b[0m\n",
      "\u001b[34m147/312 [=============>................] - ETA: 10s - loss: 1.1858 - acc: 0.5765\u001b[0m\n",
      "\u001b[34m148/312 [=============>................] - ETA: 10s - loss: 1.1861 - acc: 0.5764\u001b[0m\n",
      "\u001b[34m149/312 [=============>................] - ETA: 10s - loss: 1.1858 - acc: 0.5764\u001b[0m\n",
      "\u001b[34m150/312 [=============>................] - ETA: 10s - loss: 1.1842 - acc: 0.5767\u001b[0m\n",
      "\u001b[34m151/312 [=============>................] - ETA: 10s - loss: 1.1834 - acc: 0.5771\u001b[0m\n",
      "\u001b[34m152/312 [=============>................] - ETA: 10s - loss: 1.1823 - acc: 0.5771\u001b[0m\n",
      "\u001b[34m153/312 [=============>................] - ETA: 10s - loss: 1.1818 - acc: 0.5772\u001b[0m\n",
      "\u001b[34m154/312 [=============>................] - ETA: 10s - loss: 1.1810 - acc: 0.5773\u001b[0m\n",
      "\u001b[34m155/312 [=============>................] - ETA: 9s - loss: 1.1801 - acc: 0.5778 \u001b[0m\n",
      "\u001b[34m156/312 [==============>...............] - ETA: 9s - loss: 1.1801 - acc: 0.5777\u001b[0m\n",
      "\u001b[34m157/312 [==============>...............] - ETA: 9s - loss: 1.1799 - acc: 0.5775\u001b[0m\n",
      "\u001b[34m158/312 [==============>...............] - ETA: 9s - loss: 1.1797 - acc: 0.5773\u001b[0m\n",
      "\u001b[34m159/312 [==============>...............] - ETA: 9s - loss: 1.1793 - acc: 0.5776\u001b[0m\n",
      "\u001b[34m160/312 [==============>...............] - ETA: 9s - loss: 1.1791 - acc: 0.5779\u001b[0m\n",
      "\u001b[34m161/312 [==============>...............] - ETA: 9s - loss: 1.1787 - acc: 0.5778\u001b[0m\n",
      "\u001b[34m162/312 [==============>...............] - ETA: 9s - loss: 1.1782 - acc: 0.5778\u001b[0m\n",
      "\u001b[34m163/312 [==============>...............] - ETA: 9s - loss: 1.1781 - acc: 0.5776\u001b[0m\n",
      "\u001b[34m164/312 [==============>...............] - ETA: 9s - loss: 1.1777 - acc: 0.5776\u001b[0m\n",
      "\u001b[34m165/312 [==============>...............] - ETA: 9s - loss: 1.1776 - acc: 0.5779\u001b[0m\n",
      "\u001b[34m166/312 [==============>...............] - ETA: 9s - loss: 1.1779 - acc: 0.5779\u001b[0m\n",
      "\u001b[34m167/312 [===============>..............] - ETA: 9s - loss: 1.1788 - acc: 0.5778\u001b[0m\n",
      "\u001b[34m168/312 [===============>..............] - ETA: 9s - loss: 1.1775 - acc: 0.5781\u001b[0m\n",
      "\u001b[34m169/312 [===============>..............] - ETA: 9s - loss: 1.1774 - acc: 0.5783\u001b[0m\n",
      "\u001b[34m170/312 [===============>..............] - ETA: 8s - loss: 1.1764 - acc: 0.5787\u001b[0m\n",
      "\u001b[34m171/312 [===============>..............] - ETA: 8s - loss: 1.1768 - acc: 0.5791\u001b[0m\n",
      "\u001b[34m172/312 [===============>..............] - ETA: 8s - loss: 1.1771 - acc: 0.5791\u001b[0m\n",
      "\u001b[34m173/312 [===============>..............] - ETA: 8s - loss: 1.1771 - acc: 0.5786\u001b[0m\n",
      "\u001b[34m174/312 [===============>..............] - ETA: 8s - loss: 1.1763 - acc: 0.5789\u001b[0m\n",
      "\u001b[34m175/312 [===============>..............] - ETA: 8s - loss: 1.1757 - acc: 0.5794\u001b[0m\n",
      "\u001b[34m176/312 [===============>..............] - ETA: 8s - loss: 1.1763 - acc: 0.5792\u001b[0m\n",
      "\u001b[34m177/312 [================>.............] - ETA: 8s - loss: 1.1768 - acc: 0.5790\u001b[0m\n",
      "\u001b[34m178/312 [================>.............] - ETA: 8s - loss: 1.1773 - acc: 0.5788\u001b[0m\n",
      "\u001b[34m179/312 [================>.............] - ETA: 8s - loss: 1.1767 - acc: 0.5793\u001b[0m\n",
      "\u001b[34m180/312 [================>.............] - ETA: 8s - loss: 1.1764 - acc: 0.5793\u001b[0m\n",
      "\u001b[34m181/312 [================>.............] - ETA: 8s - loss: 1.1755 - acc: 0.5795\u001b[0m\n",
      "\u001b[34m182/312 [================>.............] - ETA: 8s - loss: 1.1756 - acc: 0.5796\u001b[0m\n",
      "\u001b[34m183/312 [================>.............] - ETA: 8s - loss: 1.1754 - acc: 0.5798\u001b[0m\n",
      "\u001b[34m184/312 [================>.............] - ETA: 8s - loss: 1.1753 - acc: 0.5796\u001b[0m\n",
      "\u001b[34m185/312 [================>.............] - ETA: 8s - loss: 1.1758 - acc: 0.5794\u001b[0m\n",
      "\u001b[34m186/312 [================>.............] - ETA: 7s - loss: 1.1761 - acc: 0.5796\u001b[0m\n",
      "\u001b[34m187/312 [================>.............] - ETA: 7s - loss: 1.1756 - acc: 0.5801\u001b[0m\n",
      "\u001b[34m188/312 [=================>............] - ETA: 7s - loss: 1.1762 - acc: 0.5797\u001b[0m\n",
      "\u001b[34m189/312 [=================>............] - ETA: 7s - loss: 1.1757 - acc: 0.5800\u001b[0m\n",
      "\u001b[34m190/312 [=================>............] - ETA: 7s - loss: 1.1749 - acc: 0.5804\u001b[0m\n",
      "\u001b[34m191/312 [=================>............] - ETA: 7s - loss: 1.1758 - acc: 0.5802\u001b[0m\n",
      "\u001b[34m192/312 [=================>............] - ETA: 7s - loss: 1.1756 - acc: 0.5802\u001b[0m\n",
      "\u001b[34m193/312 [=================>............] - ETA: 7s - loss: 1.1763 - acc: 0.5801\u001b[0m\n",
      "\u001b[34m194/312 [=================>............] - ETA: 7s - loss: 1.1758 - acc: 0.5806\u001b[0m\n",
      "\u001b[34m195/312 [=================>............] - ETA: 7s - loss: 1.1763 - acc: 0.5804\u001b[0m\n",
      "\u001b[34m196/312 [=================>............] - ETA: 7s - loss: 1.1764 - acc: 0.5801\u001b[0m\n",
      "\u001b[34m197/312 [=================>............] - ETA: 7s - loss: 1.1755 - acc: 0.5805\u001b[0m\n",
      "\u001b[34m198/312 [==================>...........] - ETA: 7s - loss: 1.1754 - acc: 0.5807\u001b[0m\n",
      "\u001b[34m199/312 [==================>...........] - ETA: 7s - loss: 1.1752 - acc: 0.5806\u001b[0m\n",
      "\u001b[34m200/312 [==================>...........] - ETA: 7s - loss: 1.1747 - acc: 0.5807\u001b[0m\n",
      "\u001b[34m201/312 [==================>...........] - ETA: 7s - loss: 1.1752 - acc: 0.5805\u001b[0m\n",
      "\u001b[34m202/312 [==================>...........] - ETA: 6s - loss: 1.1754 - acc: 0.5802\u001b[0m\n",
      "\u001b[34m203/312 [==================>...........] - ETA: 6s - loss: 1.1760 - acc: 0.5801\u001b[0m\n",
      "\u001b[34m204/312 [==================>...........] - ETA: 6s - loss: 1.1753 - acc: 0.5803\u001b[0m\n",
      "\u001b[34m205/312 [==================>...........] - ETA: 6s - loss: 1.1745 - acc: 0.5806\u001b[0m\n",
      "\u001b[34m206/312 [==================>...........] - ETA: 6s - loss: 1.1742 - acc: 0.5806\u001b[0m\n",
      "\u001b[34m207/312 [==================>...........] - ETA: 6s - loss: 1.1733 - acc: 0.5808\u001b[0m\n",
      "\u001b[34m208/312 [===================>..........] - ETA: 6s - loss: 1.1730 - acc: 0.5812\u001b[0m\n",
      "\u001b[34m209/312 [===================>..........] - ETA: 6s - loss: 1.1721 - acc: 0.5814\u001b[0m\n",
      "\u001b[34m210/312 [===================>..........] - ETA: 6s - loss: 1.1726 - acc: 0.5812\u001b[0m\n",
      "\u001b[34m211/312 [===================>..........] - ETA: 6s - loss: 1.1730 - acc: 0.5810\u001b[0m\n",
      "\u001b[34m212/312 [===================>..........] - ETA: 6s - loss: 1.1727 - acc: 0.5809\u001b[0m\n",
      "\u001b[34m213/312 [===================>..........] - ETA: 6s - loss: 1.1722 - acc: 0.5809\u001b[0m\n",
      "\u001b[34m214/312 [===================>..........] - ETA: 6s - loss: 1.1719 - acc: 0.5813\u001b[0m\n",
      "\u001b[34m215/312 [===================>..........] - ETA: 6s - loss: 1.1720 - acc: 0.5812\u001b[0m\n",
      "\u001b[34m216/312 [===================>..........] - ETA: 6s - loss: 1.1704 - acc: 0.5817\u001b[0m\n",
      "\u001b[34m217/312 [===================>..........] - ETA: 6s - loss: 1.1708 - acc: 0.5817\u001b[0m\n",
      "\u001b[34m218/312 [===================>..........] - ETA: 5s - loss: 1.1703 - acc: 0.5818\u001b[0m\n",
      "\u001b[34m219/312 [====================>.........] - ETA: 5s - loss: 1.1706 - acc: 0.5818\u001b[0m\n",
      "\u001b[34m220/312 [====================>.........] - ETA: 5s - loss: 1.1693 - acc: 0.5823\u001b[0m\n",
      "\u001b[34m221/312 [====================>.........] - ETA: 5s - loss: 1.1688 - acc: 0.5824\u001b[0m\n",
      "\u001b[34m222/312 [====================>.........] - ETA: 5s - loss: 1.1690 - acc: 0.5822\u001b[0m\n",
      "\u001b[34m223/312 [====================>.........] - ETA: 5s - loss: 1.1689 - acc: 0.5820\u001b[0m\n",
      "\u001b[34m224/312 [====================>.........] - ETA: 5s - loss: 1.1692 - acc: 0.5821\u001b[0m\n",
      "\u001b[34m225/312 [====================>.........] - ETA: 5s - loss: 1.1691 - acc: 0.5821\u001b[0m\n",
      "\u001b[34m226/312 [====================>.........] - ETA: 5s - loss: 1.1685 - acc: 0.5824\u001b[0m\n",
      "\u001b[34m227/312 [====================>.........] - ETA: 5s - loss: 1.1683 - acc: 0.5822\u001b[0m\n",
      "\u001b[34m228/312 [====================>.........] - ETA: 5s - loss: 1.1688 - acc: 0.5821\u001b[0m\n",
      "\u001b[34m229/312 [=====================>........] - ETA: 5s - loss: 1.1686 - acc: 0.5820\u001b[0m\n",
      "\u001b[34m230/312 [=====================>........] - ETA: 5s - loss: 1.1687 - acc: 0.5818\u001b[0m\n",
      "\u001b[34m231/312 [=====================>........] - ETA: 5s - loss: 1.1684 - acc: 0.5817\u001b[0m\n",
      "\u001b[34m232/312 [=====================>........] - ETA: 5s - loss: 1.1680 - acc: 0.5819\u001b[0m\n",
      "\u001b[34m233/312 [=====================>........] - ETA: 5s - loss: 1.1686 - acc: 0.5816\u001b[0m\n",
      "\u001b[34m234/312 [=====================>........] - ETA: 4s - loss: 1.1680 - acc: 0.5817\u001b[0m\n",
      "\u001b[34m235/312 [=====================>........] - ETA: 4s - loss: 1.1676 - acc: 0.5820\u001b[0m\n",
      "\u001b[34m236/312 [=====================>........] - ETA: 4s - loss: 1.1669 - acc: 0.5821\u001b[0m\n",
      "\u001b[34m237/312 [=====================>........] - ETA: 4s - loss: 1.1674 - acc: 0.5819\u001b[0m\n",
      "\u001b[34m238/312 [=====================>........] - ETA: 4s - loss: 1.1672 - acc: 0.5820\u001b[0m\n",
      "\u001b[34m239/312 [=====================>........] - ETA: 4s - loss: 1.1681 - acc: 0.5818\u001b[0m\n",
      "\u001b[34m240/312 [======================>.......] - ETA: 4s - loss: 1.1682 - acc: 0.5816\u001b[0m\n",
      "\u001b[34m241/312 [======================>.......] - ETA: 4s - loss: 1.1683 - acc: 0.5817\u001b[0m\n",
      "\u001b[34m242/312 [======================>.......] - ETA: 4s - loss: 1.1679 - acc: 0.5819\u001b[0m\n",
      "\u001b[34m243/312 [======================>.......] - ETA: 4s - loss: 1.1677 - acc: 0.5820\u001b[0m\n",
      "\u001b[34m244/312 [======================>.......] - ETA: 4s - loss: 1.1675 - acc: 0.5819\u001b[0m\n",
      "\u001b[34m245/312 [======================>.......] - ETA: 4s - loss: 1.1672 - acc: 0.5821\u001b[0m\n",
      "\u001b[34m246/312 [======================>.......] - ETA: 4s - loss: 1.1677 - acc: 0.5820\u001b[0m\n",
      "\u001b[34m247/312 [======================>.......] - ETA: 4s - loss: 1.1679 - acc: 0.5820\u001b[0m\n",
      "\u001b[34m248/312 [======================>.......] - ETA: 4s - loss: 1.1682 - acc: 0.5819\u001b[0m\n",
      "\u001b[34m249/312 [======================>.......] - ETA: 3s - loss: 1.1681 - acc: 0.5818\u001b[0m\n",
      "\u001b[34m250/312 [=======================>......] - ETA: 3s - loss: 1.1681 - acc: 0.5819\u001b[0m\n",
      "\u001b[34m251/312 [=======================>......] - ETA: 3s - loss: 1.1674 - acc: 0.5823\u001b[0m\n",
      "\u001b[34m252/312 [=======================>......] - ETA: 3s - loss: 1.1679 - acc: 0.5819\u001b[0m\n",
      "\u001b[34m253/312 [=======================>......] - ETA: 3s - loss: 1.1673 - acc: 0.5820\u001b[0m\n",
      "\u001b[34m254/312 [=======================>......] - ETA: 3s - loss: 1.1669 - acc: 0.5821\u001b[0m\n",
      "\u001b[34m255/312 [=======================>......] - ETA: 3s - loss: 1.1670 - acc: 0.5823\u001b[0m\n",
      "\u001b[34m256/312 [=======================>......] - ETA: 3s - loss: 1.1672 - acc: 0.5819\u001b[0m\n",
      "\u001b[34m257/312 [=======================>......] - ETA: 3s - loss: 1.1674 - acc: 0.5818\u001b[0m\n",
      "\u001b[34m258/312 [=======================>......] - ETA: 3s - loss: 1.1669 - acc: 0.5818\u001b[0m\n",
      "\u001b[34m259/312 [=======================>......] - ETA: 3s - loss: 1.1676 - acc: 0.5815\u001b[0m\n",
      "\u001b[34m260/312 [========================>.....] - ETA: 3s - loss: 1.1674 - acc: 0.5816\u001b[0m\n",
      "\u001b[34m261/312 [========================>.....] - ETA: 3s - loss: 1.1674 - acc: 0.5816\u001b[0m\n",
      "\u001b[34m262/312 [========================>.....] - ETA: 3s - loss: 1.1672 - acc: 0.5818\u001b[0m\n",
      "\u001b[34m263/312 [========================>.....] - ETA: 3s - loss: 1.1672 - acc: 0.5818\u001b[0m\n",
      "\u001b[34m264/312 [========================>.....] - ETA: 3s - loss: 1.1667 - acc: 0.5820\u001b[0m\n",
      "\u001b[34m265/312 [========================>.....] - ETA: 2s - loss: 1.1660 - acc: 0.5819\u001b[0m\n",
      "\u001b[34m266/312 [========================>.....] - ETA: 2s - loss: 1.1655 - acc: 0.5822\u001b[0m\n",
      "\u001b[34m267/312 [========================>.....] - ETA: 2s - loss: 1.1652 - acc: 0.5823\u001b[0m\n",
      "\u001b[34m268/312 [========================>.....] - ETA: 2s - loss: 1.1654 - acc: 0.5821\u001b[0m\n",
      "\u001b[34m269/312 [========================>.....] - ETA: 2s - loss: 1.1659 - acc: 0.5819\u001b[0m\n",
      "\u001b[34m270/312 [========================>.....] - ETA: 2s - loss: 1.1655 - acc: 0.5824\u001b[0m\n",
      "\u001b[34m271/312 [=========================>....] - ETA: 2s - loss: 1.1649 - acc: 0.5825\u001b[0m\n",
      "\u001b[34m272/312 [=========================>....] - ETA: 2s - loss: 1.1646 - acc: 0.5826\u001b[0m\n",
      "\u001b[34m273/312 [=========================>....] - ETA: 2s - loss: 1.1638 - acc: 0.5829\u001b[0m\n",
      "\u001b[34m274/312 [=========================>....] - ETA: 2s - loss: 1.1634 - acc: 0.5831\u001b[0m\n",
      "\u001b[34m275/312 [=========================>....] - ETA: 2s - loss: 1.1636 - acc: 0.5831\u001b[0m\n",
      "\u001b[34m276/312 [=========================>....] - ETA: 2s - loss: 1.1635 - acc: 0.5830\u001b[0m\n",
      "\u001b[34m277/312 [=========================>....] - ETA: 2s - loss: 1.1629 - acc: 0.5832\u001b[0m\n",
      "\u001b[34m278/312 [=========================>....] - ETA: 2s - loss: 1.1620 - acc: 0.5837\u001b[0m\n",
      "\u001b[34m279/312 [=========================>....] - ETA: 2s - loss: 1.1611 - acc: 0.5839\u001b[0m\n",
      "\u001b[34m280/312 [=========================>....] - ETA: 2s - loss: 1.1610 - acc: 0.5839\u001b[0m\n",
      "\u001b[34m281/312 [==========================>...] - ETA: 1s - loss: 1.1601 - acc: 0.5842\u001b[0m\n",
      "\u001b[34m282/312 [==========================>...] - ETA: 1s - loss: 1.1596 - acc: 0.5842\u001b[0m\n",
      "\u001b[34m283/312 [==========================>...] - ETA: 1s - loss: 1.1601 - acc: 0.5840\u001b[0m\n",
      "\u001b[34m284/312 [==========================>...] - ETA: 1s - loss: 1.1598 - acc: 0.5843\u001b[0m\n",
      "\u001b[34m285/312 [==========================>...] - ETA: 1s - loss: 1.1597 - acc: 0.5845\u001b[0m\n",
      "\u001b[34m286/312 [==========================>...] - ETA: 1s - loss: 1.1591 - acc: 0.5847\u001b[0m\n",
      "\u001b[34m287/312 [==========================>...] - ETA: 1s - loss: 1.1588 - acc: 0.5847\u001b[0m\n",
      "\u001b[34m288/312 [==========================>...] - ETA: 1s - loss: 1.1584 - acc: 0.5850\u001b[0m\n",
      "\u001b[34m289/312 [==========================>...] - ETA: 1s - loss: 1.1578 - acc: 0.5852\u001b[0m\n",
      "\u001b[34m290/312 [==========================>...] - ETA: 1s - loss: 1.1577 - acc: 0.5852\u001b[0m\n",
      "\u001b[34m291/312 [==========================>...] - ETA: 1s - loss: 1.1568 - acc: 0.5854\u001b[0m\n",
      "\u001b[34m292/312 [===========================>..] - ETA: 1s - loss: 1.1562 - acc: 0.5857\u001b[0m\n",
      "\u001b[34m293/312 [===========================>..] - ETA: 1s - loss: 1.1565 - acc: 0.5856\u001b[0m\n",
      "\u001b[34m294/312 [===========================>..] - ETA: 1s - loss: 1.1561 - acc: 0.5855\u001b[0m\n",
      "\u001b[34m295/312 [===========================>..] - ETA: 1s - loss: 1.1556 - acc: 0.5856\u001b[0m\n",
      "\u001b[34m296/312 [===========================>..] - ETA: 1s - loss: 1.1560 - acc: 0.5857\u001b[0m\n",
      "\u001b[34m297/312 [===========================>..] - ETA: 0s - loss: 1.1564 - acc: 0.5856\u001b[0m\n",
      "\u001b[34m298/312 [===========================>..] - ETA: 0s - loss: 1.1560 - acc: 0.5858\u001b[0m\n",
      "\u001b[34m299/312 [===========================>..] - ETA: 0s - loss: 1.1559 - acc: 0.5859\u001b[0m\n",
      "\u001b[34m300/312 [===========================>..] - ETA: 0s - loss: 1.1552 - acc: 0.5862\u001b[0m\n",
      "\u001b[34m301/312 [===========================>..] - ETA: 0s - loss: 1.1549 - acc: 0.5862\u001b[0m\n",
      "\u001b[34m302/312 [============================>.] - ETA: 0s - loss: 1.1541 - acc: 0.5864\u001b[0m\n",
      "\u001b[34m303/312 [============================>.] - ETA: 0s - loss: 1.1536 - acc: 0.5866\u001b[0m\n",
      "\u001b[34m304/312 [============================>.] - ETA: 0s - loss: 1.1538 - acc: 0.5865\u001b[0m\n",
      "\u001b[34m305/312 [============================>.] - ETA: 0s - loss: 1.1537 - acc: 0.5866\u001b[0m\n",
      "\u001b[34m306/312 [============================>.] - ETA: 0s - loss: 1.1532 - acc: 0.5869\u001b[0m\n",
      "\u001b[34m307/312 [============================>.] - ETA: 0s - loss: 1.1534 - acc: 0.5870\u001b[0m\n",
      "\u001b[34m308/312 [============================>.] - ETA: 0s - loss: 1.1531 - acc: 0.5871\u001b[0m\n",
      "\u001b[34m309/312 [============================>.] - ETA: 0s - loss: 1.1526 - acc: 0.5871\u001b[0m\n",
      "\u001b[34m310/312 [============================>.] - ETA: 0s - loss: 1.1518 - acc: 0.5873\u001b[0m\n",
      "\u001b[34m311/312 [============================>.] - ETA: 0s - loss: 1.1516 - acc: 0.5873\u001b[0m\n",
      "\u001b[34m312/312 [==============================] - 22s 70ms/step - loss: 1.1518 - acc: 0.5871 - val_loss: 1.0429 - val_acc: 0.6191\u001b[0m\n",
      "\u001b[34mEpoch 5/5\n",
      "\n",
      "  1/312 [..............................] - ETA: 19s - loss: 0.8368 - acc: 0.7266\n",
      "  2/312 [..............................] - ETA: 19s - loss: 0.9058 - acc: 0.6836\n",
      "  3/312 [..............................] - ETA: 19s - loss: 0.9377 - acc: 0.6693\u001b[0m\n",
      "\u001b[34m  4/312 [..............................] - ETA: 19s - loss: 0.9523 - acc: 0.6582\n",
      "  5/312 [..............................] - ETA: 19s - loss: 0.9961 - acc: 0.6438\n",
      "  6/312 [..............................] - ETA: 19s - loss: 1.0141 - acc: 0.6406\n",
      "  7/312 [..............................] - ETA: 19s - loss: 1.0365 - acc: 0.6317\n",
      "  8/312 [..............................] - ETA: 19s - loss: 1.0255 - acc: 0.6387\n",
      "  9/312 [..............................] - ETA: 19s - loss: 1.0192 - acc: 0.6363\n",
      " 10/312 [..............................] - ETA: 19s - loss: 1.0238 - acc: 0.6359\n",
      " 11/312 [>.............................] - ETA: 19s - loss: 1.0230 - acc: 0.6328\n",
      " 12/312 [>.............................] - ETA: 19s - loss: 1.0229 - acc: 0.6341\n",
      " 13/312 [>.............................] - ETA: 18s - loss: 1.0176 - acc: 0.6334\n",
      " 14/312 [>.............................] - ETA: 18s - loss: 1.0288 - acc: 0.6306\n",
      " 15/312 [>.............................] - ETA: 18s - loss: 1.0395 - acc: 0.6260\n",
      " 16/312 [>.............................] - ETA: 18s - loss: 1.0457 - acc: 0.6245\n",
      " 17/312 [>.............................] - ETA: 18s - loss: 1.0502 - acc: 0.6222\n",
      " 18/312 [>.............................] - ETA: 18s - loss: 1.0508 - acc: 0.6246\u001b[0m\n",
      "\u001b[34m 19/312 [>.............................] - ETA: 18s - loss: 1.0491 - acc: 0.6234\n",
      " 20/312 [>.............................] - ETA: 18s - loss: 1.0443 - acc: 0.6254\n",
      " 21/312 [=>............................] - ETA: 18s - loss: 1.0476 - acc: 0.6213\n",
      " 22/312 [=>............................] - ETA: 18s - loss: 1.0486 - acc: 0.6222\n",
      " 23/312 [=>............................] - ETA: 18s - loss: 1.0522 - acc: 0.6206\n",
      " 24/312 [=>............................] - ETA: 18s - loss: 1.0504 - acc: 0.6217\n",
      " 25/312 [=>............................] - ETA: 18s - loss: 1.0508 - acc: 0.6200\n",
      " 26/312 [=>............................] - ETA: 18s - loss: 1.0484 - acc: 0.6223\n",
      " 27/312 [=>............................] - ETA: 17s - loss: 1.0461 - acc: 0.6230\n",
      " 28/312 [=>............................] - ETA: 17s - loss: 1.0489 - acc: 0.6208\n",
      " 29/312 [=>............................] - ETA: 17s - loss: 1.0517 - acc: 0.6202\n",
      " 30/312 [=>............................] - ETA: 17s - loss: 1.0534 - acc: 0.6216\n",
      " 31/312 [=>............................] - ETA: 17s - loss: 1.0544 - acc: 0.6217\n",
      " 32/312 [==>...........................] - ETA: 17s - loss: 1.0575 - acc: 0.6204\n",
      " 33/312 [==>...........................] - ETA: 17s - loss: 1.0617 - acc: 0.6193\n",
      " 34/312 [==>...........................] - ETA: 17s - loss: 1.0615 - acc: 0.6199\u001b[0m\n",
      "\u001b[34m 35/312 [==>...........................] - ETA: 17s - loss: 1.0570 - acc: 0.6212\n",
      " 36/312 [==>...........................] - ETA: 17s - loss: 1.0609 - acc: 0.6198\n",
      " 37/312 [==>...........................] - ETA: 17s - loss: 1.0605 - acc: 0.6197\n",
      " 38/312 [==>...........................] - ETA: 17s - loss: 1.0595 - acc: 0.6188\n",
      " 39/312 [==>...........................] - ETA: 17s - loss: 1.0613 - acc: 0.6182\n",
      " 40/312 [==>...........................] - ETA: 17s - loss: 1.0578 - acc: 0.6191\n",
      " 41/312 [==>...........................] - ETA: 17s - loss: 1.0618 - acc: 0.6170\n",
      " 42/312 [===>..........................] - ETA: 17s - loss: 1.0588 - acc: 0.6168\n",
      " 43/312 [===>..........................] - ETA: 17s - loss: 1.0608 - acc: 0.6152\n",
      " 44/312 [===>..........................] - ETA: 17s - loss: 1.0619 - acc: 0.6145\n",
      " 45/312 [===>..........................] - ETA: 17s - loss: 1.0623 - acc: 0.6128\n",
      " 46/312 [===>..........................] - ETA: 17s - loss: 1.0613 - acc: 0.6135\n",
      " 47/312 [===>..........................] - ETA: 16s - loss: 1.0609 - acc: 0.6139\n",
      " 48/312 [===>..........................] - ETA: 16s - loss: 1.0591 - acc: 0.6147\n",
      " 49/312 [===>..........................] - ETA: 16s - loss: 1.0589 - acc: 0.6151\n",
      " 50/312 [===>..........................] - ETA: 16s - loss: 1.0596 - acc: 0.6148\u001b[0m\n",
      "\u001b[34m 51/312 [===>..........................] - ETA: 16s - loss: 1.0559 - acc: 0.6157\n",
      " 52/312 [====>.........................] - ETA: 16s - loss: 1.0535 - acc: 0.6164\n",
      " 53/312 [====>.........................] - ETA: 16s - loss: 1.0536 - acc: 0.6167\n",
      " 54/312 [====>.........................] - ETA: 16s - loss: 1.0524 - acc: 0.6173\n",
      " 55/312 [====>.........................] - ETA: 16s - loss: 1.0519 - acc: 0.6176\n",
      " 56/312 [====>.........................] - ETA: 16s - loss: 1.0524 - acc: 0.6180\n",
      " 57/312 [====>.........................] - ETA: 16s - loss: 1.0534 - acc: 0.6171\n",
      " 58/312 [====>.........................] - ETA: 16s - loss: 1.0540 - acc: 0.6173\n",
      " 59/312 [====>.........................] - ETA: 16s - loss: 1.0558 - acc: 0.6163\n",
      " 60/312 [====>.........................] - ETA: 16s - loss: 1.0567 - acc: 0.6165\n",
      " 61/312 [====>.........................] - ETA: 16s - loss: 1.0565 - acc: 0.6160\n",
      " 62/312 [====>.........................] - ETA: 15s - loss: 1.0597 - acc: 0.6157\n",
      " 63/312 [=====>........................] - ETA: 15s - loss: 1.0591 - acc: 0.6161\n",
      " 64/312 [=====>........................] - ETA: 15s - loss: 1.0591 - acc: 0.6156\n",
      " 65/312 [=====>........................] - ETA: 15s - loss: 1.0623 - acc: 0.6150\u001b[0m\n",
      "\u001b[34m 66/312 [=====>........................] - ETA: 15s - loss: 1.0604 - acc: 0.6153\n",
      " 67/312 [=====>........................] - ETA: 15s - loss: 1.0591 - acc: 0.6154\n",
      " 68/312 [=====>........................] - ETA: 15s - loss: 1.0622 - acc: 0.6144\n",
      " 69/312 [=====>........................] - ETA: 15s - loss: 1.0651 - acc: 0.6138\n",
      " 70/312 [=====>........................] - ETA: 15s - loss: 1.0678 - acc: 0.6135\n",
      " 71/312 [=====>........................] - ETA: 15s - loss: 1.0675 - acc: 0.6126\n",
      " 72/312 [=====>........................] - ETA: 15s - loss: 1.0682 - acc: 0.6121\n",
      " 73/312 [======>.......................] - ETA: 15s - loss: 1.0693 - acc: 0.6119\n",
      " 74/312 [======>.......................] - ETA: 15s - loss: 1.0706 - acc: 0.6118\n",
      " 75/312 [======>.......................] - ETA: 15s - loss: 1.0709 - acc: 0.6120\n",
      " 76/312 [======>.......................] - ETA: 15s - loss: 1.0696 - acc: 0.6123\n",
      " 77/312 [======>.......................] - ETA: 14s - loss: 1.0706 - acc: 0.6121\n",
      " 78/312 [======>.......................] - ETA: 14s - loss: 1.0716 - acc: 0.6116\n",
      " 79/312 [======>.......................] - ETA: 14s - loss: 1.0717 - acc: 0.6116\n",
      " 80/312 [======>.......................] - ETA: 14s - loss: 1.0740 - acc: 0.6108\n",
      " 81/312 [======>.......................] - ETA: 14s - loss: 1.0739 - acc: 0.6106\u001b[0m\n",
      "\u001b[34m 82/312 [======>.......................] - ETA: 14s - loss: 1.0743 - acc: 0.6110\n",
      " 83/312 [======>.......................] - ETA: 14s - loss: 1.0743 - acc: 0.6109\n",
      " 84/312 [=======>......................] - ETA: 14s - loss: 1.0745 - acc: 0.6105\n",
      " 85/312 [=======>......................] - ETA: 14s - loss: 1.0736 - acc: 0.6110\n",
      " 86/312 [=======>......................] - ETA: 14s - loss: 1.0731 - acc: 0.6117\n",
      " 87/312 [=======>......................] - ETA: 14s - loss: 1.0714 - acc: 0.6125\n",
      " 88/312 [=======>......................] - ETA: 14s - loss: 1.0734 - acc: 0.6121\n",
      " 89/312 [=======>......................] - ETA: 14s - loss: 1.0755 - acc: 0.6114\n",
      " 90/312 [=======>......................] - ETA: 14s - loss: 1.0758 - acc: 0.6115\n",
      " 91/312 [=======>......................] - ETA: 14s - loss: 1.0749 - acc: 0.6115\n",
      " 92/312 [=======>......................] - ETA: 13s - loss: 1.0736 - acc: 0.6122\n",
      " 93/312 [=======>......................] - ETA: 13s - loss: 1.0729 - acc: 0.6123\n",
      " 94/312 [========>.....................] - ETA: 13s - loss: 1.0747 - acc: 0.6114\n",
      " 95/312 [========>.....................] - ETA: 13s - loss: 1.0747 - acc: 0.6112\n",
      " 96/312 [========>.....................] - ETA: 13s - loss: 1.0755 - acc: 0.6106\n",
      " 97/312 [========>.....................] - ETA: 13s - loss: 1.0767 - acc: 0.6105\u001b[0m\n",
      "\u001b[34m 98/312 [========>.....................] - ETA: 13s - loss: 1.0775 - acc: 0.6094\n",
      " 99/312 [========>.....................] - ETA: 13s - loss: 1.0772 - acc: 0.6095\u001b[0m\n",
      "\u001b[34m100/312 [========>.....................] - ETA: 13s - loss: 1.0776 - acc: 0.6092\u001b[0m\n",
      "\u001b[34m101/312 [========>.....................] - ETA: 13s - loss: 1.0775 - acc: 0.6093\u001b[0m\n",
      "\u001b[34m102/312 [========>.....................] - ETA: 13s - loss: 1.0780 - acc: 0.6092\u001b[0m\n",
      "\u001b[34m103/312 [========>.....................] - ETA: 13s - loss: 1.0777 - acc: 0.6093\u001b[0m\n",
      "\u001b[34m104/312 [=========>....................] - ETA: 13s - loss: 1.0789 - acc: 0.6086\u001b[0m\n",
      "\u001b[34m105/312 [=========>....................] - ETA: 13s - loss: 1.0801 - acc: 0.6081\u001b[0m\n",
      "\u001b[34m106/312 [=========>....................] - ETA: 13s - loss: 1.0790 - acc: 0.6082\u001b[0m\n",
      "\u001b[34m107/312 [=========>....................] - ETA: 13s - loss: 1.0777 - acc: 0.6085\u001b[0m\n",
      "\u001b[34m108/312 [=========>....................] - ETA: 12s - loss: 1.0745 - acc: 0.6099\u001b[0m\n",
      "\u001b[34m109/312 [=========>....................] - ETA: 12s - loss: 1.0732 - acc: 0.6103\u001b[0m\n",
      "\u001b[34m110/312 [=========>....................] - ETA: 12s - loss: 1.0722 - acc: 0.6104\u001b[0m\n",
      "\u001b[34m111/312 [=========>....................] - ETA: 12s - loss: 1.0713 - acc: 0.6105\u001b[0m\n",
      "\u001b[34m112/312 [=========>....................] - ETA: 12s - loss: 1.0724 - acc: 0.6099\u001b[0m\n",
      "\u001b[34m113/312 [=========>....................] - ETA: 12s - loss: 1.0719 - acc: 0.6099\u001b[0m\n",
      "\u001b[34m114/312 [=========>....................] - ETA: 12s - loss: 1.0695 - acc: 0.6107\u001b[0m\n",
      "\u001b[34m115/312 [==========>...................] - ETA: 12s - loss: 1.0700 - acc: 0.6105\u001b[0m\n",
      "\u001b[34m116/312 [==========>...................] - ETA: 12s - loss: 1.0697 - acc: 0.6109\u001b[0m\n",
      "\u001b[34m117/312 [==========>...................] - ETA: 12s - loss: 1.0706 - acc: 0.6108\u001b[0m\n",
      "\u001b[34m118/312 [==========>...................] - ETA: 12s - loss: 1.0687 - acc: 0.6115\u001b[0m\n",
      "\u001b[34m119/312 [==========>...................] - ETA: 12s - loss: 1.0690 - acc: 0.6114\u001b[0m\n",
      "\u001b[34m120/312 [==========>...................] - ETA: 12s - loss: 1.0685 - acc: 0.6115\u001b[0m\n",
      "\u001b[34m121/312 [==========>...................] - ETA: 12s - loss: 1.0690 - acc: 0.6113\u001b[0m\n",
      "\u001b[34m122/312 [==========>...................] - ETA: 12s - loss: 1.0683 - acc: 0.6117\u001b[0m\n",
      "\u001b[34m123/312 [==========>...................] - ETA: 12s - loss: 1.0677 - acc: 0.6122\u001b[0m\n",
      "\u001b[34m124/312 [==========>...................] - ETA: 12s - loss: 1.0678 - acc: 0.6122\u001b[0m\n",
      "\u001b[34m125/312 [===========>..................] - ETA: 11s - loss: 1.0667 - acc: 0.6128\u001b[0m\n",
      "\u001b[34m126/312 [===========>..................] - ETA: 11s - loss: 1.0672 - acc: 0.6128\u001b[0m\n",
      "\u001b[34m127/312 [===========>..................] - ETA: 11s - loss: 1.0670 - acc: 0.6133\u001b[0m\n",
      "\u001b[34m128/312 [===========>..................] - ETA: 11s - loss: 1.0681 - acc: 0.6132\u001b[0m\n",
      "\u001b[34m129/312 [===========>..................] - ETA: 11s - loss: 1.0678 - acc: 0.6132\u001b[0m\n",
      "\u001b[34m130/312 [===========>..................] - ETA: 11s - loss: 1.0677 - acc: 0.6130\u001b[0m\n",
      "\u001b[34m131/312 [===========>..................] - ETA: 11s - loss: 1.0684 - acc: 0.6129\u001b[0m\n",
      "\u001b[34m132/312 [===========>..................] - ETA: 11s - loss: 1.0695 - acc: 0.6129\u001b[0m\n",
      "\u001b[34m133/312 [===========>..................] - ETA: 11s - loss: 1.0687 - acc: 0.6133\u001b[0m\n",
      "\u001b[34m134/312 [===========>..................] - ETA: 11s - loss: 1.0682 - acc: 0.6136\u001b[0m\n",
      "\u001b[34m135/312 [===========>..................] - ETA: 11s - loss: 1.0675 - acc: 0.6138\u001b[0m\n",
      "\u001b[34m136/312 [============>.................] - ETA: 11s - loss: 1.0677 - acc: 0.6139\u001b[0m\n",
      "\u001b[34m137/312 [============>.................] - ETA: 11s - loss: 1.0682 - acc: 0.6135\u001b[0m\n",
      "\u001b[34m138/312 [============>.................] - ETA: 11s - loss: 1.0677 - acc: 0.6139\u001b[0m\n",
      "\u001b[34m139/312 [============>.................] - ETA: 11s - loss: 1.0675 - acc: 0.6142\u001b[0m\n",
      "\u001b[34m140/312 [============>.................] - ETA: 10s - loss: 1.0670 - acc: 0.6145\u001b[0m\n",
      "\u001b[34m141/312 [============>.................] - ETA: 10s - loss: 1.0665 - acc: 0.6145\u001b[0m\n",
      "\u001b[34m142/312 [============>.................] - ETA: 10s - loss: 1.0658 - acc: 0.6147\u001b[0m\n",
      "\u001b[34m143/312 [============>.................] - ETA: 10s - loss: 1.0665 - acc: 0.6143\u001b[0m\n",
      "\u001b[34m144/312 [============>.................] - ETA: 10s - loss: 1.0660 - acc: 0.6145\u001b[0m\n",
      "\u001b[34m145/312 [============>.................] - ETA: 10s - loss: 1.0656 - acc: 0.6147\u001b[0m\n",
      "\u001b[34m146/312 [=============>................] - ETA: 10s - loss: 1.0655 - acc: 0.6144\u001b[0m\n",
      "\u001b[34m147/312 [=============>................] - ETA: 10s - loss: 1.0647 - acc: 0.6147\u001b[0m\n",
      "\u001b[34m148/312 [=============>................] - ETA: 10s - loss: 1.0638 - acc: 0.6149\u001b[0m\n",
      "\u001b[34m149/312 [=============>................] - ETA: 10s - loss: 1.0637 - acc: 0.6151\u001b[0m\n",
      "\u001b[34m150/312 [=============>................] - ETA: 10s - loss: 1.0641 - acc: 0.6153\u001b[0m\n",
      "\u001b[34m151/312 [=============>................] - ETA: 10s - loss: 1.0631 - acc: 0.6158\u001b[0m\n",
      "\u001b[34m152/312 [=============>................] - ETA: 10s - loss: 1.0621 - acc: 0.6161\u001b[0m\n",
      "\u001b[34m153/312 [=============>................] - ETA: 10s - loss: 1.0624 - acc: 0.6162\u001b[0m\n",
      "\u001b[34m154/312 [=============>................] - ETA: 10s - loss: 1.0623 - acc: 0.6161\u001b[0m\n",
      "\u001b[34m155/312 [=============>................] - ETA: 10s - loss: 1.0623 - acc: 0.6158\u001b[0m\n",
      "\u001b[34m157/312 [==============>...............] - ETA: 9s - loss: 1.0632 - acc: 0.6160 \u001b[0m\n",
      "\u001b[34m159/312 [==============>...............] - ETA: 9s - loss: 1.0642 - acc: 0.6162\u001b[0m\n",
      "\u001b[34m161/312 [==============>...............] - ETA: 9s - loss: 1.0649 - acc: 0.6162\u001b[0m\n",
      "\u001b[34m163/312 [==============>...............] - ETA: 9s - loss: 1.0645 - acc: 0.6163\u001b[0m\n",
      "\u001b[34m165/312 [==============>...............] - ETA: 9s - loss: 1.0640 - acc: 0.6167\u001b[0m\n",
      "\u001b[34m167/312 [===============>..............] - ETA: 9s - loss: 1.0632 - acc: 0.6169\u001b[0m\n",
      "\u001b[34m169/312 [===============>..............] - ETA: 8s - loss: 1.0628 - acc: 0.6171\u001b[0m\n",
      "\u001b[34m171/312 [===============>..............] - ETA: 8s - loss: 1.0632 - acc: 0.6170\u001b[0m\n",
      "\u001b[34m173/312 [===============>..............] - ETA: 8s - loss: 1.0629 - acc: 0.6171\u001b[0m\n",
      "\u001b[34m175/312 [===============>..............] - ETA: 8s - loss: 1.0604 - acc: 0.6178\u001b[0m\n",
      "\u001b[34m177/312 [================>.............] - ETA: 8s - loss: 1.0623 - acc: 0.6177\u001b[0m\n",
      "\u001b[34m179/312 [================>.............] - ETA: 8s - loss: 1.0622 - acc: 0.6181\u001b[0m\n",
      "\u001b[34m181/312 [================>.............] - ETA: 7s - loss: 1.0618 - acc: 0.6184\u001b[0m\n",
      "\u001b[34m183/312 [================>.............] - ETA: 7s - loss: 1.0632 - acc: 0.6180\u001b[0m\n",
      "\u001b[34m185/312 [================>.............] - ETA: 7s - loss: 1.0633 - acc: 0.6185\u001b[0m\n",
      "\u001b[34m187/312 [================>.............] - ETA: 7s - loss: 1.0639 - acc: 0.6182\u001b[0m\n",
      "\u001b[34m189/312 [=================>............] - ETA: 7s - loss: 1.0633 - acc: 0.6185\u001b[0m\n",
      "\u001b[34m191/312 [=================>............] - ETA: 7s - loss: 1.0634 - acc: 0.6187\u001b[0m\n",
      "\u001b[34m193/312 [=================>............] - ETA: 7s - loss: 1.0636 - acc: 0.6189\u001b[0m\n",
      "\u001b[34m195/312 [=================>............] - ETA: 6s - loss: 1.0642 - acc: 0.6188\u001b[0m\n",
      "\u001b[34m197/312 [=================>............] - ETA: 6s - loss: 1.0640 - acc: 0.6187\u001b[0m\n",
      "\u001b[34m199/312 [==================>...........] - ETA: 6s - loss: 1.0639 - acc: 0.6187\u001b[0m\n",
      "\u001b[34m201/312 [==================>...........] - ETA: 6s - loss: 1.0636 - acc: 0.6189\u001b[0m\n",
      "\u001b[34m203/312 [==================>...........] - ETA: 6s - loss: 1.0629 - acc: 0.6193\u001b[0m\n",
      "\u001b[34m205/312 [==================>...........] - ETA: 6s - loss: 1.0640 - acc: 0.6192\u001b[0m\n",
      "\u001b[34m207/312 [==================>...........] - ETA: 6s - loss: 1.0637 - acc: 0.6193\u001b[0m\n",
      "\u001b[34m209/312 [===================>..........] - ETA: 5s - loss: 1.0634 - acc: 0.6196\u001b[0m\n",
      "\u001b[34m211/312 [===================>..........] - ETA: 5s - loss: 1.0638 - acc: 0.6191\u001b[0m\n",
      "\u001b[34m213/312 [===================>..........] - ETA: 5s - loss: 1.0649 - acc: 0.6189\u001b[0m\n",
      "\u001b[34m215/312 [===================>..........] - ETA: 5s - loss: 1.0661 - acc: 0.6188\u001b[0m\n",
      "\u001b[34m217/312 [===================>..........] - ETA: 5s - loss: 1.0673 - acc: 0.6182\u001b[0m\n",
      "\u001b[34m219/312 [====================>.........] - ETA: 5s - loss: 1.0673 - acc: 0.6184\u001b[0m\n",
      "\u001b[34m221/312 [====================>.........] - ETA: 5s - loss: 1.0678 - acc: 0.6185\u001b[0m\n",
      "\u001b[34m223/312 [====================>.........] - ETA: 5s - loss: 1.0673 - acc: 0.6185\u001b[0m\n",
      "\u001b[34m225/312 [====================>.........] - ETA: 4s - loss: 1.0666 - acc: 0.6187\u001b[0m\n",
      "\u001b[34m227/312 [====================>.........] - ETA: 4s - loss: 1.0664 - acc: 0.6190\u001b[0m\n",
      "\u001b[34m229/312 [=====================>........] - ETA: 4s - loss: 1.0657 - acc: 0.6192\u001b[0m\n",
      "\u001b[34m231/312 [=====================>........] - ETA: 4s - loss: 1.0656 - acc: 0.6191\u001b[0m\n",
      "\u001b[34m233/312 [=====================>........] - ETA: 4s - loss: 1.0654 - acc: 0.6189\u001b[0m\n",
      "\u001b[34m235/312 [=====================>........] - ETA: 4s - loss: 1.0655 - acc: 0.6189\u001b[0m\n",
      "\u001b[34m237/312 [=====================>........] - ETA: 4s - loss: 1.0654 - acc: 0.6190\u001b[0m\n",
      "\u001b[34m239/312 [=====================>........] - ETA: 4s - loss: 1.0667 - acc: 0.6184\u001b[0m\n",
      "\u001b[34m241/312 [======================>.......] - ETA: 3s - loss: 1.0669 - acc: 0.6186\u001b[0m\n",
      "\u001b[34m243/312 [======================>.......] - ETA: 3s - loss: 1.0672 - acc: 0.6184\u001b[0m\n",
      "\u001b[34m245/312 [======================>.......] - ETA: 3s - loss: 1.0669 - acc: 0.6183\u001b[0m\n",
      "\u001b[34m247/312 [======================>.......] - ETA: 3s - loss: 1.0660 - acc: 0.6186\u001b[0m\n",
      "\u001b[34m249/312 [======================>.......] - ETA: 3s - loss: 1.0662 - acc: 0.6187\u001b[0m\n",
      "\u001b[34m251/312 [=======================>......] - ETA: 3s - loss: 1.0650 - acc: 0.6193\u001b[0m\n",
      "\u001b[34m253/312 [=======================>......] - ETA: 3s - loss: 1.0652 - acc: 0.6192\u001b[0m\n",
      "\u001b[34m255/312 [=======================>......] - ETA: 3s - loss: 1.0650 - acc: 0.6191\u001b[0m\n",
      "\u001b[34m257/312 [=======================>......] - ETA: 2s - loss: 1.0636 - acc: 0.6196\u001b[0m\n",
      "\u001b[34m259/312 [=======================>......] - ETA: 2s - loss: 1.0636 - acc: 0.6197\u001b[0m\n",
      "\u001b[34m261/312 [========================>.....] - ETA: 2s - loss: 1.0631 - acc: 0.6199\u001b[0m\n",
      "\u001b[34m263/312 [========================>.....] - ETA: 2s - loss: 1.0628 - acc: 0.6199\u001b[0m\n",
      "\u001b[34m265/312 [========================>.....] - ETA: 2s - loss: 1.0636 - acc: 0.6195\u001b[0m\n",
      "\u001b[34m267/312 [========================>.....] - ETA: 2s - loss: 1.0644 - acc: 0.6196\u001b[0m\n",
      "\u001b[34m269/312 [========================>.....] - ETA: 2s - loss: 1.0635 - acc: 0.6197\u001b[0m\n",
      "\u001b[34m271/312 [=========================>....] - ETA: 2s - loss: 1.0624 - acc: 0.6200\u001b[0m\n",
      "\u001b[34m273/312 [=========================>....] - ETA: 2s - loss: 1.0616 - acc: 0.6200\u001b[0m\n",
      "\u001b[34m275/312 [=========================>....] - ETA: 1s - loss: 1.0613 - acc: 0.6201\u001b[0m\n",
      "\u001b[34m277/312 [=========================>....] - ETA: 1s - loss: 1.0606 - acc: 0.6203\u001b[0m\n",
      "\u001b[34m279/312 [=========================>....] - ETA: 1s - loss: 1.0597 - acc: 0.6206\u001b[0m\n",
      "\u001b[34m281/312 [==========================>...] - ETA: 1s - loss: 1.0598 - acc: 0.6208\u001b[0m\n",
      "\u001b[34m283/312 [==========================>...] - ETA: 1s - loss: 1.0593 - acc: 0.6211\u001b[0m\n",
      "\u001b[34m285/312 [==========================>...] - ETA: 1s - loss: 1.0592 - acc: 0.6211\u001b[0m\n",
      "\u001b[34m287/312 [==========================>...] - ETA: 1s - loss: 1.0601 - acc: 0.6210\u001b[0m\n",
      "\u001b[34m289/312 [==========================>...] - ETA: 1s - loss: 1.0592 - acc: 0.6213\u001b[0m\n",
      "\n",
      "2020-02-27 14:01:04 Uploading - Uploading generated training model\u001b[34m291/312 [==========================>...] - ETA: 1s - loss: 1.0587 - acc: 0.6214\u001b[0m\n",
      "\u001b[34m293/312 [===========================>..] - ETA: 0s - loss: 1.0588 - acc: 0.6215\u001b[0m\n",
      "\u001b[34m295/312 [===========================>..] - ETA: 0s - loss: 1.0586 - acc: 0.6216\u001b[0m\n",
      "\u001b[34m297/312 [===========================>..] - ETA: 0s - loss: 1.0583 - acc: 0.6216\u001b[0m\n",
      "\u001b[34m299/312 [===========================>..] - ETA: 0s - loss: 1.0580 - acc: 0.6217\u001b[0m\n",
      "\u001b[34m301/312 [===========================>..] - ETA: 0s - loss: 1.0577 - acc: 0.6217\u001b[0m\n",
      "\u001b[34m303/312 [============================>.] - ETA: 0s - loss: 1.0568 - acc: 0.6220\u001b[0m\n",
      "\u001b[34m305/312 [============================>.] - ETA: 0s - loss: 1.0566 - acc: 0.6219\u001b[0m\n",
      "\u001b[34m307/312 [============================>.] - ETA: 0s - loss: 1.0572 - acc: 0.6218\u001b[0m\n",
      "\u001b[34m309/312 [============================>.] - ETA: 0s - loss: 1.0571 - acc: 0.6220\u001b[0m\n",
      "\u001b[34m311/312 [============================>.] - ETA: 0s - loss: 1.0572 - acc: 0.6222\u001b[0m\n",
      "\u001b[34m312/312 [==============================] - 18s 59ms/step - loss: 1.0572 - acc: 0.6220 - val_loss: 0.9945 - val_acc: 0.6329\u001b[0m\n",
      "\u001b[34mI0227 14:00:57.944197 140443157276416 cifar10_keras_sm.py:220] Test loss:1.024827268643257\u001b[0m\n",
      "\u001b[34mI0227 14:00:57.944500 140443157276416 cifar10_keras_sm.py:221] Test accuracy:0.6234975961538461\u001b[0m\n",
      "\u001b[34mW0227 14:00:57.944754 140443157276416 deprecation_wrapper.py:119] From cifar10_keras_sm.py:184: The name tf.saved_model.signature_def_utils.predict_signature_def is deprecated. Please use tf.compat.v1.saved_model.signature_def_utils.predict_signature_def instead.\n",
      "\u001b[0m\n",
      "\u001b[34mW0227 14:00:57.945052 140443157276416 deprecation.py:323] From /usr/local/lib/python3.6/site-packages/tensorflow/python/saved_model/signature_def_utils_impl.py:201: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mThis function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\u001b[0m\n",
      "\u001b[34mW0227 14:00:57.945544 140443157276416 deprecation_wrapper.py:119] From cifar10_keras_sm.py:187: The name tf.saved_model.builder.SavedModelBuilder is deprecated. Please use tf.compat.v1.saved_model.builder.SavedModelBuilder instead.\n",
      "\u001b[0m\n",
      "\u001b[34mW0227 14:00:57.946324 140443157276416 deprecation_wrapper.py:119] From cifar10_keras_sm.py:190: The name tf.saved_model.tag_constants.SERVING is deprecated. Please use tf.saved_model.SERVING instead.\n",
      "\u001b[0m\n",
      "\u001b[34mI0227 14:00:57.946809 140443157276416 builder_impl.py:636] No assets to save.\u001b[0m\n",
      "\u001b[34mI0227 14:00:57.946965 140443157276416 builder_impl.py:456] No assets to write.\u001b[0m\n",
      "\u001b[34m2020-02-27 14:00:58.353995: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\u001b[0m\n",
      "\u001b[34mI0227 14:00:58.671239 140443157276416 builder_impl.py:421] SavedModel written to: /opt/ml/model/1/saved_model.pb\u001b[0m\n",
      "\u001b[34mI0227 14:00:58.671474 140443157276416 cifar10_keras_sm.py:194] Model successfully saved at: /opt/ml/model\u001b[0m\n",
      "\n",
      "2020-02-27 14:01:16 Completed - Training job completed\n",
      "Training seconds: 234\n",
      "Billable seconds: 234\n",
      "CPU times: user 828 ms, sys: 32.4 ms, total: 861 ms\n",
      "Wall time: 6min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "estimator.fit({'train':'{}/train'.format(dataset_location),\n",
    "              'validation':'{}/validation'.format(dataset_location),\n",
    "              'eval':'{}/eval'.format(dataset_location)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**잘 하셨습니다.** \n",
    "\n",
    "SageMaker에서 GPU 인스턴스를 사용해 5 epoch를 정상적으로 학습할 수 있었습니다.<br>\n",
    "다음 노트북으로 계속 진행하기 전에 SageMaker 콘솔의 Training jobs 섹션을 살펴보고 여러분이 수행한 job을 찾아 configuration을 확인하세요.\n",
    "\n",
    "스크립트 모드 학습에 대한 자세한 내용은 아래의 AWS 블로그를 참조해 주세요.<br>\n",
    "[Using TensorFlow eager execution with Amazon SageMaker script mode](https://aws.amazon.com/ko/blogs/machine-learning/using-tensorflow-eager-execution-with-amazon-sagemaker-script-mode/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
